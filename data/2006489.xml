<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle><![CDATA[CIF: Small: Learning and estimation with rough non-convex objectives: Fundamental limits and efficient algorithms]]></AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2020</AwardEffectiveDate>
<AwardExpirationDate>06/30/2023</AwardExpirationDate>
<AwardTotalIntnAmount>330003.00</AwardTotalIntnAmount>
<AwardAmount>330003</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Phillip Regalia</SignBlockName>
<PO_EMAI>pregalia@nsf.gov</PO_EMAI>
<PO_PHON>7032922981</PO_PHON>
</ProgramOfficer>
<AbstractNarration>A large number of problems in signal processing, statistics and machine learning require minimizing a cost function with many unknown variables. In a general setting, this task is computationally hard, unless the cost function has the mathematical property called convexity, a case that has attracted a large amount of work over the last fifty years. On the other hand, modern applications often rely on the more complex non-convex formulations, which are optimized using simple algorithms that are not necessarily optimal.  This project explores the hypothesis that, for a broad class of non-convex cost functions, one can find optimal solutions in the typical instances of these functions, even if the worst case might be impossible to optimize. The outcomes will have impacts on all scientific fields and real-world problems that feature non-convex cost functions.&lt;br/&gt;&lt;br/&gt;This project aims at studying probabilistic models of cost functions that are of `mean-field' type, namely they do not have a latent low-dimensional structure. This setting is quite common in high-dimensional statistics and machine learning: Each degree of freedom is equally likely to interact (or not) with every other one. The conjectured connection between the geometry of sublevel sets and tractability was recently established in special cases by developing new algorithms that achieve the desired optimization goal. These algorithms are based on message passing ideas and free energy approximations. This project develops these new methods in a broader domain and investigates precise conditions for their applicability. Furthermore, it develops potential alternatives and improvements.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>07/28/2020</MinAmdLetterDate>
<MaxAmdLetterDate>07/28/2020</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2006489</AwardID>
<Investigator>
<FirstName>Andrea</FirstName>
<LastName>Montanari</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Andrea Montanari</PI_FULL_NAME>
<EmailAddress><![CDATA[montanari@stanford.edu]]></EmailAddress>
<NSF_ID>000107366</NSF_ID>
<StartDate>07/28/2020</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name><![CDATA[Stanford University]]></Name>
<CityName>STANFORD</CityName>
<ZipCode>943052004</ZipCode>
<PhoneNumber>6507232300</PhoneNumber>
<StreetAddress><![CDATA[450 JANE STANFORD WAY]]></StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>16</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA16</CONGRESS_DISTRICT_ORG>
<ORG_UEI_NUM>HJD6G4D6TJY5</ORG_UEI_NUM>
<ORG_LGL_BUS_NAME>THE LELAND STANFORD JUNIOR UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_UEI_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[Stanford University]]></Name>
<CityName>Palo Alto</CityName>
<StateCode>CA</StateCode>
<ZipCode>943052004</ZipCode>
<StreetAddress><![CDATA[350 Jane Stanford Way, Room 272]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>16</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA16</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>779700</Code>
<Text>Comm &amp; Information Foundations</Text>
</ProgramElement>
<ProgramReference>
<Code>7797</Code>
<Text>COMM &amp; INFORMATION FOUNDATIONS</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7936</Code>
<Text>SIGNAL PROCESSING</Text>
</ProgramReference>
<Appropriation>
<Code>0120</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Fund>
<Code>01002021DB</Code>
<Name><![CDATA[NSF RESEARCH & RELATED ACTIVIT]]></Name>
<FUND_SYMB_ID>040100</FUND_SYMB_ID>
</Fund>
<FUND_OBLG>2020~330003</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Optimization of rough and high-dimensional cost functions is the core computational&nbsp;problem in modern machine learning and artificial intelligence. Classical treatments&nbsp;of this problem assume that the cost function is convex. More recent research assumes&nbsp;that it is convex near the global minimum, and that a good guess of the&nbsp;global minimizer is known a priori.<br />Over the last 10 years, it has become increasingly clear that this&nbsp;theory does not apply to the cost functions that are routinely optimized&nbsp;by gradient descent or stochastic gradient descent in machine learning and&nbsp;high-dimensional statistics. From a classical/theoretical perspective, these problems&nbsp;should be unsolvable and yet in practice we see that they are efficiently&nbsp;treated by practitioners at huge scale.&nbsp;<br />What are the fundamental principles that make this "miracle" possible?<br />This project investigated several general principles that make (near-) global optimizationpossible beyond convexity.- Randomness: the cost functions of interest are not constructed by an adversary,&nbsp;but depend on observed data, that are modeled as random.- Overparametrization: Each data sample typically enforces a one-dimensional constraint on&nbsp;the model parameters. Now, these models are often operated in a regime in which thenumber of parameters is comparable or larger than the number of constraints.In this setting, the landscape appears to acquire a simple structure, which makes optimization possible.- Hidden organization of near optima: In certain rich classes of random optimization problems,the problem has multiple near-minima that are far apart, and yet connected through an hidden treestructure. Under certain conditions, this tree structure can be developed to construct&nbsp;novel optimization algorithms. (See Figure)<br />This project investigated these three principles in a number of settingsthat are simple to describe mathematically, and yet conceptually very richand challenging. In several of these models, the PI and collaborators couldprecisely characterize the degree to which these cost functions can be optimizedefficiently using broad classes of efficient algorithms.&nbsp;Most importantly, they could develop algorithms basedon novel principles that achieve those fundamental limits.</p><br> <p>  Last Modified: 01/07/2024<br> Modified by: Andrea&nbsp;Montanari</p></div> <div class="porSideCol" ><div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Image         </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation onePhoto" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2024/2006489/2006489_10690674_1704655942141_algo_sketch--rgov-214x142.png" original="/por/images/Reports/POR/2024/2006489/2006489_10690674_1704655942141_algo_sketch--rgov-800width.png" title="Sketch of the structure of near optima and algorithm behavior"><img src="/por/images/Reports/POR/2024/2006489/2006489_10690674_1704655942141_algo_sketch--rgov-66x44.png" alt="Sketch of the structure of near optima and algorithm behavior"></a> <div class="imageCaptionContainer"> <div class="imageCaption">A cartoon of the behavior of one of the algorithms developed within this project, for optimization of certain random functions on the high-dimensional hypercube.</div> <div class="imageCredit">Andrea Montanari</div> <div class="imagePermisssions">Royalty-free (unrestricted use)</div> <div class="imageSubmitted">Andrea&nbsp;Montanari <div class="imageTitle">Sketch of the structure of near optima and algorithm behavior</div> </div> </li></ul> </div> </div></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[  Optimization of rough and high-dimensional cost functions is the core computationalproblem in modern machine learning and artificial intelligence. Classical treatmentsof this problem assume that the cost function is convex. More recent research assumesthat it is convex near the global minimum, and that a good guess of theglobal minimizer is known a priori. Over the last 10 years, it has become increasingly clear that thistheory does not apply to the cost functions that are routinely optimizedby gradient descent or stochastic gradient descent in machine learning andhigh-dimensional statistics. From a classical/theoretical perspective, these problemsshould be unsolvable and yet in practice we see that they are efficientlytreated by practitioners at huge scale. What are the fundamental principles that make this "miracle" possible? This project investigated several general principles that make (near-) global optimizationpossible beyond convexity.- Randomness: the cost functions of interest are not constructed by an adversary,but depend on observed data, that are modeled as random.- Overparametrization: Each data sample typically enforces a one-dimensional constraint onthe model parameters. Now, these models are often operated in a regime in which thenumber of parameters is comparable or larger than the number of constraints.In this setting, the landscape appears to acquire a simple structure, which makes optimization possible.- Hidden organization of near optima: In certain rich classes of random optimization problems,the problem has multiple near-minima that are far apart, and yet connected through an hidden treestructure. Under certain conditions, this tree structure can be developed to constructnovel optimization algorithms. (See Figure) This project investigated these three principles in a number of settingsthat are simple to describe mathematically, and yet conceptually very richand challenging. In several of these models, the PI and collaborators couldprecisely characterize the degree to which these cost functions can be optimizedefficiently using broad classes of efficient algorithms.Most importantly, they could develop algorithms basedon novel principles that achieve those fundamental limits.     Last Modified: 01/07/2024       Submitted by: AndreaMontanari]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
