<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle><![CDATA[Semiparametric Methods for Data Assimilation and Uncertainty Quantification]]></AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2020</AwardEffectiveDate>
<AwardExpirationDate>08/31/2024</AwardExpirationDate>
<AwardTotalIntnAmount>233747.00</AwardTotalIntnAmount>
<AwardAmount>233747</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>03040000</Code>
<Directorate>
<Abbreviation>MPS</Abbreviation>
<LongName>Direct For Mathematical &amp; Physical Scien</LongName>
</Directorate>
<Division>
<Abbreviation>DMS</Abbreviation>
<LongName>Division Of Mathematical Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Stacey Levine</SignBlockName>
<PO_EMAI>slevine@nsf.gov</PO_EMAI>
<PO_PHON>7032922948</PO_PHON>
</ProgramOfficer>
<AbstractNarration>There is a growing demand in many scientific disciplines for efficient tools to automatically learn models and make predictions from limited noisy observations. For these predictions to be actionable, they must also have quantifiable uncertainty, and be robust to model misspecification. This is particularly relevant in light of events such as the COVID-19 pandemic, where models have to be constantly adapted to include new phenomena such as unreported and asymptomatic cases and constantly evolving social distancing rules and compliance. Other applications include large complex systems such as weather forecasting and social network dynamics where first-principles models are powerful but have difficulty capturing the full range of phenomena involved. The semiparametric framework will help address the growing problem of un-modeled phenomena by allowing existing models to be automatically merged with model-free methods that leverage data to learn a correction to the model in order to match the observed data. The new tools will allow application to a class of high dimensional problems with spatial structure, such as geosystems problems, social networks, and global disease dynamics. Beyond improving forecasting, the semiparametric approach will include accurate uncertainty quantification, which is critical in these application domains. The investigator will train a graduate student and undergraduate students who will be able to carry this research forward, as well as developing and disseminating this key expertise. These students will learn to apply both state-of-the-art and the newly developed methods which will prepare them for future work in applied and computational mathematics.&lt;br/&gt;&lt;br/&gt;The investigator will develop semiparametric modeling techniques that optimally leverage the strengths of parametric (model based) and nonparametric (model-free or data-driven) methods. Specifically, the semiparametric framework allows the flexible nonparametric models to fill in the gaps and correct the low-dimensional model error in a parametric model. The framework employs an ensemble of states in the parametric model to represent the uncertainty in a forecast or state estimate, while a full probability distribution is estimated for the nonparametric model. At each filtering or forecasting step, the ensemble is updated by sampling individual corrections from the model error distribution estimated by the nonparametric model. These sampled corrections will automatically correct biases in the model and inflate the uncertainty when necessary in order to match reality. The evolution of the nonparametric model will typically need to be conditional to the high-dimensional state of the parametric model, which current methods to do not allow. In other words, information must flow in both directions: the nonparametric model corrects the parametric model, but is also informed by the current state of the parametric model. In order to overcome this crucial challenge, supervised dimensionality reduction techniques will be combined with a novel method of learning mappings between non-diffeomorphic spaces. This will allow a Bayesian update of the nonparametric state estimate based on the learned projection of the parametric state. The research includes a novel higher order unscented ensemble forecast that will form the basis for a higher order Kalman filter. These advances will make the best use of available computation resources, since the higher order ensemble forecasting and filtering methods can scale up from small to large ensembles as resources allow. The higher order methods will improve accuracy and uncertainty quantification by estimating higher order moments  of the state estimate and the forecast. For the ensemble forecast, a novel multivariate quadrature method will be applied that uses rank-1 tensor decompositions of the higher moments as quadrature nodes. For the Kalman update, higher order equations will be used based on a maximum entropy closure of the moment equations derived from the Kushner equation (which fully describes the true solution). The advances will effectively use data to learn a model-free correction to a parametric model, simultaneously alleviating model error and the curse-of-dimensionality.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>07/10/2020</MinAmdLetterDate>
<MaxAmdLetterDate>07/10/2020</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.049</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2006808</AwardID>
<Investigator>
<FirstName>Tyrus</FirstName>
<LastName>Berry</LastName>
<PI_MID_INIT>H</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Tyrus H Berry</PI_FULL_NAME>
<EmailAddress><![CDATA[tberry@gmu.edu]]></EmailAddress>
<NSF_ID>000750170</NSF_ID>
<StartDate>07/10/2020</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name><![CDATA[George Mason University]]></Name>
<CityName>FAIRFAX</CityName>
<ZipCode>220304422</ZipCode>
<PhoneNumber>7039932295</PhoneNumber>
<StreetAddress><![CDATA[4400 UNIVERSITY DR]]></StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Virginia</StateName>
<StateCode>VA</StateCode>
<CONGRESSDISTRICT>11</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>VA11</CONGRESS_DISTRICT_ORG>
<ORG_UEI_NUM>EADLFP7Z72E5</ORG_UEI_NUM>
<ORG_LGL_BUS_NAME>GEORGE MASON UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_UEI_NUM>H4NRWLFCDF43</ORG_PRNT_UEI_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[George Mason University]]></Name>
<CityName/>
<StateCode>VA</StateCode>
<ZipCode>220304444</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Virginia</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>11</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>VA11</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>126600</Code>
<Text>APPLIED MATHEMATICS</Text>
</ProgramElement>
<Appropriation>
<Code>0120</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Fund>
<Code>01002021DB</Code>
<Name><![CDATA[NSF RESEARCH & RELATED ACTIVIT]]></Name>
<FUND_SYMB_ID>040100</FUND_SYMB_ID>
</Fund>
<FUND_OBLG>2020~233747</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Mathematical models are critical to describing physcal phenomena, however such models by their very nature must simplify or summarize many of the complexities of reality.&nbsp; These models must then be integrated with and fit to noisy data so they can be used for forecasting, engineering, and control.&nbsp; When imperfect models meet noisy data the result is a measurable mismatch, and this research developed methods to collect and analyze these mismatches and build a model correction using statistical machine learning tools.&nbsp; This required developing a mathematical and algorithmic bridge between traditional modeling methods and statistical machine learning methods.&nbsp; The traditional modeling methods are known as "data assimilation" and this is where the noisy real world data meets an imperfect model and the model is tuned to the data.&nbsp; This project leveraged a form of data assimilation first used in the Apollo program known as a Kalman filter.&nbsp; The Kalman filter has been developed significantly since the 1960s, however there has only been limited work on applying this method to model error and non-Gaussian statistics.&nbsp; The project first trained a PhD student whose thesis has significantly advanced Kalman filtering and forecasting methods for non-Gaussian statistics.&nbsp; The project produced several publications which considered applications of these developments to infectious disease modeling.&nbsp; With the traditional modeling tools improved to be capable of integration with machine learning tools, attention was then turned to the machine learning methods themselves.&nbsp; The methods chosen are known as "manifold learning" tools based on their strong mathematical theory and statistical guarantees.&nbsp; However, a weakness of these methods is that they require data to lie on a rigid mathematical structure known as a manifold.&nbsp; In order to overcome this shortcoming, a second PhD student was trained and the mathematical theory was generalized beyond manifolds to a much more applicable structure known as a metric-measure spaces.&nbsp; With this advance the traditional and machine learning methods were now ready to be integrated and tested on real data.&nbsp; A third PhD student was trained in collaboration with a researcher at NIST to integrate the methods for Nitrogen Vacancy (NV) diamond sensor data.&nbsp; The type of model error considered in this application was drift in the parameters of a sensor.&nbsp; It was discovered that in an idealized sensor (where the sensor does not effect its environment and vice versa) that it was theoretically impossible to extract the drift of the sensor without a "ground truth" to calibrate the sensor to, this is due to a phenomenon known as non-observability.&nbsp; However, real sensor do have small interactions with their environment, and vice versa, and these tiny interactions can overcome non-observability.&nbsp; This lead to the development of a theory of self-calibrating sensors which can automatically correct for sensor drift without reference to a "ground truth" to calibrate them.&nbsp; &nbsp; &nbsp;&nbsp;</p><br> <p>  Last Modified: 12/22/2024<br> Modified by: Tyrus&nbsp;H&nbsp;Berry</p></div> <div class="porSideCol" ></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[  Mathematical models are critical to describing physcal phenomena, however such models by their very nature must simplify or summarize many of the complexities of reality. These models must then be integrated with and fit to noisy data so they can be used for forecasting, engineering, and control. When imperfect models meet noisy data the result is a measurable mismatch, and this research developed methods to collect and analyze these mismatches and build a model correction using statistical machine learning tools. This required developing a mathematical and algorithmic bridge between traditional modeling methods and statistical machine learning methods. The traditional modeling methods are known as "data assimilation" and this is where the noisy real world data meets an imperfect model and the model is tuned to the data. This project leveraged a form of data assimilation first used in the Apollo program known as a Kalman filter. The Kalman filter has been developed significantly since the 1960s, however there has only been limited work on applying this method to model error and non-Gaussian statistics. The project first trained a PhD student whose thesis has significantly advanced Kalman filtering and forecasting methods for non-Gaussian statistics. The project produced several publications which considered applications of these developments to infectious disease modeling. With the traditional modeling tools improved to be capable of integration with machine learning tools, attention was then turned to the machine learning methods themselves. The methods chosen are known as "manifold learning" tools based on their strong mathematical theory and statistical guarantees. However, a weakness of these methods is that they require data to lie on a rigid mathematical structure known as a manifold. In order to overcome this shortcoming, a second PhD student was trained and the mathematical theory was generalized beyond manifolds to a much more applicable structure known as a metric-measure spaces. With this advance the traditional and machine learning methods were now ready to be integrated and tested on real data. A third PhD student was trained in collaboration with a researcher at NIST to integrate the methods for Nitrogen Vacancy (NV) diamond sensor data. The type of model error considered in this application was drift in the parameters of a sensor. It was discovered that in an idealized sensor (where the sensor does not effect its environment and vice versa) that it was theoretically impossible to extract the drift of the sensor without a "ground truth" to calibrate the sensor to, this is due to a phenomenon known as non-observability. However, real sensor do have small interactions with their environment, and vice versa, and these tiny interactions can overcome non-observability. This lead to the development of a theory of self-calibrating sensors which can automatically correct for sensor drift without reference to a "ground truth" to calibrate them.       Last Modified: 12/22/2024       Submitted by: TyrusHBerry]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
