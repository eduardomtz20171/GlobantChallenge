<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle><![CDATA[CHS: Small: Collaborative Research: Articulate+ - A Conversational Interface for Democr atizing Visual Analysis]]></AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2020</AwardEffectiveDate>
<AwardExpirationDate>09/30/2023</AwardExpirationDate>
<AwardTotalIntnAmount>333000.00</AwardTotalIntnAmount>
<AwardAmount>333000</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Dan Cosley</SignBlockName>
<PO_EMAI>dcosley@nsf.gov</PO_EMAI>
<PO_PHON>7032928832</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The Internet gives the public, including scientists, access to vast amounts of data. Visualization is a powerful tool to take that data and turn it into a form that is easier to understand, helping people make better decisions. However, creating good visualizations from data is not easy. The Articulate+ Project will work to generate new knowledge about how to empower people with information visualization, by using speech and gesture as inputs, in new ways. A goal is to enable users to benefit from imprecise statements about what they want, rather than needing to give specific commands telling the computer what to do. This project will make it easier for citizens to engage in conversations with the computer about economic, medical data, demographic, transportation data, climate, and sports data; to answers that are more meaningful, and so to democratize data literacy. &lt;br/&gt;&lt;br/&gt;This research will use speech, gesture, and log data as new input interaction modalities for visual analytics. It will develop new understandings of how the imprecise and vague nature of natural language queries and gestures, as contextualized by work on a specific visual analysis problem, can be modeled as an opportunity for an intelligent software system to learn more about the underlying intent of the users. This model of intent, in turn, will be used to provide contextualized visualizations, which are expected to help those users gain valuable insights from their data. The audio speech data will be used to computationally model overhearing, to help infer users' current contextualized goals. The gesture data will be used to disambiguate expressions that refer to visual elements of a visualization (e.g., "that pie chart about electricity consumption"). The project will develop new understandings of how to combine the log of visualization state, the computational model of overhearing, and the gesture data in order to generate new visualizations in support of users’ work on tasks. Evaluations will be performed in both controlled laboratory and naturalistic study settings to determine whether effective semantic parsers can be derived for these specific visual analysis domains, and how this contextual interface affects users’ experience of visualization and discovery.  The integrated annotated datasets from the studies will be made available to the research community satisfying a need for ecologically valid situated language interaction corpora.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>09/15/2020</MinAmdLetterDate>
<MaxAmdLetterDate>08/31/2021</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2007257</AwardID>
<Investigator>
<FirstName>Moira</FirstName>
<LastName>Zellner</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Moira Zellner</PI_FULL_NAME>
<EmailAddress><![CDATA[m.zellner@northeastern.edu]]></EmailAddress>
<NSF_ID>000072111</NSF_ID>
<StartDate>09/15/2020</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Andrew</FirstName>
<LastName>Johnson</LastName>
<PI_MID_INIT>E</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Andrew E Johnson</PI_FULL_NAME>
<EmailAddress><![CDATA[ajohnson@uic.edu]]></EmailAddress>
<NSF_ID>000332025</NSF_ID>
<StartDate>09/15/2020</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Barbara</FirstName>
<LastName>DiEugenio</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Barbara DiEugenio</PI_FULL_NAME>
<EmailAddress><![CDATA[bdieugen@uic.edu]]></EmailAddress>
<NSF_ID>000192544</NSF_ID>
<StartDate>09/15/2020</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name><![CDATA[University of Illinois at Chicago]]></Name>
<CityName>CHICAGO</CityName>
<ZipCode>606124305</ZipCode>
<PhoneNumber>3129962862</PhoneNumber>
<StreetAddress><![CDATA[809 S MARSHFIELD AVE M/C 551]]></StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<StateCode>IL</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IL07</CONGRESS_DISTRICT_ORG>
<ORG_UEI_NUM>W8XEAJDKMXH3</ORG_UEI_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF ILLINOIS</ORG_LGL_BUS_NAME>
<ORG_PRNT_UEI_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Illinois at Chicago]]></Name>
<CityName>Chicago</CityName>
<StateCode>IL</StateCode>
<ZipCode>606124305</ZipCode>
<StreetAddress><![CDATA[809 S Marshfield]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IL07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>736700</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0120</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0121</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Fund>
<Code>01002021DB</Code>
<Name><![CDATA[NSF RESEARCH & RELATED ACTIVIT]]></Name>
<FUND_SYMB_ID>040100</FUND_SYMB_ID>
</Fund>
<Fund>
<Code>01002122DB</Code>
<Name><![CDATA[NSF RESEARCH & RELATED ACTIVIT]]></Name>
<FUND_SYMB_ID>040100</FUND_SYMB_ID>
</Fund>
<FUND_OBLG>2020~163901</FUND_OBLG>
<FUND_OBLG>2021~169099</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span id="docs-internal-guid-649d224c-7fff-deda-43bb-d28c1abd289e"> </span></p> <p dir="ltr"><span>The goal of this project is to make it possible for humans to speak to a computer as naturally as they would another human, and have the computer be able to interpret complex data and produce answers in the form of data visualizations such as charts and graphs. This is important because it enables experts such as scientists and engineers, as well as non-experts such as policy makers, decision makers and the general public, to better understand complex data through the use of state-of-the-art visualization techniques (the process of turning data into informative charts), without having to be data visualization experts themselves.</span></p> <p dir="ltr">In working to achieve this we made advances in Artificial Intelligence (AI) and data visualization research, especially in how to create natural language processing techniques to translate natural human speech to the language of data visualization and to support collaboration among people.</p> <p dir="ltr">The project successfully developed and demonstrated a working prototype (Articulate+) which was able to produce data visualizations by listening to a conversation between two people, and interjecting during the conversation to produce data visualizations that were relevant to the conversation. Our system was tested with 50 participants (in 25 groups of two), who could ask natural questions on COVID data; Articulate+ was able to answer questions by interfacing to a number of authentic datasets and producing relevant charts (such as 'show me the correlation between COVID risk and access to medical care'). This moves us closer to a future where AI systems can routinely aid humans in complex tasks as if the AI were a human assistant- like a co-pilot of an airplane.</p> <p dir="ltr">Our work represents a major paradigm shift from conventional AI systems like Amazon's Alexa and Apple's Siri, which can only respond to a user when explicitly commanded to do so using a wake word, like 'Alexa&hellip;' or 'Hey Siri&hellip;'. Our system is able to listen to a conversation, build up a context of what has already been said, and use that context to determine the relevant charts to produce. We discovered a number of benefits to this approach:</p> <ul> <li dir="ltr"> <p dir="ltr"><span>By having an AI continuously listening to a conversation, the collected context can be used by our system to correct imprecise, misheard, or incomplete queries from users, enabling the AI to better determine the intent of its users during a conversation.</span></p> </li> <li dir="ltr"> <p dir="ltr"><span>The common use of wake words in conventional AI systems, has an unanticipated effect of conditioning users to speak in an unnatural and stilted way when conversing with AI. By eliminating the need for wake words, conversations between users and our system flowed more naturally like in a human to human conversation.&nbsp;</span></p> </li> </ul> <p dir="ltr"><span>Our project also examined how well current state-of-the-art AI systems like ChatGPT can be made to perform data visualization tasks, in particular how its Large Language Models can be improved to produce more accurate data visualizations. We discovered that currently, ChatGPT is able to produce only the most basic chart types.</span></p> <p><span>During the course of this project 4 papers were published. One graduate student obtained his MS degree, and is continuing toward a PhD degree in AI, another graduate student finished her MS degree and is continuing toward a PhD in human-computer interaction, </span><span>and yet another graduate&nbsp; student is continuing on with her PhD in natural language processing</span><span>. One graduate student completed a PhD, and another graduated to become an assistant professor. </span><span>Of the 5 students mentioned here, 3 are women.</span></p> <p>&nbsp;</p><br> <p>  Last Modified: 12/21/2023<br> Modified by: Andrew&nbsp;E&nbsp;Johnson</p></div> <div class="porSideCol" ></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[      The goal of this project is to make it possible for humans to speak to a computer as naturally as they would another human, and have the computer be able to interpret complex data and produce answers in the form of data visualizations such as charts and graphs. This is important because it enables experts such as scientists and engineers, as well as non-experts such as policy makers, decision makers and the general public, to better understand complex data through the use of state-of-the-art visualization techniques (the process of turning data into informative charts), without having to be data visualization experts themselves.   In working to achieve this we made advances in Artificial Intelligence (AI) and data visualization research, especially in how to create natural language processing techniques to translate natural human speech to the language of data visualization and to support collaboration among people.   The project successfully developed and demonstrated a working prototype (Articulate+) which was able to produce data visualizations by listening to a conversation between two people, and interjecting during the conversation to produce data visualizations that were relevant to the conversation. Our system was tested with 50 participants (in 25 groups of two), who could ask natural questions on COVID data; Articulate+ was able to answer questions by interfacing to a number of authentic datasets and producing relevant charts (such as 'show me the correlation between COVID risk and access to medical care'). This moves us closer to a future where AI systems can routinely aid humans in complex tasks as if the AI were a human assistant- like a co-pilot of an airplane.   Our work represents a major paradigm shift from conventional AI systems like Amazon's Alexa and Apple's Siri, which can only respond to a user when explicitly commanded to do so using a wake word, like 'Alexa' or 'Hey Siri'. Our system is able to listen to a conversation, build up a context of what has already been said, and use that context to determine the relevant charts to produce. We discovered a number of benefits to this approach:     By having an AI continuously listening to a conversation, the collected context can be used by our system to correct imprecise, misheard, or incomplete queries from users, enabling the AI to better determine the intent of its users during a conversation.     The common use of wake words in conventional AI systems, has an unanticipated effect of conditioning users to speak in an unnatural and stilted way when conversing with AI. By eliminating the need for wake words, conversations between users and our system flowed more naturally like in a human to human conversation.     Our project also examined how well current state-of-the-art AI systems like ChatGPT can be made to perform data visualization tasks, in particular how its Large Language Models can be improved to produce more accurate data visualizations. We discovered that currently, ChatGPT is able to produce only the most basic chart types.   During the course of this project 4 papers were published. One graduate student obtained his MS degree, and is continuing toward a PhD degree in AI, another graduate student finished her MS degree and is continuing toward a PhD in human-computer interaction, and yet another graduate student is continuing on with her PhD in natural language processing. One graduate student completed a PhD, and another graduated to become an assistant professor. Of the 5 students mentioned here, 3 are women.        Last Modified: 12/21/2023       Submitted by: AndrewEJohnson]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
