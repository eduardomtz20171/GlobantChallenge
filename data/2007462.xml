<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle><![CDATA[AF: Small: Computational Complexity Lower Bounds: Time, Space and Communication]]></AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2020</AwardEffectiveDate>
<AwardExpirationDate>06/30/2024</AwardExpirationDate>
<AwardTotalIntnAmount>450000.00</AwardTotalIntnAmount>
<AwardAmount>450000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Peter Brass</SignBlockName>
<PO_EMAI>pbrass@nsf.gov</PO_EMAI>
<PO_PHON>7032922182</PO_PHON>
</ProgramOfficer>
<AbstractNarration>While computers have revolutionized our world, the resources&lt;br/&gt;required to perform computational tasks are poorly understood.&lt;br/&gt;Developing a mathematical theory of computation is crucial in our&lt;br/&gt;information age, where computers are involved in essentially every&lt;br/&gt;part of our life. Computational complexity, the study of the amount&lt;br/&gt;of resources needed to perform computational tasks, is essential&lt;br/&gt;for understanding the power of computation and for the development&lt;br/&gt;of a theory of computation. It is also essential in designing&lt;br/&gt;efficient communication protocols and secure cryptographic protocols,&lt;br/&gt;and in understanding human and machine learning. Proving lower&lt;br/&gt;bounds for the resources required by different computational&lt;br/&gt;models, and for different computational tasks, is among the most&lt;br/&gt;exciting, most challenging and most important topics in theoretical&lt;br/&gt;computer science. The project will study computational complexity&lt;br/&gt;lower bounds, focusing on two research directions: Lower bounds&lt;br/&gt;related to learning; and, studying the relative power of quantum&lt;br/&gt;and classical computational models.&lt;br/&gt;&lt;br/&gt;In more details, we will focus on the following directions. (1)&lt;br/&gt;Memory-Samples lower bounds for learning: memory/samples lower&lt;br/&gt;bounds for online learning algorithms is a very exciting research&lt;br/&gt;direction that has been studied in a line of recent work. For&lt;br/&gt;example, it was proved that any algorithm for learning parities&lt;br/&gt;requires either a memory of quadratic size or an exponential number&lt;br/&gt;of samples. The project will further study memory/samples lower&lt;br/&gt;bounds for learning and their relations to other topics in&lt;br/&gt;computational complexity. (2) The relative power of quantum and&lt;br/&gt;classical computational models: The project will study gaps between&lt;br/&gt;quantum and classical complexity in various models. In particular, it&lt;br/&gt;will investigate separations between quantum and classical communication complexity,&lt;br/&gt;as well as the relative power of quantum and classical algorithms&lt;br/&gt;in the context of memory/samples trade-offs for learning. (3)&lt;br/&gt;Circuit complexity lower bounds related to learning: The amazing&lt;br/&gt;success of machine learning, and in particular deep learning,&lt;br/&gt;motivates a study of closely related computational models. The&lt;br/&gt;project will study linear-threshold circuits and other models of&lt;br/&gt;circuits that are related to learning.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>06/24/2020</MinAmdLetterDate>
<MaxAmdLetterDate>06/24/2020</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2007462</AwardID>
<Investigator>
<FirstName>Ran</FirstName>
<LastName>Raz</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Ran Raz</PI_FULL_NAME>
<EmailAddress><![CDATA[ranr@princeton.edu]]></EmailAddress>
<NSF_ID>000658795</NSF_ID>
<StartDate>06/24/2020</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name><![CDATA[Princeton University]]></Name>
<CityName>PRINCETON</CityName>
<ZipCode>085442001</ZipCode>
<PhoneNumber>6092583090</PhoneNumber>
<StreetAddress><![CDATA[1 NASSAU HALL]]></StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New Jersey</StateName>
<StateCode>NJ</StateCode>
<CONGRESSDISTRICT>12</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NJ12</CONGRESS_DISTRICT_ORG>
<ORG_UEI_NUM>NJ1YPQXQG7U5</ORG_UEI_NUM>
<ORG_LGL_BUS_NAME>THE TRUSTEES OF PRINCETON UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_UEI_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[Princeton University]]></Name>
<CityName>Princeton</CityName>
<StateCode>NJ</StateCode>
<ZipCode>085442020</ZipCode>
<StreetAddress><![CDATA[87 Prospect Avenue]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New Jersey</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>12</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NJ12</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>287800</Code>
<Text>Special Projects - CCF</Text>
</ProgramElement>
<ProgramElement>
<Code>779600</Code>
<Text>Algorithmic Foundations</Text>
</ProgramElement>
<ProgramReference>
<Code>075Z</Code>
<Text>Artificial Intelligence (AI)</Text>
</ProgramReference>
<ProgramReference>
<Code>079Z</Code>
<Text>Machine Learning Theory</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7927</Code>
<Text>COMPLEXITY &amp; CRYPTOGRAPHY</Text>
</ProgramReference>
<ProgramReference>
<Code>7929</Code>
<Text>COMPUTATIONAL GEOMETRY</Text>
</ProgramReference>
<Appropriation>
<Code>0120</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Fund>
<Code>01002021DB</Code>
<Name><![CDATA[NSF RESEARCH & RELATED ACTIVIT]]></Name>
<FUND_SYMB_ID>040100</FUND_SYMB_ID>
</Fund>
<FUND_OBLG>2020~450000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Below we describe three of the research outcomes of the project:</p> <p><strong>Certified Hardness vs. Randomness for Log-Space</strong></p> <p>We study the relative power of randomized versus deterministic algorithms with logarithmic space. Are randomized algorithms more powerful than deterministic ones? Using the well-known hardness-versus-randomness paradigm of Nisan and Wigderson, Klivans and van Melkebeek showed how to fully derandomize any randomized logspace algorithm, under a certain, widely believed, complexity-theoretic assumption. However, as in all previous works on the hardness-versus-randomness paradigm, their derandomized algorithm relies blindly on the complexity assumption. If the assumption is false, the derandomized algorithm may output incorrect values, and thus a user cannot be sure that an output given by the algorithm is correct.</p> <p>In a joint work with Pyne and Zhan, we show how to derandomize any randomized logspace algorithm, under the same complexity assumption, with a derandomized algorithm <em>D</em> that never outputs an incorrect value (even if the complexity assumption is false). If the assumption is true, <em>D</em> always outputs the correct value and if the assumption is false, <em>D</em> either outputs the correct value or alerts that the assumption was found to be false. Thus, if <em>D</em> outputs a value, that value is certified to be correct. Moreover, if <em>D</em> does not output a value, it alerts that the complexity assumption was found to be false and refutes the assumption.</p> <p><strong>Memory-Samples Lower Bounds for Learning:</strong></p> <p>In prior work, we demonstrated that certain online learning problems inherently require either super-linear (up to quadratic) memory size or a super-polynomial number of samples. In this project, in joint works with Garg, Kothari, Liu, Liu and Zhan, we further studied memory-samples lower bounds for learning and explored connections to other areas in complexity theory. Memory-constrained learning lower bounds highlight the critical role of memory in learning and cognitive processes. They may be relevant to understanding human learning and may have impacts on machine learning and optimization. Furthermore, they have applications in cryptography.</p> <p>Among the results that we obtained: We extended this line of work to noisy learning problems and proved improved memory-samples lower bounds in the noisy case. We also studied the quantum case, where the learning algorithm is a quantum algorithm with both, classical memory and quantum memory. We proved that for a large class of classical learning tasks, the known memory-samples lower bounds remain qualitatively the same, even if the learning algorithm can use, in addition to the classical memory, a quantum memory of size <em>cn</em> (for some constant <em>c &gt; 0</em>).</p> <p><strong>A Parallel Repetition Theorem for All 3-Player Games with Binary Inputs:</strong></p> <p>Multi-player games are a central notion that changed the landscape of theoretical computer science and led to milestone achievements, such as, the PCP theorem, delegation of computation, and the theory of hardness of approximation. The same notion is also central in the study of quantum entanglement and is related to several topics in mathematics. In a <em>k</em>-player game, a referee chooses <em>k</em> random questions according to some (publicly known) joint distribution and sends the <em>i</em>-th question to the <em>i</em>-th player. Each player needs to send an answer, without any communication between the players. The players jointly win if a (publicly known) predicate holds. The value of the game is the maximal probability of success that the players can achieve. The parallel repetition of a multi-player game is a game where the players try to win <em>n</em> copies of the original game, simultaneously. The main question is what can be proved about the value of the parallel repetition of a game. For parallel repetition of 2-player games, with value &lt; 1, it is well known that the value of the <em>n</em>-fold parallel repetition of the game decays exponentially fast in <em>n</em>, but games involving 3 or more players have proven much more difficult to analyze, and the only known general bound on their parallel repeated value is an inverse Ackermann bound. Proving a better bound is a notoriously hard long-standing open problem.</p> <p>In a sequence works, joint with Girish, Holmgren, Mittal, and Zhan, we proved that for every 3-player game, with binary questions and value &lt; 1, the value of the <em>n</em>-fold parallel repetition of the game decays polynomially fast to 0. For many of these games, only inverse Ackerman bound was previously known and all previously known techniques failed. Along the way to proving this result, we prove several additional parallel repetition theorems for multi-player games.</p><br> <p>  Last Modified: 11/06/2024<br> Modified by: Ran&nbsp;Raz</p></div> <div class="porSideCol" ></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[  Below we describe three of the research outcomes of the project:   Certified Hardness vs. Randomness for Log-Space   We study the relative power of randomized versus deterministic algorithms with logarithmic space. Are randomized algorithms more powerful than deterministic ones? Using the well-known hardness-versus-randomness paradigm of Nisan and Wigderson, Klivans and van Melkebeek showed how to fully derandomize any randomized logspace algorithm, under a certain, widely believed, complexity-theoretic assumption. However, as in all previous works on the hardness-versus-randomness paradigm, their derandomized algorithm relies blindly on the complexity assumption. If the assumption is false, the derandomized algorithm may output incorrect values, and thus a user cannot be sure that an output given by the algorithm is correct.   In a joint work with Pyne and Zhan, we show how to derandomize any randomized logspace algorithm, under the same complexity assumption, with a derandomized algorithm D that never outputs an incorrect value (even if the complexity assumption is false). If the assumption is true, D always outputs the correct value and if the assumption is false, D either outputs the correct value or alerts that the assumption was found to be false. Thus, if D outputs a value, that value is certified to be correct. Moreover, if D does not output a value, it alerts that the complexity assumption was found to be false and refutes the assumption.   Memory-Samples Lower Bounds for Learning:   In prior work, we demonstrated that certain online learning problems inherently require either super-linear (up to quadratic) memory size or a super-polynomial number of samples. In this project, in joint works with Garg, Kothari, Liu, Liu and Zhan, we further studied memory-samples lower bounds for learning and explored connections to other areas in complexity theory. Memory-constrained learning lower bounds highlight the critical role of memory in learning and cognitive processes. They may be relevant to understanding human learning and may have impacts on machine learning and optimization. Furthermore, they have applications in cryptography.   Among the results that we obtained: We extended this line of work to noisy learning problems and proved improved memory-samples lower bounds in the noisy case. We also studied the quantum case, where the learning algorithm is a quantum algorithm with both, classical memory and quantum memory. We proved that for a large class of classical learning tasks, the known memory-samples lower bounds remain qualitatively the same, even if the learning algorithm can use, in addition to the classical memory, a quantum memory of size cn (for some constant c  0).   A Parallel Repetition Theorem for All 3-Player Games with Binary Inputs:   Multi-player games are a central notion that changed the landscape of theoretical computer science and led to milestone achievements, such as, the PCP theorem, delegation of computation, and the theory of hardness of approximation. The same notion is also central in the study of quantum entanglement and is related to several topics in mathematics. In a k-player game, a referee chooses k random questions according to some (publicly known) joint distribution and sends the i-th question to the i-th player. Each player needs to send an answer, without any communication between the players. The players jointly win if a (publicly known) predicate holds. The value of the game is the maximal probability of success that the players can achieve. The parallel repetition of a multi-player game is a game where the players try to win n copies of the original game, simultaneously. The main question is what can be proved about the value of the parallel repetition of a game. For parallel repetition of 2-player games, with value n-fold parallel repetition of the game decays exponentially fast in n, but games involving 3 or more players have proven much more difficult to analyze, and the only known general bound on their parallel repeated value is an inverse Ackermann bound. Proving a better bound is a notoriously hard long-standing open problem.   In a sequence works, joint with Girish, Holmgren, Mittal, and Zhan, we proved that for every 3-player game, with binary questions and value n-fold parallel repetition of the game decays polynomially fast to 0. For many of these games, only inverse Ackerman bound was previously known and all previously known techniques failed. Along the way to proving this result, we prove several additional parallel repetition theorems for multi-player games.     Last Modified: 11/06/2024       Submitted by: RanRaz]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
