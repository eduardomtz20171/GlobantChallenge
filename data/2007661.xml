<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle><![CDATA[OAC Core: Small: Open-Source Robust 4D Reconstruction Framework for Real-Time Dynamic Human Capture]]></AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2020</AwardEffectiveDate>
<AwardExpirationDate>08/31/2024</AwardExpirationDate>
<AwardTotalIntnAmount>499997.00</AwardTotalIntnAmount>
<AwardAmount>531997</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05090000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>OAC</Abbreviation>
<LongName>Office of Advanced Cyberinfrastructure (OAC)</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Varun Chandola</SignBlockName>
<PO_EMAI>vchandol@nsf.gov</PO_EMAI>
<PO_PHON>7032922656</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The upcoming deployment of 5G technology makes it feasible to communicate with extremely low latency the vast amounts of data needed for Augmented Reality (AR) and Mixed Reality (MR), which enables transformative applications such as 3D tele-immersive (or 3D facetime) communication, mobile AR apps using captured 4D human contents, AI assistants of lifelike and personalized avatars, human-aware robots that can serve or work with humans, tele-rehabilitation to connect physical therapists with wounded patients far from treatment facilities, etc. All these examples of AR and MR applications require the development of real-time 4D (space and time) capture and reconstruction of dynamic scenes involving human bodies, faces, body add-ons (like clothes), and their surrounding environments. Despite prior research on real-time 4D reconstruction, there is still no open-source and robust reconstruction system that can model topological changes of the dynamic scenes and track the moving surfaces with accuracy and robustness. This project is designed to bridge such gaps, to develop an open-source and robust 4D reconstruction framework that can benefit researchers and developers in broader scientific communities as well as industry alliances, including 5G medical standards, AR and MR game engines, lifelike AI assistants, human-aware robotics, tele-rehabilitation, etc. This project also provides curriculum development and educational activities for graduate, undergraduate, and K-12 students through summer camps. &lt;br/&gt; &lt;br/&gt;The research proposed for this project centers around the elegant modeling of topological changes in dynamic scenes and the robust tracking of moving surfaces, towards the ultimate goal of developing an open-source robust 4D reconstruction framework for real-time capture of dynamic human scenes. To solve the challenges of topological changes, the volumetric fusion framework and its data structures will be fundamentally redesigned, by introducing Non-manifold Volumetric Grids into both Truncated Signed Distance Field (TSDF) and Embedded Deformation Graph (EDG) representations, allowing both the volumetric cells to replicate themselves and the edges to be broken. Such a novel topology-change-aware framework will allow the reconstructed mesh geometry to update its connectivity on-the-fly, along with a flexible deformation graph updating its connectivity between nodes throughout the 4D capture process. To solve the robust surface tracking problem in 4D dynamic human capture, a Parameterized Animatable Volumetric Model (PAVM) is proposed to combine the benefits of both the parametric human body model and the volumetric TSDF. The TSDF volumetric grids are built on top of the parameterized human body surfaces, so that they can be used to represent the add-ons (e.g. clothes) to the human body. The regular volumetric structure of our PAVM makes it easy to integrate into deep neural networks for robust surface tracking, as well as providing semantic modeling capability. The technical feasibility of the 4D reconstruction framework will be validated by the development of a Mobile 3D Facetime testbed, which will allow people in remote places to interact with each other in a natural AR fashion.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>06/24/2020</MinAmdLetterDate>
<MaxAmdLetterDate>06/10/2022</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2007661</AwardID>
<Investigator>
<FirstName>Xiaohu</FirstName>
<LastName>Guo</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Xiaohu Guo</PI_FULL_NAME>
<EmailAddress><![CDATA[xguo@utdallas.edu]]></EmailAddress>
<NSF_ID>000178016</NSF_ID>
<StartDate>06/24/2020</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name><![CDATA[University of Texas at Dallas]]></Name>
<CityName>RICHARDSON</CityName>
<ZipCode>750803021</ZipCode>
<PhoneNumber>9728832313</PhoneNumber>
<StreetAddress><![CDATA[800 WEST CAMPBELL RD.]]></StreetAddress>
<StreetAddress2><![CDATA[SP2.25]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<StateCode>TX</StateCode>
<CONGRESSDISTRICT>24</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>TX24</CONGRESS_DISTRICT_ORG>
<ORG_UEI_NUM>EJCVPNN1WFS5</ORG_UEI_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF TEXAS AT DALLAS</ORG_LGL_BUS_NAME>
<ORG_PRNT_UEI_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Texas at Dallas]]></Name>
<CityName>Richardson</CityName>
<StateCode>TX</StateCode>
<ZipCode>750803021</ZipCode>
<StreetAddress><![CDATA[800 W. Campbell Rd., AD15]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>24</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>TX24</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>090Y00</Code>
<Text>OAC-Advanced Cyberinfrast Core</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code/>
<Name/>
<APP_SYMB_ID/>
</Appropriation>
<Appropriation>
<Code>0120</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0121</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Fund>
<Code>01002223DB</Code>
<Name><![CDATA[NSF RESEARCH & RELATED ACTIVIT]]></Name>
<FUND_SYMB_ID>040100</FUND_SYMB_ID>
</Fund>
<Fund>
<Code>01002021DB</Code>
<Name><![CDATA[NSF RESEARCH & RELATED ACTIVIT]]></Name>
<FUND_SYMB_ID>040100</FUND_SYMB_ID>
</Fund>
<Fund>
<Code>01002122DB</Code>
<Name><![CDATA[NSF RESEARCH & RELATED ACTIVIT]]></Name>
<FUND_SYMB_ID>040100</FUND_SYMB_ID>
</Fund>
<FUND_OBLG>2020~499997</FUND_OBLG>
<FUND_OBLG>2021~16000</FUND_OBLG>
<FUND_OBLG>2022~16000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The intellectual merits of this project focus on the sophisticated modeling of topological changes in dynamic scenes and the robust reconstruction of moving surfaces, with the ultimate goal of developing an open-source 4D reconstruction framework for real-time capture and generation of dynamic humans and their surrounding objects.</p>  <p>To address the challenges of topological changes, the volumetric fusion framework and its underlying data structures were fundamentally redesigned. We introduced&nbsp;Non-manifold Volumetric Grids&nbsp;into both the Truncated Signed Distance Field (TSDF) and Embedded Deformation Graph (EDG) representations. This innovation allows volumetric cells to replicate and edges to break, enabling a topology-change-aware framework that dynamically updates the connectivity of reconstructed mesh geometry. Simultaneously, a flexible deformation graph adapts its connectivity between nodes throughout the 4D capture process.</p>  <p>For modeling the human body alongside multi-layer garments, we developed a 3D human body and garment dataset and introduced a novel multi-layer garment reconstruction algorithm. Using purchased 3D models from Renderpeople and AXYZ Design, we performed SMPL fitting to estimate intrinsic body shapes and poses, extracting garment geometries and textures. This dataset enabled the creation of&nbsp;Layered-Garment-Net (LGN), a reconstruction algorithm capable of generating intersection-free, multiple garment layers defined by implicit function fields over the body surface. By leveraging specially designed&nbsp;Garment Indication Fields (GIF), LGN enforces implicit covering relationships among the signed distance fields (SDFs) of different garment layers, avoiding self-intersections between garments and the body.</p>  <p>This project has broader impacts on several applications spanning from automatic generation of talking avatars, to the study of vocal strain and paralaryngeal-respiratory motor coordination in those voice disorder patients.</p>  <p>One key application of 4D human reconstruction is the generation of talking avatars. Given an audio input, the system generates 3D or 2D (video) avatars with personalized pose dynamics, facial expressions, synchronized lip movements, and natural eye blinks. To achieve this, we built an original, person-specific, dynamic 3D talking dataset by analyzing "in-the-wild" videos. This dataset captures dynamic head poses, eye movements, lip synchronization, and 3D face models for each frame in an automated manner. Based on this dataset, we developed&nbsp;FACIAL-GAN (FACe Implicit Attribute Learning Generative Adversarial Network), which integrates phonetic, contextual, and identity information to synthesize 3D face animations with realistic lip motions, head poses, and eye blinks. Additionally, we proposed a data-efficient&nbsp;Disentangled Recurrent Representation (DR&sup2;)&nbsp;learning method, which can generate photorealistic talking videos, complete with full-body gestures, using just 2--5 minutes of training video. Furthermore, we unified co-speech gesture and talking head generation within a single model architecture, proposing a novel diffusion-based framework with shared weights across modalities and adapters for adaptation to a common latent space.</p>  <p>This project also has interdisciplinary applications. For example, we applied our 4D skin motion tracking algorithms to develop a&nbsp;skin deformation metric&nbsp;for analyzing muscle activation patterns in individuals with voice disorders. In collaboration with Dr. Adrianna Shembel from the School of Behavioral and Brain Sciences at UT-Dallas, we used consumer RGB-D cameras to track skin deformation in the perilaryngeal anterior neck regions of participants with and without vocal strain. Differences in neck movement variability between the two groups provided insights into extrinsic laryngeal vocal muscles contributing to vocal strain. This system was later extended to a six-camera motion-capture setup to simultaneously study (a) the underside of the chin and anterior neck and (b) anterior and posterior thoracoabdominal regions, enabling analysis of paralaryngeal-respiratory motor coordination on a spatiotemporal scale.</p>  <p>The findings from this research have been incorporated into both undergraduate and graduate courses taught by the PI at UT Dallas. Graduate students, including one woman PhD student, were directly involved in this project. Additionally, five undergraduate students supported through REU supplements received rigorous training in 4D reconstruction and human avatar generation. Each summer, the PI hosts two outreach activities as part of this project: (1) supervising research by Clark Scholars -- National Merit Scholars committed to UT Dallas and members of the Hobson Wildenthal Honors College -- and (2) hosting high school students for the&nbsp;CAST-STEM Bridge Summer Camp, a six-week program offering hands-on research experience. Clark Scholars under the PI's mentorship received the&nbsp;CS Honors Presentation Award&nbsp;in 2022, while high school participants earned&nbsp;1st Prize CAST STAR Awards&nbsp;in both 2021 and 2024.</p>  <p>&nbsp;</p><br> <p>  Last Modified: 12/06/2024<br> Modified by: Xiaohu&nbsp;Guo</p></div> <div class="porSideCol" ></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[  The intellectual merits of this project focus on the sophisticated modeling of topological changes in dynamic scenes and the robust reconstruction of moving surfaces, with the ultimate goal of developing an open-source 4D reconstruction framework for real-time capture and generation of dynamic humans and their surrounding objects.    To address the challenges of topological changes, the volumetric fusion framework and its underlying data structures were fundamentally redesigned. We introducedNon-manifold Volumetric Gridsinto both the Truncated Signed Distance Field (TSDF) and Embedded Deformation Graph (EDG) representations. This innovation allows volumetric cells to replicate and edges to break, enabling a topology-change-aware framework that dynamically updates the connectivity of reconstructed mesh geometry. Simultaneously, a flexible deformation graph adapts its connectivity between nodes throughout the 4D capture process.    For modeling the human body alongside multi-layer garments, we developed a 3D human body and garment dataset and introduced a novel multi-layer garment reconstruction algorithm. Using purchased 3D models from Renderpeople and AXYZ Design, we performed SMPL fitting to estimate intrinsic body shapes and poses, extracting garment geometries and textures. This dataset enabled the creation ofLayered-Garment-Net (LGN), a reconstruction algorithm capable of generating intersection-free, multiple garment layers defined by implicit function fields over the body surface. By leveraging specially designedGarment Indication Fields (GIF), LGN enforces implicit covering relationships among the signed distance fields (SDFs) of different garment layers, avoiding self-intersections between garments and the body.    This project has broader impacts on several applications spanning from automatic generation of talking avatars, to the study of vocal strain and paralaryngeal-respiratory motor coordination in those voice disorder patients.    One key application of 4D human reconstruction is the generation of talking avatars. Given an audio input, the system generates 3D or 2D (video) avatars with personalized pose dynamics, facial expressions, synchronized lip movements, and natural eye blinks. To achieve this, we built an original, person-specific, dynamic 3D talking dataset by analyzing "in-the-wild" videos. This dataset captures dynamic head poses, eye movements, lip synchronization, and 3D face models for each frame in an automated manner. Based on this dataset, we developedFACIAL-GAN (FACe Implicit Attribute Learning Generative Adversarial Network), which integrates phonetic, contextual, and identity information to synthesize 3D face animations with realistic lip motions, head poses, and eye blinks. Additionally, we proposed a data-efficientDisentangled Recurrent Representation (DR&sup2;)learning method, which can generate photorealistic talking videos, complete with full-body gestures, using just 2--5 minutes of training video. Furthermore, we unified co-speech gesture and talking head generation within a single model architecture, proposing a novel diffusion-based framework with shared weights across modalities and adapters for adaptation to a common latent space.    This project also has interdisciplinary applications. For example, we applied our 4D skin motion tracking algorithms to develop askin deformation metricfor analyzing muscle activation patterns in individuals with voice disorders. In collaboration with Dr. Adrianna Shembel from the School of Behavioral and Brain Sciences at UT-Dallas, we used consumer RGB-D cameras to track skin deformation in the perilaryngeal anterior neck regions of participants with and without vocal strain. Differences in neck movement variability between the two groups provided insights into extrinsic laryngeal vocal muscles contributing to vocal strain. This system was later extended to a six-camera motion-capture setup to simultaneously study (a) the underside of the chin and anterior neck and (b) anterior and posterior thoracoabdominal regions, enabling analysis of paralaryngeal-respiratory motor coordination on a spatiotemporal scale.    The findings from this research have been incorporated into both undergraduate and graduate courses taught by the PI at UT Dallas. Graduate students, including one woman PhD student, were directly involved in this project. Additionally, five undergraduate students supported through REU supplements received rigorous training in 4D reconstruction and human avatar generation. Each summer, the PI hosts two outreach activities as part of this project: (1) supervising research by Clark Scholars -- National Merit Scholars committed to UT Dallas and members of the Hobson Wildenthal Honors College -- and (2) hosting high school students for theCAST-STEM Bridge Summer Camp, a six-week program offering hands-on research experience. Clark Scholars under the PI's mentorship received theCS Honors Presentation Awardin 2022, while high school participants earned1st Prize CAST STAR Awardsin both 2021 and 2024.         Last Modified: 12/06/2024       Submitted by: XiaohuGuo]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
