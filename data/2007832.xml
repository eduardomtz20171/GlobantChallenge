<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle><![CDATA[SHF: Small: Dynamic Gating and Adaptation of Deep Neural Networks for Efficient Inference and Training]]></AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>06/15/2020</AwardEffectiveDate>
<AwardExpirationDate>05/31/2024</AwardExpirationDate>
<AwardTotalIntnAmount>500000.00</AwardTotalIntnAmount>
<AwardAmount>500000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Sankar Basu</SignBlockName>
<PO_EMAI>sabasu@nsf.gov</PO_EMAI>
<PO_PHON>7032927843</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The past half-decade has seen unprecedented growth in machine learning with deep neural networks (DNNs), which now represent the state-of-the-art in many AI applications. However, existing DNN models require substantial memory and computing power, which greatly limit their use in resource-constrained systems such as mobile and IoT devices. This project will develop new algorithms and hardware to significantly improve the efficiency of DNNs, and represents an important step towards enabling fast and adaptive DNN executions even in resource-limited environments. In that sense, this project has the potential to enable a wider deployment of machine learning, which will play a critical role in many aspects of the future smart society. The research project will provide research training opportunities to the students as well as new curriculum development by leveraging existing resources at Cornell, e.g., summer camps as well as an outreach programs for high-school students including women.&lt;br/&gt;&lt;br/&gt;This project aims to significantly improve the efficiency of DNNs, while maintaining high accuracy, by co-developing algorithm optimizations and  an efficient hardware-accelerator architecture. While there exist many lines of work on reducing DNN execution costs, the majority of these techniques are designed primarily to improve inference and perform static optimizations that reduce computation uniformly for all inputs or only exploit a limited form of dynamic sparsity, namely zeros. This project aims to enable new performance-accuracy trade-off points for DNNs that are not possible today by exploiting general forms of dynamic sparsity that are specific to each input at run-time. More specifically, the project plans to investigate input-specific gating techniques that can remove redundant computations for both training and inference, develop dynamic quantization techniques that do not require training data, and design an efficient and unified hardware accelerator architecture that provides both real-world performance and energy improvements.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>06/05/2020</MinAmdLetterDate>
<MaxAmdLetterDate>06/05/2020</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2007832</AwardID>
<Investigator>
<FirstName>Gookwon</FirstName>
<LastName>Suh</LastName>
<PI_MID_INIT>E</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Gookwon E Suh</PI_FULL_NAME>
<EmailAddress><![CDATA[gs272@cornell.edu]]></EmailAddress>
<NSF_ID>000104889</NSF_ID>
<StartDate>06/05/2020</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Zhiru</FirstName>
<LastName>Zhang</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Zhiru Zhang</PI_FULL_NAME>
<EmailAddress><![CDATA[zhiruz@cornell.edu]]></EmailAddress>
<NSF_ID>000631474</NSF_ID>
<StartDate>06/05/2020</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name><![CDATA[Cornell University]]></Name>
<CityName>ITHACA</CityName>
<ZipCode>148502820</ZipCode>
<PhoneNumber>6072555014</PhoneNumber>
<StreetAddress><![CDATA[341 PINE TREE RD]]></StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>19</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY19</CONGRESS_DISTRICT_ORG>
<ORG_UEI_NUM>G56PUALJ3KT5</ORG_UEI_NUM>
<ORG_LGL_BUS_NAME>CORNELL UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_UEI_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[Cornell University]]></Name>
<CityName>Ithaca</CityName>
<StateCode>NY</StateCode>
<ZipCode>148535401</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>19</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY19</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>287800</Code>
<Text>Special Projects - CCF</Text>
</ProgramElement>
<ProgramElement>
<Code>779800</Code>
<Text>Software &amp; Hardware Foundation</Text>
</ProgramElement>
<ProgramReference>
<Code>075Z</Code>
<Text>Artificial Intelligence (AI)</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7945</Code>
<Text>DES AUTO FOR MICRO &amp; NANO SYST</Text>
</ProgramReference>
<Appropriation>
<Code>0120</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Fund>
<Code>01002021DB</Code>
<Name><![CDATA[NSF RESEARCH & RELATED ACTIVIT]]></Name>
<FUND_SYMB_ID>040100</FUND_SYMB_ID>
</Fund>
<FUND_OBLG>2020~500000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span id="docs-internal-guid-8d29e4e1-7fff-7f9a-1bfe-49c2b037bca9"> <p dir="ltr"><span>Deep neural networks (DNNs) are reshaping many aspects of our lives by enabling computers to better understand/generate contents such as pictures, videos, and texts and providing more personalized experiences for individuals. However, DNN models require significant amounts of computational power. Moreover, improving the security, robustness, interpretability, and fairness of the DNN models requires even more computational power. The project investigated a new direction to improve the efficiency of DNNs while still maintaining high accuracy by leveraging characteristics that are specific to individual inputs at run-time. The new technologies developed from this project has a potential to enable DNNs to be more widely deployed even in resource-constrained devices and also reduce the costs of improving robustness and security of DNN models.&nbsp;</span></p> <p dir="ltr"><span>The project developed a set of new dynamic run-time optimization techniques, which can significantly improve the efficiency of DNNs. For DNN inference, which makes a prediction or generates content using a DNN model, the project found that the cost of computation can be significantly reduced by carefully adopting single-bit (binary) operations instead of traditional floating-point operations on multi-bit (for example, 32-bit) values. Surprisingly, this project showed that the accuracy of DNN models can be maintained even with single-bit operations. For example, FracBNN introduced an accurate and efficient binary neural network (BNN) architecture through dynamic quantization, which automatically chooses the number of bits needed for different parts of the DNN for individual inputs. PokeBNN further improved the accuracy-efficiency trade-off through new binary convolutions and careful design space explorations. Finally, this project showed that binarization can be applied to Transformer models in machine translation (MT) to significantly reduce the size of the model while maintaining the quality of floating-point models. For the first time, these results showed that binarized models can match the accuracy of floating-point models for both computer vision and natural language processing.</span></p> <p dir="ltr"><span>In addition to binarization, this project also found that dynamic optimizations can significantly reduce the cost of improving robustness and security of machine learning. Leveraging the observation that only a small subset of training examples are important in improving the robustness of a model, BulletTrain significantly improved training time to make models more robust against both malicious and corrupted inputs. The project also led to development of efficient hardware-based protection for DNN models. GuardNN and MGX showed that DNN models can be run inside a secure accelerator with almost no overhead by carefully optimizing memory encryption and verification techniques based on a DNN model&rsquo;s access pattern. Similarly, we also showed that the protection overhead of running a DNN model inside a CPU-based trusted execution environment (TEE) can be significantly improved through an algorithm-protection co-design.</span></p> <p dir="ltr"><span>For broader impacts, this project trained graduate (PhD and MEng) and undergraduate students, including several from underrepresented minority groups. The project disseminated its research outcomes through publications, external talks, and multiple open-source ML models. It also contributed to graduate-level teaching materials and diversity initiatives at Cornell University.</span></p> </span></p><br> <p>  Last Modified: 10/17/2024<br> Modified by: Gookwon&nbsp;E&nbsp;Suh</p></div> <div class="porSideCol" ></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[     Deep neural networks (DNNs) are reshaping many aspects of our lives by enabling computers to better understand/generate contents such as pictures, videos, and texts and providing more personalized experiences for individuals. However, DNN models require significant amounts of computational power. Moreover, improving the security, robustness, interpretability, and fairness of the DNN models requires even more computational power. The project investigated a new direction to improve the efficiency of DNNs while still maintaining high accuracy by leveraging characteristics that are specific to individual inputs at run-time. The new technologies developed from this project has a potential to enable DNNs to be more widely deployed even in resource-constrained devices and also reduce the costs of improving robustness and security of DNN models.   The project developed a set of new dynamic run-time optimization techniques, which can significantly improve the efficiency of DNNs. For DNN inference, which makes a prediction or generates content using a DNN model, the project found that the cost of computation can be significantly reduced by carefully adopting single-bit (binary) operations instead of traditional floating-point operations on multi-bit (for example, 32-bit) values. Surprisingly, this project showed that the accuracy of DNN models can be maintained even with single-bit operations. For example, FracBNN introduced an accurate and efficient binary neural network (BNN) architecture through dynamic quantization, which automatically chooses the number of bits needed for different parts of the DNN for individual inputs. PokeBNN further improved the accuracy-efficiency trade-off through new binary convolutions and careful design space explorations. Finally, this project showed that binarization can be applied to Transformer models in machine translation (MT) to significantly reduce the size of the model while maintaining the quality of floating-point models. For the first time, these results showed that binarized models can match the accuracy of floating-point models for both computer vision and natural language processing.   In addition to binarization, this project also found that dynamic optimizations can significantly reduce the cost of improving robustness and security of machine learning. Leveraging the observation that only a small subset of training examples are important in improving the robustness of a model, BulletTrain significantly improved training time to make models more robust against both malicious and corrupted inputs. The project also led to development of efficient hardware-based protection for DNN models. GuardNN and MGX showed that DNN models can be run inside a secure accelerator with almost no overhead by carefully optimizing memory encryption and verification techniques based on a DNN models access pattern. Similarly, we also showed that the protection overhead of running a DNN model inside a CPU-based trusted execution environment (TEE) can be significantly improved through an algorithm-protection co-design.   For broader impacts, this project trained graduate (PhD and MEng) and undergraduate students, including several from underrepresented minority groups. The project disseminated its research outcomes through publications, external talks, and multiple open-source ML models. It also contributed to graduate-level teaching materials and diversity initiatives at Cornell University.      Last Modified: 10/17/2024       Submitted by: GookwonESuh]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
