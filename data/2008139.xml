<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle><![CDATA[III: Small: Fairness and Control of Exposure in Ranking]]></AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2020</AwardEffectiveDate>
<AwardExpirationDate>09/30/2024</AwardExpirationDate>
<AwardTotalIntnAmount>496762.00</AwardTotalIntnAmount>
<AwardAmount>496762</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Sylvia Spengler</SignBlockName>
<PO_EMAI>sspengle@nsf.gov</PO_EMAI>
<PO_PHON>7032927347</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Ranking functions trained via machine learning are ubiquitous in todayâ€™s online systems, where they are used to rank virtually anything - from products and movies to job candidates. By deciding where individual items get ranked and how easily users can find them, the ranking function greatly influences which products get purchased, which candidates get a job, and which movies get streamed. This raises questions of fairness, asserting that the ranking functions should be fair to both the users of the systems as well as to the items being ranked. The project develops new fairness criteria for ranking functions, as well as new methods for designing and learning ranking functions with fairness guarantees.&lt;br/&gt;&lt;br/&gt;The project is based on a model of ranking systems as two-sided markets, where utility goes not only to the users issuing the queries, but also to the items that are being ranked. Unfortunately, virtually all learning-to-rank (LTR) methods in use today only optimize the average utility to the users, which can lead to unfair treatment of the items and of minority user groups. To overcome this deficiency, the project develops LTR methods that can enforce desirable fairness constraints. These new methods can remedy disparate treatment of user groups (e.g. amplification of gender bias in a hiring system), market concentration in online markets, and the dynamics of participation in online systems (e.g. polarization). To achieve these goals, the project addresses both endogenous and exogenous causes of unfairness. Exogenous causes are due to biases in the training data, which often lead to rich-get-richer dynamics. However, even when trained with unbiased data, causes endogenous in the design of the LTR algorithm can lead to unfairness. Therefore, the LTR methods developed in the project address both endogenous and exogenous causes.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>09/04/2020</MinAmdLetterDate>
<MaxAmdLetterDate>10/19/2020</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2008139</AwardID>
<Investigator>
<FirstName>Thorsten</FirstName>
<LastName>Joachims</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Thorsten Joachims</PI_FULL_NAME>
<EmailAddress><![CDATA[tj@cs.cornell.edu]]></EmailAddress>
<NSF_ID>000224646</NSF_ID>
<StartDate>09/04/2020</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name><![CDATA[Cornell University]]></Name>
<CityName>ITHACA</CityName>
<ZipCode>148502820</ZipCode>
<PhoneNumber>6072555014</PhoneNumber>
<StreetAddress><![CDATA[341 PINE TREE RD]]></StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>19</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY19</CONGRESS_DISTRICT_ORG>
<ORG_UEI_NUM>G56PUALJ3KT5</ORG_UEI_NUM>
<ORG_LGL_BUS_NAME>CORNELL UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_UEI_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[Cornell University]]></Name>
<CityName>ITHACA</CityName>
<StateCode>NY</StateCode>
<ZipCode>148537501</ZipCode>
<StreetAddress><![CDATA[107 Hoy Road]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>19</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY19</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>736400</Code>
<Text>Info Integration &amp; Informatics</Text>
</ProgramElement>
<ProgramReference>
<Code>7364</Code>
<Text>INFO INTEGRATION &amp; INFORMATICS</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0120</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Fund>
<Code>01002021DB</Code>
<Name><![CDATA[NSF RESEARCH & RELATED ACTIVIT]]></Name>
<FUND_SYMB_ID>040100</FUND_SYMB_ID>
</Fund>
<FUND_OBLG>2020~496762</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span id="docs-internal-guid-7e849a64-7fff-cada-5367-b89843b962c7"> </span></p>  <p dir="ltr"><span>Learned ranking functions are ubiquitous in today's online systems, where they are used to rank virtually anything -- from products and movies to job candidates and potential romantic partners. In these ranking applications, it is evident that utility goes not only to the users issuing the queries, but also to the items that are being ranked. This makes these systems two-sided markets where, from the items' perspective, the ranking function takes the role of an arbiter of exposure to the users. In this way, the ranking function has control over the economic fortunes of the items, greatly influencing which products get purchased, which candidates get a job, and which movies get streamed.&nbsp;</span></p>  <p dir="ltr"><span>This view of ranking functions as matchmakers in a two-sided market implies that they have responsibilities not only to their users but also to the items being ranked. In particular, there is growing recognition and legal demand that ranking functions need to be fair in how they allocate exposure based on the merit of the items. Unfortunately, virtually all conventional learning-to-rank (LTR) methods consider utility to the users as the only criterion to optimize as part of their training objective. It is thus not surprising that this one-sided objective can lead to unfairness among the items, and the proposal argues that it can lead to undesirable market dynamics more generally.</span></p>  <p dir="ltr"><span>In order to develop ranking methods that can accurately consider the tradeoffs between the utilities of all stakeholders, the project addressed both endogenous and exogenous causes of unfairness. Exogenous causes are due to biases in the training data, which often lead to rich-get-richer dynamics. However, even when trained with unbiased data, causes endogenous in the design of the LTR algorithm can lead to unfairness. Therefore, the LTR methods developed in the project address both endogenous and exogenous causes.</span></p>  <p dir="ltr"><span>In particular, the project developed methods and new understanding of biases along the following directions. First, the project developed new ranking methods for multi-sided markets and matching markets, where the rankings provide different types of utility to different sides of the market. Defining fairness-of-exposure as a fairness criterion for user-provider markets, the new ranking methods were able to tie exposure to the merit of providers and their items, and the project made connections to axiomatic definitions of fairness and Nash social welfare. Second, a line of research on rankings in high-stakes decision making investigated how rankings act as a screening mechanism in a human decision-making process. Focusing on college admission as a representative application domain, the project investigated fairness issues in standard ranking approaches, in two-stage ranking pipelines, and in ranking problems that have diversity constraints. Finally, the project investigated biases in GenAI systems, and how current systems may serve some user groups better than others.&nbsp;</span></p>  <p>&nbsp;</p><br> <p>  Last Modified: 12/06/2024<br> Modified by: Thorsten&nbsp;Joachims</p></div> <div class="porSideCol" ></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[       Learned ranking functions are ubiquitous in today's online systems, where they are used to rank virtually anything -- from products and movies to job candidates and potential romantic partners. In these ranking applications, it is evident that utility goes not only to the users issuing the queries, but also to the items that are being ranked. This makes these systems two-sided markets where, from the items' perspective, the ranking function takes the role of an arbiter of exposure to the users. In this way, the ranking function has control over the economic fortunes of the items, greatly influencing which products get purchased, which candidates get a job, and which movies get streamed.    This view of ranking functions as matchmakers in a two-sided market implies that they have responsibilities not only to their users but also to the items being ranked. In particular, there is growing recognition and legal demand that ranking functions need to be fair in how they allocate exposure based on the merit of the items. Unfortunately, virtually all conventional learning-to-rank (LTR) methods consider utility to the users as the only criterion to optimize as part of their training objective. It is thus not surprising that this one-sided objective can lead to unfairness among the items, and the proposal argues that it can lead to undesirable market dynamics more generally.    In order to develop ranking methods that can accurately consider the tradeoffs between the utilities of all stakeholders, the project addressed both endogenous and exogenous causes of unfairness. Exogenous causes are due to biases in the training data, which often lead to rich-get-richer dynamics. However, even when trained with unbiased data, causes endogenous in the design of the LTR algorithm can lead to unfairness. Therefore, the LTR methods developed in the project address both endogenous and exogenous causes.    In particular, the project developed methods and new understanding of biases along the following directions. First, the project developed new ranking methods for multi-sided markets and matching markets, where the rankings provide different types of utility to different sides of the market. Defining fairness-of-exposure as a fairness criterion for user-provider markets, the new ranking methods were able to tie exposure to the merit of providers and their items, and the project made connections to axiomatic definitions of fairness and Nash social welfare. Second, a line of research on rankings in high-stakes decision making investigated how rankings act as a screening mechanism in a human decision-making process. Focusing on college admission as a representative application domain, the project investigated fairness issues in standard ranking approaches, in two-stage ranking pipelines, and in ranking problems that have diversity constraints. Finally, the project investigated biases in GenAI systems, and how current systems may serve some user groups better than others.         Last Modified: 12/06/2024       Submitted by: ThorstenJoachims]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
