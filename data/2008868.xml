<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle><![CDATA[AF: Small: Rehabilitating Constants in Sublinear Algorithms]]></AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2020</AwardEffectiveDate>
<AwardExpirationDate>06/30/2024</AwardExpirationDate>
<AwardTotalIntnAmount>500000.00</AwardTotalIntnAmount>
<AwardAmount>500000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Peter Brass</SignBlockName>
<PO_EMAI>pbrass@nsf.gov</PO_EMAI>
<PO_PHON>7032922182</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The scale of data produced in the world is growing faster than the&lt;br/&gt;ability to process it.  One approach for dealing with this data deluge&lt;br/&gt;involves sublinear algorithms, which are designed to estimate some&lt;br/&gt;feature of data without needing to store, or sometimes even see, the&lt;br/&gt;entire data set.  Sublinear algorithms include distribution testing&lt;br/&gt;(e.g., estimating if a lottery is fair or not), streaming algorithms&lt;br/&gt;(e.g., finding the most common URLs on the web), and property testing&lt;br/&gt;(e.g., estimating the maximum degree of a network).  For these&lt;br/&gt;problems, computer scientists have carefully studied how the&lt;br/&gt;complexity of the solution grows with the problem parameters---for&lt;br/&gt;example, estimating if a lottery is fair requires a number of draws&lt;br/&gt;that scales with the square root of the number of possible numbers&lt;br/&gt;drawn.  But results so far have not been able to analyze the solution&lt;br/&gt;complexity for concrete instances (e.g., for a birthday lottery with&lt;br/&gt;366 possible numbers, how many samples are necessary to verify&lt;br/&gt;fairness?).  This project aims to change that, by finding solutions&lt;br/&gt;with not only good asymptotic scaling, but good constant factors.&lt;br/&gt;&lt;br/&gt;Developing sublinear algorithms with good constant factors will&lt;br/&gt;require new algorithmic techniques.  The sublinear-algorithms&lt;br/&gt;literature is based on several widespread techniques like probability&lt;br/&gt;amplification that are simple, general, and optimal up to constant&lt;br/&gt;factors---and significantly suboptimal in their constant factors.  By&lt;br/&gt;replacing these techniques with more fine-grained ones, this project&lt;br/&gt;aims to develop new algorithms with better performance in practice.&lt;br/&gt;This project also aims to empirically measure the worst-case&lt;br/&gt;performance of algorithms, by identifying which input distributions&lt;br/&gt;are provably hardest to solve.  By testing different algorithms in&lt;br/&gt;practice, the project will discover and compare the actual impact of&lt;br/&gt;different algorithmic choices.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>06/17/2020</MinAmdLetterDate>
<MaxAmdLetterDate>06/17/2020</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2008868</AwardID>
<Investigator>
<FirstName>Eric</FirstName>
<LastName>Price</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Eric Price</PI_FULL_NAME>
<EmailAddress><![CDATA[ecprice@cs.utexas.edu]]></EmailAddress>
<NSF_ID>000700457</NSF_ID>
<StartDate>06/17/2020</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name><![CDATA[University of Texas at Austin]]></Name>
<CityName>AUSTIN</CityName>
<ZipCode>787121139</ZipCode>
<PhoneNumber>5124716424</PhoneNumber>
<StreetAddress><![CDATA[110 INNER CAMPUS DR]]></StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<StateCode>TX</StateCode>
<CONGRESSDISTRICT>25</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>TX25</CONGRESS_DISTRICT_ORG>
<ORG_UEI_NUM>V6AFQPN18437</ORG_UEI_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF TEXAS AT AUSTIN</ORG_LGL_BUS_NAME>
<ORG_PRNT_UEI_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[The University of Texas at Austin]]></Name>
<CityName>Austin</CityName>
<StateCode>TX</StateCode>
<ZipCode>787121810</ZipCode>
<StreetAddress><![CDATA[2317 Speedway]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>25</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>TX25</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>779600</Code>
<Text>Algorithmic Foundations</Text>
</ProgramElement>
<ProgramReference>
<Code>079Z</Code>
<Text>Machine Learning Theory</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7926</Code>
<Text>ALGORITHMS</Text>
</ProgramReference>
<ProgramReference>
<Code>7934</Code>
<Text>PARAL/DISTRIBUTED ALGORITHMS</Text>
</ProgramReference>
<Appropriation>
<Code>0120</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Fund>
<Code>01002021DB</Code>
<Name><![CDATA[NSF RESEARCH & RELATED ACTIVIT]]></Name>
<FUND_SYMB_ID>040100</FUND_SYMB_ID>
</Fund>
<FUND_OBLG>2020~500000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The field of sublinear algorithms has historically ignored constant<br />factors.&nbsp; This has led to a proliferation of techniques that are<br />simple, easy, and terrible in practice.&nbsp; In this project we examined<br />constant factors in sample and query complexity, developing more<br />efficient techniques that demonstrably work better in practice.<br /><br />Much of the project focused on estimating the average (i.e., the<br />arithmetic mean) of a distribution from samples.&nbsp; The sample average<br />is a fairly good estimator of the distribution average, but it turns<br />out that one can do better.&nbsp; First, the sample average is very<br />sensitive to outliers, which can make it unreliable.&nbsp; Second, the<br />sample average is not always efficient -- for example, for the Laplace<br />distribution the sample median takes 30% fewer samples than the sample<br />mean.&nbsp; There are classical statistics solutions to either one of these<br />issues in isolation, but not to both at the same time.&nbsp; We developed<br />techniques that can handle both issues in a variety of settings, such<br />as estimating known distributions in multiple dimensions or symmetric<br />distributions in one dimension.&nbsp; We also showed how to get better<br />constants for the generic setting of estimating bounded-covariance<br />distributions in high dimension.<br /><br />In addition to mean estimation, we also gave new algorithms with<br />better constants and better practical performance for noisy binary<br />search and uniformity testing.&nbsp; Uniformity testing addresses questions<br />like: how many rolls of a many-sided die does it take to tell if it is<br />unbiased?&nbsp; We showed that a classical statistics method -- the Pearson<br />chi-square test -- often outperforms more modern methods that were<br />developed in theoretical computer science without regard for constant<br />factors; we then gave a new method that combines the advantages of<br />each approach.</p><br> <p>  Last Modified: 10/29/2024<br> Modified by: Eric&nbsp;Price</p></div> <div class="porSideCol" ></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[  The field of sublinear algorithms has historically ignored constant factors. This has led to a proliferation of techniques that are simple, easy, and terrible in practice. In this project we examined constant factors in sample and query complexity, developing more efficient techniques that demonstrably work better in practice.  Much of the project focused on estimating the average (i.e., the arithmetic mean) of a distribution from samples. The sample average is a fairly good estimator of the distribution average, but it turns out that one can do better. First, the sample average is very sensitive to outliers, which can make it unreliable. Second, the sample average is not always efficient -- for example, for the Laplace distribution the sample median takes 30% fewer samples than the sample mean. There are classical statistics solutions to either one of these issues in isolation, but not to both at the same time. We developed techniques that can handle both issues in a variety of settings, such as estimating known distributions in multiple dimensions or symmetric distributions in one dimension. We also showed how to get better constants for the generic setting of estimating bounded-covariance distributions in high dimension.  In addition to mean estimation, we also gave new algorithms with better constants and better practical performance for noisy binary search and uniformity testing. Uniformity testing addresses questions like: how many rolls of a many-sided die does it take to tell if it is unbiased? We showed that a classical statistics method -- the Pearson chi-square test -- often outperforms more modern methods that were developed in theoretical computer science without regard for constant factors; we then gave a new method that combines the advantages of each approach.     Last Modified: 10/29/2024       Submitted by: EricPrice]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
