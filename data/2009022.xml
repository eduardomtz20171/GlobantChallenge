<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle><![CDATA[SHF: Small: Omega-Regular Objectives for Model-Free Reinforcement Learning]]></AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>06/15/2020</AwardEffectiveDate>
<AwardExpirationDate>05/31/2024</AwardExpirationDate>
<AwardTotalIntnAmount>500000.00</AwardTotalIntnAmount>
<AwardAmount>500000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Pavithra Prabhakar</SignBlockName>
<PO_EMAI>pprabhak@nsf.gov</PO_EMAI>
<PO_PHON>7032922585</PO_PHON>
</ProgramOfficer>
<AbstractNarration>In Reinforcement Learning (RL) agents rely on rewards that promote the achievement of given objectives.  Widespread use of RL-enabled systems, such as swarm robots, autonomous vehicles, Internet-of-Things, and social networks, will dramatically improve the quality of modern life.  However, their applications in safety-critical settings imply that methods to ensure their correctness are of paramount importance. This project develops a rigorous approach to the design and verification of RL-enabled systems that addresses issues of safety, efficiency, and scalability. Logic provides a foundation for the rigorous specification of learning objectives.  Model-free RL, which is the type of learning supported by neural networks, promises scalability. Hence this project is about translating logic-based requirements into the scalar reward form that is needed in model-free RL. &lt;br/&gt;&lt;br/&gt;Bridging the gap between logic specifications and model-free RL requires a translation that is faithful (greater reward means higher probability of satisfying the objective) and effective (the reward should help RL algorithms to learn quickly and reliably). This project develops foundations for faithful and effective translations of omega-regular specifications and explores their applications to synthesis of RL-enabled systems.  The transition from theory to practice will be measured by the success of an open-source tool for the synthesis of interpreters that translate environment observations into rewards for state-of-the-art, off-the-shelf RL algorithms. Both the formal-methods and the RL communities will benefit from this project. The PIs will extend their record of technology transfer with the release of software and educational materials.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>06/03/2020</MinAmdLetterDate>
<MaxAmdLetterDate>06/03/2020</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2009022</AwardID>
<Investigator>
<FirstName>Ashutosh</FirstName>
<LastName>Trivedi</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Ashutosh Trivedi</PI_FULL_NAME>
<EmailAddress><![CDATA[ashutosh.trivedi@colorado.edu]]></EmailAddress>
<NSF_ID>000724243</NSF_ID>
<StartDate>06/03/2020</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Fabio</FirstName>
<LastName>Somenzi</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Fabio Somenzi</PI_FULL_NAME>
<EmailAddress><![CDATA[Fabio@Colorado.EDU]]></EmailAddress>
<NSF_ID>000415741</NSF_ID>
<StartDate>06/03/2020</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name><![CDATA[University of Colorado at Boulder]]></Name>
<CityName>Boulder</CityName>
<ZipCode>803090001</ZipCode>
<PhoneNumber>3034926221</PhoneNumber>
<StreetAddress><![CDATA[3100 MARINE ST]]></StreetAddress>
<StreetAddress2><![CDATA[STE 481 572 UCB]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Colorado</StateName>
<StateCode>CO</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CO02</CONGRESS_DISTRICT_ORG>
<ORG_UEI_NUM>SPVKK1RC2MZ3</ORG_UEI_NUM>
<ORG_LGL_BUS_NAME>THE REGENTS OF THE UNIVERSITY OF COLORADO</ORG_LGL_BUS_NAME>
<ORG_PRNT_UEI_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Colorado at Boulder]]></Name>
<CityName/>
<StateCode>CO</StateCode>
<ZipCode>803031058</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Colorado</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CO02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>287800</Code>
<Text>Special Projects - CCF</Text>
</ProgramElement>
<ProgramElement>
<Code>779800</Code>
<Text>Software &amp; Hardware Foundation</Text>
</ProgramElement>
<ProgramReference>
<Code>075Z</Code>
<Text>Artificial Intelligence (AI)</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>8206</Code>
<Text>Formal Methods and Verification</Text>
</ProgramReference>
<Appropriation>
<Code>0120</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Fund>
<Code>01002021DB</Code>
<Name><![CDATA[NSF RESEARCH & RELATED ACTIVIT]]></Name>
<FUND_SYMB_ID>040100</FUND_SYMB_ID>
</Fund>
<FUND_OBLG>2020~500000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p id="docs-internal-guid-0351b534-7fff-989d-f0f7-317b66fa344a" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;" dir="ltr"><span style="font-family: Arial, sans-serif; font-size: 11pt; white-space: pre-wrap;">Reinforcement learning (RL) is a sequential optimization approach inspired by how dopamine-driven organisms latch on to past rewarding actions. &nbsp; In RL, a decision maker learns to optimally resolve a sequence of choices based on feedback received from the environment.&nbsp; This feedback often takes the form of rewards and punishments proportional to the fitness of the agent's decisions as judged by the environment towards some higher-level objectives.&nbsp; We call such objectives learning objectives.</span></p> <p style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;" dir="ltr">&nbsp;</p> <p style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;" dir="ltr"><span style="font-family: Arial, sans-serif; font-size: 11pt; white-space: pre-wrap;">&nbsp;</span><span style="font-family: Arial, sans-serif; font-size: 11pt; white-space: pre-wrap;">In some applications of RL, the translation from objective to rewards is straightforward. For example, when the agent is trying to minimize the consumption of battery charge. In other scenarios, the translation is far from obvious, such as when the agent's mission is to patrol several locations with constraints on the order of visit while avoiding other locations. Moreover, the agent may have multiple objectives with different priorities.</span></p> <p style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;" dir="ltr">&nbsp;</p> <p style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;" dir="ltr"><span style="font-family: Arial, sans-serif; font-size: 11pt; white-space: pre-wrap;">&nbsp;</span><span style="font-family: Arial, sans-serif; font-size: 11pt; white-space: pre-wrap;">For safety-critical system design, learning objectives must be expressed in a rigorous and intuitive fashion.&nbsp; The objective of this project was to provide formalisms and techniques to express requirements in such a rigorous manner for omega-regular specifications, which include the popular Linear-Time Logic (LTL).&nbsp;</span></p> <p style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;" dir="ltr">&nbsp;</p> <p style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;" dir="ltr"><span style="font-family: Arial, sans-serif; font-size: 11pt; white-space: pre-wrap;">&nbsp;</span><span style="font-family: Arial, sans-serif; font-size: 11pt; white-space: pre-wrap;">Building on the investigators' prior work, published in 2019 at TACAS, which introduced the Limit Reachability approach (the first correct solution to the translation of omega-regular objectives), this project explored three dimensions of the application of RL to safety-critical systems.&nbsp;</span></p> <p style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;" dir="ltr">&nbsp;</p> <p style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;" dir="ltr"><span style="font-family: Arial, sans-serif; font-size: 11pt; white-space: pre-wrap;">&nbsp;</span><span style="font-family: Arial, sans-serif; font-size: 11pt; font-weight: bold; white-space: pre-wrap;">1. Model-Free RL for finite MDP Environments against Omega-Regular Objectives. </span><span style="font-family: Arial, sans-serif; font-size: 11pt; white-space: pre-wrap;">This dimension of the project was concerned with the <em>faithful</em> (maximizing reward means maximizing the probability of achieving the objective) and <em>effective</em> (RL quickly converges to optimal strategies) translation </span><span style="font-family: Arial, sans-serif; font-size: 11pt; white-space: pre-wrap;">of omega-regular objectives into scalar rewards for off-the-shelf, model-free RL algorithms.&nbsp; It explored the type of automata that can be used for that task, developed translations for Markov Decision Processes and stochastic games, and studied the sample efficiency of learning omega-regular objectives.&nbsp; The details may be found in these publications.</span></p> <p style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;" dir="ltr"><span style="font-family: Arial, sans-serif; font-size: 11pt; white-space: pre-wrap;"><br /></span></p> <ul> <li><span style="font-size: 11pt; font-family: Arial,sans-serif; color: #666666; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">Model-Free Reinforcement Learning for Stochastic Parity Games (CONCUR 20).</span><span style="font-size: 11pt; font-family: Arial,sans-serif; color: #505b62; background-color: #ffffff; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">&nbsp;</span></li> <li><span style="color: #666666; font-family: Arial, sans-serif; font-size: 11pt; font-weight: bold; white-space: pre-wrap;">Faithful and Effective Reward Schemes for Model-Free Reinforcement Learning of Omega-Regular Objectives (ATVA 20).</span></li> <li><span style="color: #666666; font-family: Arial, sans-serif; font-size: 11pt; font-weight: bold; white-space: pre-wrap;">An Impossibility Result in Automata-Theoretic Reinforcement Learning (ATVA 22).</span></li> <li><span style="color: #666666; font-family: Arial, sans-serif; font-size: 11pt; font-weight: bold; white-space: pre-wrap;">Good-for-MDPs Automata for Probabilistic Analysis and Reinforcement Learning (TACAS 20).</span></li> <li><span style="color: #666666; font-family: Arial, sans-serif; font-size: 11pt; font-weight: bold; white-space: pre-wrap;">Alternating Good-for-MDPs Automata (ATVA 22).</span></li> <li><span style="color: #666666; font-family: Arial, sans-serif; font-size: 11pt; font-weight: bold; white-space: pre-wrap;">A PAC Learning Algorithm for LTL and Omega-Regular Objectives in MDPs (AAAI 24).</span></li> </ul> <p><span style="font-family: Arial, sans-serif; font-size: 11pt; font-weight: bold; white-space: pre-wrap;">2. More Expressive Objectives. </span><span style="font-family: Arial, sans-serif; font-size: 11pt; white-space: pre-wrap;">Omega-regular objectives have long been used to express qualitative, long-term requirements.&nbsp; In practice, though, these objectives often coexist with quantitative specifications.&nbsp; Multiple objectives may be present with different priorities.&nbsp; In the second dimension of this project, these issues were addressed by developing algorithms that allow users to specify varied and complex requirements in an intuitive, yet rigorous way. The details may be found in these publications.</span><span style="color: #666666; font-family: Arial, sans-serif; font-size: 11pt; font-weight: bold; white-space: pre;">&nbsp;</span></p> <ol style="margin-top: 0; margin-bottom: 0; padding-inline-start: 48px;"> </ol>  <ul> <li><span style="color: #666666; font-family: Arial, sans-serif; font-size: 11pt; font-weight: bold; white-space: pre-wrap;">Model-Free Reinforcement Learning for Lexicographic Omega-Regular Objectives (FM 21).</span></li> <li><span style="color: #666666; font-family: Arial, sans-serif; font-size: 11pt; font-weight: bold; white-space: pre-wrap;">Multi-objective &omega;-Regular Reinforcement Learning (FAC 23).</span></li> <li><span style="color: #666666; font-family: Arial, sans-serif; font-size: 11pt; font-weight: bold; white-space: pre-wrap;">Translating Omega-Regular Specifications to Average Objectives for Model-Free Reinforcement Learning (AAMAS 22).</span></li> <li><span style="color: #666666; font-family: Arial, sans-serif; font-size: 11pt; font-weight: bold; white-space: pre-wrap;">Omega-Regular Reward Machines (ECAI 23).</span></li> <li><span style="color: #666666; font-family: Arial, sans-serif; font-size: 11pt; font-weight: bold; white-space: pre-wrap;">Omega-Regular Decision Processes (AAAI 24).</span></li> <li><span style="color: #666666; font-family: Arial, sans-serif; font-size: 11pt; font-weight: bold; white-space: pre-wrap;">Policy Synthesis and Reinforcement Learning for Discounted LTL (CAV 23).</span></li> </ul> <p style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;" dir="ltr">&nbsp;</p> <p style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;" dir="ltr"><span style="font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">3. More Expressive Environments. </span><span style="font-size: 11pt; font-weight: 400; white-space: pre-wrap;">Markov Decision Processes are the classic formalism for finite-state environments.&nbsp; While finite-state abstractions of infinite systems may be effective, in applications, it is often necessary to directly deal with various forms of infinite-state environments.&nbsp; In the third dimension of this project, extensions of RL algorithms have been developed.&nbsp; These extensions target both continuous state-spaces and more expressive models of environments, like those that allow recursive specifications. The details may be found in these publications.</span></p> <p style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;" dir="ltr"><span style="font-size: 11pt; font-weight: 400; white-space: pre-wrap;"><br /></span></p> <ul> <li><span style="color: #666666; font-family: Arial, sans-serif; font-size: 11pt; font-weight: bold; white-space: pre-wrap;">Formal Controller Synthesis for Continuous-Space MDPs via Model-Free Reinforcement Learning (ICCPS 20).</span></li> <li><span style="color: #666666; font-family: Arial, sans-serif; font-size: 11pt; font-weight: bold; white-space: pre-wrap;">Reinforcement Learning for Omega-Regular Specifications on Continuous-Time MDP (ICAPS 23).</span></li> <li><span style="color: #666666; font-family: Arial, sans-serif; font-size: 11pt; font-weight: bold; white-space: pre-wrap;">Model-Free Reinforcement Learning for Branching Markov Decision Processes (CAV 21).</span></li> <li><span style="color: #666666; font-family: Arial, sans-serif; font-size: 11pt; font-weight: bold; white-space: pre-wrap;">Recursive Reinforcement Learning (Neurips 22).</span></li> <li><span style="color: #666666; font-family: Arial, sans-serif; font-size: 11pt; font-weight: bold; white-space: pre-wrap;">Assume-Guarantee Reinforcement Learning (AAAI 24).&nbsp;</span></li> <li><span style="color: #666666; font-family: Arial, sans-serif; font-size: 11pt; font-weight: bold; white-space: pre-wrap;">Regular Reinforcement Learning (CAV 24).</span></li> </ul> <p><span style="font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">Experimental Evaluation.</span><span style="font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;"> Mungojerrie (</span><a style="text-decoration: none;" href="https://plv.colorado.edu/mungojerrie"><span style="font-size: 11pt; font-family: Arial,sans-serif; color: #1155cc; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: underline; text-decoration-skip-ink: none; vertical-align: baseline; white-space: pre-wrap;">https://plv.colorado.edu/mungojerrie</span></a><span style="font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">) is an extensible tool that provides a framework to translate linear-time objectives into reward for RL. The tool provides convergent RL algorithms for stochastic games, reference implementations of existing reward translations for omega-regular objectives, and an internal probabilistic model checker for omega-regular objectives. Mungojerrie is distributed with a set of benchmarks for omega-regular objectives in RL.&nbsp; The tool has been adopted by researchers at several universities and it has been used in the classroom setting to teach the principles of verification and synthesis of reactive systems in uncertain environments.</span></p> <ul> <li><span style="color: #666666; font-family: Arial, sans-serif; font-size: 11pt; font-weight: bold; white-space: pre-wrap;">Mungojerrie: Linear-Time Objectives in Model-Free Reinforcement Learning (TACAS 23).</span></li> </ul> <p><span style="font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">With Machine Learning</span><span style="font-size: 11pt; font-family: Arial,sans-serif; color: #666666; background-color: #ffffff; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;"> </span><span style="font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">being increasingly adopted in the design of safety-critical systems, the results of this project provide a much-needed foundation for a rigorous design methodology.&nbsp; The investigators have strived to disseminate the results of their research to both the formal methods and the AI community to maximize impact and adoption.</span></p> <p>&nbsp;</p><br> <p>  Last Modified: 08/05/2024<br> Modified by: Ashutosh&nbsp;Trivedi</p></div> <div class="porSideCol" ></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[  Reinforcement learning (RL) is a sequential optimization approach inspired by how dopamine-driven organisms latch on to past rewarding actions.  In RL, a decision maker learns to optimally resolve a sequence of choices based on feedback received from the environment. This feedback often takes the form of rewards and punishments proportional to the fitness of the agent's decisions as judged by the environment towards some higher-level objectives. We call such objectives learning objectives.      In some applications of RL, the translation from objective to rewards is straightforward. For example, when the agent is trying to minimize the consumption of battery charge. In other scenarios, the translation is far from obvious, such as when the agent's mission is to patrol several locations with constraints on the order of visit while avoiding other locations. Moreover, the agent may have multiple objectives with different priorities.      For safety-critical system design, learning objectives must be expressed in a rigorous and intuitive fashion. The objective of this project was to provide formalisms and techniques to express requirements in such a rigorous manner for omega-regular specifications, which include the popular Linear-Time Logic (LTL).      Building on the investigators' prior work, published in 2019 at TACAS, which introduced the Limit Reachability approach (the first correct solution to the translation of omega-regular objectives), this project explored three dimensions of the application of RL to safety-critical systems.      1. Model-Free RL for finite MDP Environments against Omega-Regular Objectives. This dimension of the project was concerned with the faithful (maximizing reward means maximizing the probability of achieving the objective) and effective (RL quickly converges to optimal strategies) translation of omega-regular objectives into scalar rewards for off-the-shelf, model-free RL algorithms. It explored the type of automata that can be used for that task, developed translations for Markov Decision Processes and stochastic games, and studied the sample efficiency of learning omega-regular objectives. The details may be found in these publications.      Model-Free Reinforcement Learning for Stochastic Parity Games (CONCUR 20). Faithful and Effective Reward Schemes for Model-Free Reinforcement Learning of Omega-Regular Objectives (ATVA 20). An Impossibility Result in Automata-Theoretic Reinforcement Learning (ATVA 22). Good-for-MDPs Automata for Probabilistic Analysis and Reinforcement Learning (TACAS 20). Alternating Good-for-MDPs Automata (ATVA 22). A PAC Learning Algorithm for LTL and Omega-Regular Objectives in MDPs (AAAI 24).    2. More Expressive Objectives. Omega-regular objectives have long been used to express qualitative, long-term requirements. In practice, though, these objectives often coexist with quantitative specifications. Multiple objectives may be present with different priorities. In the second dimension of this project, these issues were addressed by developing algorithms that allow users to specify varied and complex requirements in an intuitive, yet rigorous way. The details may be found in these publications.     Model-Free Reinforcement Learning for Lexicographic Omega-Regular Objectives (FM 21). Multi-objective -Regular Reinforcement Learning (FAC 23). Translating Omega-Regular Specifications to Average Objectives for Model-Free Reinforcement Learning (AAMAS 22). Omega-Regular Reward Machines (ECAI 23). Omega-Regular Decision Processes (AAAI 24). Policy Synthesis and Reinforcement Learning for Discounted LTL (CAV 23).       3. More Expressive Environments. Markov Decision Processes are the classic formalism for finite-state environments. While finite-state abstractions of infinite systems may be effective, in applications, it is often necessary to directly deal with various forms of infinite-state environments. In the third dimension of this project, extensions of RL algorithms have been developed. These extensions target both continuous state-spaces and more expressive models of environments, like those that allow recursive specifications. The details may be found in these publications.      Formal Controller Synthesis for Continuous-Space MDPs via Model-Free Reinforcement Learning (ICCPS 20). Reinforcement Learning for Omega-Regular Specifications on Continuous-Time MDP (ICAPS 23). Model-Free Reinforcement Learning for Branching Markov Decision Processes (CAV 21). Recursive Reinforcement Learning (Neurips 22). Assume-Guarantee Reinforcement Learning (AAAI 24). Regular Reinforcement Learning (CAV 24).    Experimental Evaluation. Mungojerrie (https://plv.colorado.edu/mungojerrie) is an extensible tool that provides a framework to translate linear-time objectives into reward for RL. The tool provides convergent RL algorithms for stochastic games, reference implementations of existing reward translations for omega-regular objectives, and an internal probabilistic model checker for omega-regular objectives. Mungojerrie is distributed with a set of benchmarks for omega-regular objectives in RL. The tool has been adopted by researchers at several universities and it has been used in the classroom setting to teach the principles of verification and synthesis of reactive systems in uncertain environments.  Mungojerrie: Linear-Time Objectives in Model-Free Reinforcement Learning (TACAS 23).    With Machine Learning being increasingly adopted in the design of safety-critical systems, the results of this project provide a much-needed foundation for a rigorous design methodology. The investigators have strived to disseminate the results of their research to both the formal methods and the AI community to maximize impact and adoption.        Last Modified: 08/05/2024       Submitted by: AshutoshTrivedi]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
