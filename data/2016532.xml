<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle><![CDATA[CCRI: Planning: A Community-Standard, Large-Scale Synthetic 3D Scene Dataset for Scene Analysis and Synthesis]]></AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2020</AwardEffectiveDate>
<AwardExpirationDate>03/31/2024</AwardExpirationDate>
<AwardTotalIntnAmount>50000.00</AwardTotalIntnAmount>
<AwardAmount>50000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jie Yang</SignBlockName>
<PO_EMAI>jyang@nsf.gov</PO_EMAI>
<PO_PHON>7032924768</PO_PHON>
</ProgramOfficer>
<AbstractNarration>To function as useful household assistants, robots need to understand what they are seeing and how to navigate in indoor environments. The current state-of-the-art approaches for solving these problems rely on machine learning, and in particular deep learning, which requires large quantities of labeled data (e.g. many images with per-pixel labels indicating what type of object is present at that pixel). Rather than asking people to laboriously label data captured from real-world spaces, a promising alternative approach is to use *synthetic* 3D scenes: virtual 3D models of indoor spaces. The 3D objects which populate these virtual spaces can be equipped with information such as their object type, which allows large sets of labeled training data to be created essentially “for free.” This project aims to construct *the* community-standard, large-scale synthetic 3D scene dataset. While some synthetic 3D scene datasets exist, they are either too small, or they have been subject to onerous use restrictions (and even lawsuits) due to copyright issues on their 3D models, which typically come from for-profit companies. This project will construct a large-scale dataset out of freely-available 3D content. The main contribution of the project is not just this dataset, but also a *scalable pipeline* for creating such 3D scene datasets. This pipeline will be released as open source, allowing others to expand the dataset or to construct their own datasets for needs which may be difficult to anticipate today. In total, the results of this project will enable any researcher (not just those at heavily-resourced institutions) to build AI systems which leverage large-scale synthetic indoor training data.&lt;br/&gt;&lt;br/&gt;The planned dataset construction pipeline will construct 3D scenes based on 2D floor plan datasets, which already exist at large scale. Using a machine-learning-based system previously developed by the investigators, these 2D floor plans will be converted to 3D models of empty houses. Then, each room in the house will be populated with objects in a plausible arrangement. Initially, this step will be performed by crowd workers on a platform such as Amazon Mechanical Turk. The workers will be instructed to place objects so as to match a photograph, where the photograph is chosen such that its (estimated) room geometry matches the geometry of the empty room to be populated. In a later stage of the project, rooms populated in this manner will be used to train a machine learning model which can automatically place objects based on an input photograph, thus further accelerating the dataset construction process.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>08/11/2020</MinAmdLetterDate>
<MaxAmdLetterDate>08/11/2020</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2016532</AwardID>
<Investigator>
<FirstName>Daniel</FirstName>
<LastName>Ritchie</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Daniel Ritchie</PI_FULL_NAME>
<EmailAddress><![CDATA[daniel_ritchie@brown.edu]]></EmailAddress>
<NSF_ID>000737205</NSF_ID>
<StartDate>08/11/2020</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name><![CDATA[Brown University]]></Name>
<CityName>PROVIDENCE</CityName>
<ZipCode>029129100</ZipCode>
<PhoneNumber>4018632777</PhoneNumber>
<StreetAddress><![CDATA[1 PROSPECT ST]]></StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Rhode Island</StateName>
<StateCode>RI</StateCode>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>RI01</CONGRESS_DISTRICT_ORG>
<ORG_UEI_NUM>E3FDXZ6TBHW3</ORG_UEI_NUM>
<ORG_LGL_BUS_NAME>BROWN UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_UEI_NUM>E3FDXZ6TBHW3</ORG_PRNT_UEI_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Brown University]]></Name>
<CityName>Providence</CityName>
<StateCode>RI</StateCode>
<ZipCode>029129002</ZipCode>
<StreetAddress><![CDATA[115 Waterman St.]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Rhode Island</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>RI01</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>735900</Code>
<Text>CCRI-CISE Cmnty Rsrch Infrstrc</Text>
</ProgramElement>
<ProgramReference>
<Code>7359</Code>
<Text>COMPUTING RES INFRASTRUCTURE</Text>
</ProgramReference>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<Appropriation>
<Code>0120</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Fund>
<Code>01002021DB</Code>
<Name><![CDATA[NSF RESEARCH & RELATED ACTIVIT]]></Name>
<FUND_SYMB_ID>040100</FUND_SYMB_ID>
</Fund>
<FUND_OBLG>2020~50000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This project led to the creation of a new dataset for visual computing (computer graphics &amp; computer vision) research, as well as the development of new tools for efficiently creating this type of dataset. The dataset consists of a large number of panoramic images of indoor residential environments (e.g. bedrooms, living rooms, etc.) Each panorama is linked to a 3D version of the scene in which that panorama was taken. These 3D scenes consist of a set of 3D objects, each of which is precisely chosen, placed, and aligned such that it accurately mirrors one of the objects in the panorama.&nbsp;</p> <p>This type of dataset can support the development of machine-learning-based visual computing systems that must reason about the structure of 3D scenes, especially systems that must make predictions about 3D scene structure from imagery. We found that on the task of panoramic scene understanding (estimating the architectural layout of a room and the objects in it from a panorama), systems trained on our dataset performed better when applied to real-world photographic panoramas than systems trained on previously-available datasets for the same task.</p> <p>This dataset is applicable to other problems in computer vision and graphics, as well as to problems in robotics involving training indoor autonomous agents. In addition, the software tools we used to create this dataset could help other researchers more efficiently construct similar datasets of their own in the future.</p><br> <p>  Last Modified: 06/26/2024<br> Modified by: Daniel&nbsp;Ritchie</p></div> <div class="porSideCol" ><div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Image         </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation onePhoto" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2024/2016532/2016532_10695694_1719448463818_Screenshot_2024_06_26_at_8.26.15_PM--rgov-214x142.png" original="/por/images/Reports/POR/2024/2016532/2016532_10695694_1719448463818_Screenshot_2024_06_26_at_8.26.15_PM--rgov-800width.png" title="Dataset Overview"><img src="/por/images/Reports/POR/2024/2016532/2016532_10695694_1719448463818_Screenshot_2024_06_26_at_8.26.15_PM--rgov-66x44.png" alt="Dataset Overview"></a> <div class="imageCaptionContainer"> <div class="imageCaption">An overview of the features provided by our new 3D scene dataset.</div> <div class="imageCredit">Daniel Ritchie</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Daniel&nbsp;Ritchie <div class="imageTitle">Dataset Overview</div> </div> </li></ul> </div> </div></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[  This project led to the creation of a new dataset for visual computing (computer graphics & computer vision) research, as well as the development of new tools for efficiently creating this type of dataset. The dataset consists of a large number of panoramic images of indoor residential environments (e.g. bedrooms, living rooms, etc.) Each panorama is linked to a 3D version of the scene in which that panorama was taken. These 3D scenes consist of a set of 3D objects, each of which is precisely chosen, placed, and aligned such that it accurately mirrors one of the objects in the panorama.   This type of dataset can support the development of machine-learning-based visual computing systems that must reason about the structure of 3D scenes, especially systems that must make predictions about 3D scene structure from imagery. We found that on the task of panoramic scene understanding (estimating the architectural layout of a room and the objects in it from a panorama), systems trained on our dataset performed better when applied to real-world photographic panoramas than systems trained on previously-available datasets for the same task.   This dataset is applicable to other problems in computer vision and graphics, as well as to problems in robotics involving training indoor autonomous agents. In addition, the software tools we used to create this dataset could help other researchers more efficiently construct similar datasets of their own in the future.     Last Modified: 06/26/2024       Submitted by: DanielRitchie]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
