<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle><![CDATA[CCRI: ENS: Collaborative Research: Open Computer System Usage Repository and Analytics Engine]]></AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2020</AwardEffectiveDate>
<AwardExpirationDate>09/30/2023</AwardExpirationDate>
<AwardTotalIntnAmount>1183897.00</AwardTotalIntnAmount>
<AwardAmount>1183897</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Marilyn McClure</SignBlockName>
<PO_EMAI>mmcclure@nsf.gov</PO_EMAI>
<PO_PHON>7032925197</PO_PHON>
</ProgramOfficer>
<AbstractNarration>In science and engineering research, large-scale, centrally managed computing clusters or “supercomputers” have been instrumental in enabling the kinds of resource-intensive simulations, analyses, and visualizations that have been used in computer-aided drug discovery, high strength materials design for cars and jet engines, and disease vector analysis to name a few. Such clusters are complex systems comprised of several hundred to thousand computer servers with fast network connections between them, various data storage resources, and highly optimized scientific software being shared with several hundred other researchers from diverse domains. Consequently, the overall dependability of such systems relies on the dependability of these individual highly interconnected elements as well as the characteristics of cascading failures. While computer systems researchers and practitioners have been at the forefront of designing and deploying dependable computing cluster systems, this task has been hampered by the lack of publicly available, real-world failure data from supercomputers currently in operation. Prior practice has largely involved tedious, manual collection and curation of small sets of data for use in specific analyses. This project will establish seamless, automated pipelines for acquiring, processing, and curating continuous, detailed system usage, monitoring, and failure data from large computing clusters at two organizations, Purdue University and the University of Texas at Austin. This data will be disseminated through a publicly accessible portal and complemented by a suite of in-situ analytics capabilities that will support and spur research in dependable computing systems. The data acquisition pipeline and analytics software will be made open-source and designed for ease of federation, extension, and adoption to cluster systems operated by other organizations.&lt;br/&gt;&lt;br/&gt;Cluster computing systems are a key resource in time-sensitive, computationally intensive research such as virus structure modeling and drug discovery and have been at the forefront of efforts to tackle global pandemics. Both unanticipated system down-times and lack of actionable feedback to researchers on computational failures can have adverse effects on research timeliness and efficiency. This project will allow the practitioners and administrators of these systems to develop data-backed best practices for ensuring high availability and utilization for their clusters. The resulting large, public data repository consisting of data from clusters with diverse workloads spanning traditional high-performance computing, modern accelerator-based computing (for example on graphics processing units (GPUs)), and cloud-style applications will allow the systems research community to consider forward-looking research questions based on real system data. The project will train a cadre of students in data analysis on live production systems and this will provide them with a unique learning experience, interfacing with a variety of stakeholders.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>08/15/2020</MinAmdLetterDate>
<MaxAmdLetterDate>11/16/2021</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2016704</AwardID>
<Investigator>
<FirstName>Xiaohui Carol</FirstName>
<LastName>Song</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Xiaohui Carol Song</PI_FULL_NAME>
<EmailAddress><![CDATA[cxsong@purdue.edu]]></EmailAddress>
<NSF_ID>000298986</NSF_ID>
<StartDate>08/15/2020</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Saurabh</FirstName>
<LastName>Bagchi</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Saurabh Bagchi</PI_FULL_NAME>
<EmailAddress><![CDATA[sbagchi@purdue.edu]]></EmailAddress>
<NSF_ID>000309372</NSF_ID>
<StartDate>08/15/2020</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Rajesh</FirstName>
<LastName>Kalyanam</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Rajesh Kalyanam</PI_FULL_NAME>
<EmailAddress><![CDATA[rkalyana@purdue.edu]]></EmailAddress>
<NSF_ID>000754242</NSF_ID>
<StartDate>08/15/2020</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Stephen</FirstName>
<LastName>Harrell</LastName>
<PI_MID_INIT>L</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Stephen L Harrell</PI_FULL_NAME>
<EmailAddress><![CDATA[sharrell@tacc.utexas.edu]]></EmailAddress>
<NSF_ID>000798073</NSF_ID>
<StartDate>08/15/2020</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Amiya</FirstName>
<LastName>Maji</LastName>
<PI_MID_INIT>K</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Amiya K Maji</PI_FULL_NAME>
<EmailAddress><![CDATA[amaji@purdue.edu]]></EmailAddress>
<NSF_ID>000847079</NSF_ID>
<StartDate>11/16/2021</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name><![CDATA[Purdue University]]></Name>
<CityName>WEST LAFAYETTE</CityName>
<ZipCode>479061332</ZipCode>
<PhoneNumber>7654941055</PhoneNumber>
<StreetAddress><![CDATA[2550 NORTHWESTERN AVE # 1100]]></StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Indiana</StateName>
<StateCode>IN</StateCode>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IN04</CONGRESS_DISTRICT_ORG>
<ORG_UEI_NUM>YRXVL4JYCEF5</ORG_UEI_NUM>
<ORG_LGL_BUS_NAME>PURDUE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_UEI_NUM>YRXVL4JYCEF5</ORG_PRNT_UEI_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Purdue University]]></Name>
<CityName>West Lafayette</CityName>
<StateCode>IN</StateCode>
<ZipCode>479072035</ZipCode>
<StreetAddress><![CDATA[465 Northwestern Avenue]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Indiana</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IN04</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>735900</Code>
<Text>CCRI-CISE Cmnty Rsrch Infrstrc</Text>
</ProgramElement>
<ProgramReference>
<Code>7359</Code>
<Text>COMPUTING RES INFRASTRUCTURE</Text>
</ProgramReference>
<Appropriation>
<Code>0120</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Fund>
<Code>01002021DB</Code>
<Name><![CDATA[NSF RESEARCH & RELATED ACTIVIT]]></Name>
<FUND_SYMB_ID>040100</FUND_SYMB_ID>
</Fund>
<FUND_OBLG>2020~1183897</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><strong id="docs-internal-guid-d71ba7c2-7fff-6d69-da5b-0c1a42e4df8c" style="font-weight: normal;"> </strong></p> <p style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;" dir="ltr"><strong id="docs-internal-guid-d71ba7c2-7fff-6d69-da5b-0c1a42e4df8c" style="font-weight: normal;"><span style="font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">The FRESCO project made progress on the systematic collection, curation, and presentation of public failure data pertinent to large-scale computing systems, all of which is consolidated in a repository named FRESCO. Originating from Purdue, U of Illinois at Urbana-Champaign, and U of Texas at Austin, the datasets encapsulate both static and dynamic information, encompassing system usage, workloads, and failure data, applicable to both planned and unplanned outages. The systems are operational central computing clusters at these universities and consequently, they see a wide variety of workloads from all different science and engineering domains. The systems have different kinds of performance characteristics and loads from the workloads stress them to different extents. Our data illuminates this intricate relationship between workload request patterns and health of the computing clusters. Further, it sheds light on the impact of planned maintenance operations, including upgrades, on the health of the computing clusters.&nbsp;</span></strong></p> <p><strong id="docs-internal-guid-d71ba7c2-7fff-6d69-da5b-0c1a42e4df8c" style="font-weight: normal;"> <p style="line-height: 1.38; margin-top: 10pt; margin-bottom: 0pt;" dir="ltr"><span style="font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">In a broader context, this rich and well-curated dataset aims to benefit researchers, technologists, and data scientists in navigating the complexities and challenges inherent in managing and maintaining robust computing infrastructures. This endeavor not only facilitates a deeper understanding of system failures but also propels further research and development in the realm of dependable computing systems.</span></p> <p style="line-height: 1.38; margin-top: 10pt; margin-bottom: 0pt;" dir="ltr"><span style="font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">Further, the project created an analytics toolbench that would demonstrate analytics queries that are commonly useful&nbsp; in such situations, e.g., what is the mean time to failure of a certain range of machines, or what is the correlation between load of a particular resource (say memory) and performance degradation of a computing node. The analytics toolbench provides users with the ability through simple user interface to create a wide variety of analytics queries. The analytics queries can span a spatial range (sets of compute nodes) as well as a temporal range (windows of time).&nbsp;</span></p> <p style="line-height: 1.38; margin-top: 10pt; margin-bottom: 0pt;" dir="ltr"><span style="font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">The project had a key focus of making the asset widely usable, to broad classes of stakeholders. With this aim, we organized Birds of Feather gatherings at the relevant conferences (Supercomputing, DSN). We also created explanatory videos showing the usage of the dataset and the analytics toolbench. We plan to continue this line of work, bringing in more partners as data providers plus encouraging usage of the asset by the broad classes of users.&nbsp;</span></p> <p style="line-height: 1.38; margin-top: 10pt; margin-bottom: 0pt;" dir="ltr"><span style="font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">In a related effort that comes under the purview of this project, we worked with commercial entities that have production workloads on the cloud to release cloud computing traces. One of this is for serverless workflows on the Microsoft Azure cloud and the other is resource utilization for Adobe&rsquo;s use on the Azure cloud.&nbsp;</span></p> </strong><br class="Apple-interchange-newline" /></p> <p>&nbsp;</p><br> <p>  Last Modified: 03/17/2024<br> Modified by: Saurabh&nbsp;Bagchi</p></div> <div class="porSideCol" ><div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)          </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2024/2016704/2016704_10697752_1710732099954_serverless_dag_invocation_frequency--rgov-214x142.png" original="/por/images/Reports/POR/2024/2016704/2016704_10697752_1710732099954_serverless_dag_invocation_frequency--rgov-800width.png" title="Serverless DAG invocation"><img src="/por/images/Reports/POR/2024/2016704/2016704_10697752_1710732099954_serverless_dag_invocation_frequency--rgov-66x44.png" alt="Serverless DAG invocation"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Serverless DAG invocation frequency (Blue solid) and its impact on the % cold starts (Red dashed). DAGs with low invocation rate (e.g., once/day) always experience a cold start, whereas very frequent DAGs (e.g., 500 invocations per day) have very rare cold starts. Data from Microsoft Azu</div> <div class="imageCredit">Saurabh Bagchi, Ashraf Mahgoub, Sameh Elnikety</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Saurabh&nbsp;Bagchi <div class="imageTitle">Serverless DAG invocation</div> </div> </li><li> <a href="/por/images/Reports/POR/2024/2016704/2016704_10697752_1710732456046_rakesh_fresco--rgov-214x142.jpg" original="/por/images/Reports/POR/2024/2016704/2016704_10697752_1710732456046_rakesh_fresco--rgov-800width.jpg" title="Submitted application job characteristics"><img src="/por/images/Reports/POR/2024/2016704/2016704_10697752_1710732456046_rakesh_fresco--rgov-66x44.jpg" alt="Submitted application job characteristics"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Characterisitcs of jobs running on System A (in brown, left in each subfigure) and System B (in green, right in each subfigure)</div> <div class="imageCredit">Saurabh Bagchi, Rakesh Kumar, Rajesh Kalyanam</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Saurabh&nbsp;Bagchi <div class="imageTitle">Submitted application job characteristics</div> </div> </li><li> <a href="/por/images/Reports/POR/2024/2016704/2016704_10697752_1710731821385_fresco_poster_1123--rgov-214x142.jpg" original="/por/images/Reports/POR/2024/2016704/2016704_10697752_1710731821385_fresco_poster_1123--rgov-800width.jpg" title="FRESCO poster"><img src="/por/images/Reports/POR/2024/2016704/2016704_10697752_1710731821385_fresco_poster_1123--rgov-66x44.jpg" alt="FRESCO poster"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Poster highlighting the latest status of the resource</div> <div class="imageCredit">Saurabh Bagchi, Aryaman Dhomne, Josh McKerracher</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Saurabh&nbsp;Bagchi <div class="imageTitle">FRESCO poster</div> </div> </li></ul> </div> </div></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[      The FRESCO project made progress on the systematic collection, curation, and presentation of public failure data pertinent to large-scale computing systems, all of which is consolidated in a repository named FRESCO. Originating from Purdue, U of Illinois at Urbana-Champaign, and U of Texas at Austin, the datasets encapsulate both static and dynamic information, encompassing system usage, workloads, and failure data, applicable to both planned and unplanned outages. The systems are operational central computing clusters at these universities and consequently, they see a wide variety of workloads from all different science and engineering domains. The systems have different kinds of performance characteristics and loads from the workloads stress them to different extents. Our data illuminates this intricate relationship between workload request patterns and health of the computing clusters. Further, it sheds light on the impact of planned maintenance operations, including upgrades, on the health of the computing clusters.      In a broader context, this rich and well-curated dataset aims to benefit researchers, technologists, and data scientists in navigating the complexities and challenges inherent in managing and maintaining robust computing infrastructures. This endeavor not only facilitates a deeper understanding of system failures but also propels further research and development in the realm of dependable computing systems.   Further, the project created an analytics toolbench that would demonstrate analytics queries that are commonly useful in such situations, e.g., what is the mean time to failure of a certain range of machines, or what is the correlation between load of a particular resource (say memory) and performance degradation of a computing node. The analytics toolbench provides users with the ability through simple user interface to create a wide variety of analytics queries. The analytics queries can span a spatial range (sets of compute nodes) as well as a temporal range (windows of time).   The project had a key focus of making the asset widely usable, to broad classes of stakeholders. With this aim, we organized Birds of Feather gatherings at the relevant conferences (Supercomputing, DSN). We also created explanatory videos showing the usage of the dataset and the analytics toolbench. We plan to continue this line of work, bringing in more partners as data providers plus encouraging usage of the asset by the broad classes of users.   In a related effort that comes under the purview of this project, we worked with commercial entities that have production workloads on the cloud to release cloud computing traces. One of this is for serverless workflows on the Microsoft Azure cloud and the other is resource utilization for Adobes use on the Azure cloud.          Last Modified: 03/17/2024       Submitted by: SaurabhBagchi]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
