<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle><![CDATA[Collaborative Research: Identifying Reproducible Research Using Human-in-the-loop Machine Learning]]></AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2020</AwardEffectiveDate>
<AwardExpirationDate>08/31/2023</AwardExpirationDate>
<AwardTotalIntnAmount>155741.00</AwardTotalIntnAmount>
<AwardAmount>155741</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>04010000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>SMA</Abbreviation>
<LongName>SBE Off Of Multidisciplinary Activities</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Mary Feeney</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>Research quality thrives under healthy skepticism where scientists retest hypotheses creating higher levels of confidence in the original findings and a sturdy foundation for extending work. Recent attempts to sample scientific research in psychology, economics, and medicine however have shown that more scientific papers fail than pass manual replication tests. Consequently, several attempts have been made to find ways to efficiently extend replication studies, including new statistics, surveys, and prediction markets. However new statistics have been very slowly adopted and the high costs associated with surveys and prediction markets makes these methods impractical for estimating the reproducibility of more than a few hundred studies out of the vast stock of millions of research papers that are used as building blocks for current and future work. The proposed research aims to develop metrics and tools to help make replication studies of existing work more efficient with one additional benefit: to help scientists, scholars, and technologists self-evaluate their work before publishing it. &lt;br/&gt;&lt;br/&gt;This proposal combines efforts to create new datasets, ‘reproducibility’ metrics, and machine learning models that estimate a confidence level in the reproducibility of a published work. The deliverables will include new datasets covering the success and failure of hundreds of scientific papers in psychology and economics and their related subfields. The metrics will go beyond a binary classification of whether a publication is estimated to be reproducible or not. They will quantify a level of confidence that the work is likely to be reproducible. The machine learning models will also help scientists interpret and explain confidence scores, which aid scientists in learning about the factors that correlate with reproducibility. In all three areas, the project will aim to provide scientists with better tools to evaluate the reproducibility of their own and others’ work, creating a better foundation of knowledge for advancing research.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>09/08/2020</MinAmdLetterDate>
<MaxAmdLetterDate>10/13/2020</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2022443</AwardID>
<Investigator>
<FirstName>David</FirstName>
<LastName>Koop</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>David A Koop</PI_FULL_NAME>
<EmailAddress><![CDATA[dakoop@niu.edu]]></EmailAddress>
<NSF_ID>000632177</NSF_ID>
<StartDate>09/08/2020</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Hamed</FirstName>
<LastName>Alhoori</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Hamed Alhoori</PI_FULL_NAME>
<EmailAddress><![CDATA[alhoori@niu.edu]]></EmailAddress>
<NSF_ID>000744735</NSF_ID>
<StartDate>09/08/2020</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name><![CDATA[Northern Illinois University]]></Name>
<CityName>DEKALB</CityName>
<ZipCode>601152828</ZipCode>
<PhoneNumber>8157531581</PhoneNumber>
<StreetAddress><![CDATA[1425 W LINCOLN HWY]]></StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<StateCode>IL</StateCode>
<CONGRESSDISTRICT>14</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IL14</CONGRESS_DISTRICT_ORG>
<ORG_UEI_NUM>M2EEE68GGCY9</ORG_UEI_NUM>
<ORG_LGL_BUS_NAME>NORTHERN ILLINOIS UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_UEI_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[Northern Illinois University]]></Name>
<CityName>DeKalb</CityName>
<StateCode>IL</StateCode>
<ZipCode>601152828</ZipCode>
<StreetAddress><![CDATA[301 Lowden Hall]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>14</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IL14</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>125Y00</Code>
<Text>Science of Science</Text>
</ProgramElement>
<ProgramReference>
<Code>7626</Code>
<Text>SCIENCE OF SCIENCE POLICY</Text>
</ProgramReference>
<Appropriation>
<Code>0120</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Fund>
<Code>01002021DB</Code>
<Name><![CDATA[NSF RESEARCH & RELATED ACTIVIT]]></Name>
<FUND_SYMB_ID>040100</FUND_SYMB_ID>
</Fund>
<FUND_OBLG>2020~155741</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Scientific research generates new insights of our world and also informs and influences legislation, regulation, and public policy. However, the "reproducibility crisis," which costs billions annually, has fostered skepticism of research outcomes across various domains. It is important for research to be reproducible, allowing its findings to be independently verified by other researchers. Research that is not reproducible can lead to misunderstandings and poor decision-making by individuals and policymakers.&nbsp;<br /><br />This project has resulted in a number of discoveries and advancements in the area of reproducibility. It examined, quantified, and evaluated the scholarship around reproducibility and developed new datasets and models.<br /><br />Our team developed datasets for reproducibility from different sources. This collection includes research papers classified as reproducible or non-reproducible, studies on reproducibility, reports on reproducibility efforts, and computational notebooks.&nbsp;<br /><br />We examined various studies and artifacts related to reproducibility and determined some important aspects surrounding the practice of reproducibility. This included examining efforts to reproduce scientific studies and sharing those outcomes. While some scientists may reproduce published studies as they work on their own studies, others conduct reproducibility attempts specifically to validate results in the field. These efforts range from large-scale reproducibility studies to brief reports or sections within reviews used to evaluate research or award a reproducibility badge.<br /><br />We analyzed reproducibility studies and reports to extract insights. This process allowed us to understand the common challenges in reproducing scientific work, to recognize the effort required from scientists in evaluating reproducibility, and to develop models that prioritize reproducible research. By analyzing the original research papers and reproducibility studies, we were able to identify key factors that contribute to reproducible science.<br /><br />For computational notebooks, such as Jupyter, we have worked to expand the methods used to evaluate reproducibility to allow for more nuanced comparison. We found that different computing environment configurations play a role in producing results that communicate the same insights but are not exactly the same. Sometimes these differences are minor and clearly cosmetic (e.g., a change in the font used for text in a figure), but other times, they are complex and harder to evaluate (e.g., a small difference in a numerical output could be due to roundoff error but could also point to a problem in the code). In addition, we have developed approaches like Dataflow Notebooks that seek to remove some of the ambiguity in reproducing the work in a computational notebook.<br /><br />The project outcomes have been disseminated widely across academic venues and online platforms. The insights and findings have been shared with hundreds of undergraduate and graduate students through their courses, enriching their education and emphasizing the importance of reproducible research.&nbsp;</p><br> <p>  Last Modified: 12/30/2023<br> Modified by: Hamed&nbsp;Alhoori</p></div> <div class="porSideCol" ></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[  Scientific research generates new insights of our world and also informs and influences legislation, regulation, and public policy. However, the "reproducibility crisis," which costs billions annually, has fostered skepticism of research outcomes across various domains. It is important for research to be reproducible, allowing its findings to be independently verified by other researchers. Research that is not reproducible can lead to misunderstandings and poor decision-making by individuals and policymakers.  This project has resulted in a number of discoveries and advancements in the area of reproducibility. It examined, quantified, and evaluated the scholarship around reproducibility and developed new datasets and models.  Our team developed datasets for reproducibility from different sources. This collection includes research papers classified as reproducible or non-reproducible, studies on reproducibility, reports on reproducibility efforts, and computational notebooks.  We examined various studies and artifacts related to reproducibility and determined some important aspects surrounding the practice of reproducibility. This included examining efforts to reproduce scientific studies and sharing those outcomes. While some scientists may reproduce published studies as they work on their own studies, others conduct reproducibility attempts specifically to validate results in the field. These efforts range from large-scale reproducibility studies to brief reports or sections within reviews used to evaluate research or award a reproducibility badge.  We analyzed reproducibility studies and reports to extract insights. This process allowed us to understand the common challenges in reproducing scientific work, to recognize the effort required from scientists in evaluating reproducibility, and to develop models that prioritize reproducible research. By analyzing the original research papers and reproducibility studies, we were able to identify key factors that contribute to reproducible science.  For computational notebooks, such as Jupyter, we have worked to expand the methods used to evaluate reproducibility to allow for more nuanced comparison. We found that different computing environment configurations play a role in producing results that communicate the same insights but are not exactly the same. Sometimes these differences are minor and clearly cosmetic (e.g., a change in the font used for text in a figure), but other times, they are complex and harder to evaluate (e.g., a small difference in a numerical output could be due to roundoff error but could also point to a problem in the code). In addition, we have developed approaches like Dataflow Notebooks that seek to remove some of the ambiguity in reproducing the work in a computational notebook.  The project outcomes have been disseminated widely across academic venues and online platforms. The insights and findings have been shared with hundreds of undergraduate and graduate students through their courses, enriching their education and emphasizing the importance of reproducible research.     Last Modified: 12/30/2023       Submitted by: HamedAlhoori]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
