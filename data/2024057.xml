<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle><![CDATA[Collaborative Research: NRI: FND: Graph Neural Networks for Multi-Object Manipulation]]></AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2020</AwardEffectiveDate>
<AwardExpirationDate>12/31/2023</AwardExpirationDate>
<AwardTotalIntnAmount>405037.00</AwardTotalIntnAmount>
<AwardAmount>429037</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Cang Ye</SignBlockName>
<PO_EMAI>cye@nsf.gov</PO_EMAI>
<PO_PHON>7032924702</PO_PHON>
</ProgramOfficer>
<AbstractNarration>For robots to act as ubiquitous assistants in daily life, they must regularly contend with environments involving many objects and objects built of many constituent parts. Current robotics research focuses on providing solutions to isolated manipulation tasks, developing specialized representations that do not readily work across tasks. This project seeks to enable robots to learn to represent and understand the world from multiple sensors, across many manipulation tasks. Specifically, the project will examine tasks in heavily cluttered environments that require multiple distinct picking and placing actions. This project will develop autonomous manipulation methods suitable for use in robotic assistants. Assistive robots stand to make a substantial impact in increasing the quality of life of older adults and persons with certain degenerative diseases. These methods also apply to manipulation in natural or man-made disasters areas, where explicit object models are not available. The tools developed in this project can also improve robot perception, grasping, and multi-step manipulation skills for manufacturing. &lt;br/&gt;&lt;br/&gt;With their ability to learn powerful representations from raw perceptual data, deep neural networks provide the most promising framework to approach key perceptual and reasoning challenges underlying autonomous robot manipulation. Despite​ ​their success, existing approaches scale poorly to the diverse set of scenarios autonomous robots will handle in natural environments. These current limitations of neural networks arise from being trained on isolated tasks, use of different architectures for different problems, and inability to scale to complex scenes containing a varying or large number of objects. This project hypothesizes that graph neural networks provide a powerful framework that can encode multiple sensor streams over time to provide robots with rich and scalable representations for multi-object and multi-task perception and manipulation. This project examines a number of extensions to graph neural networks in order to address current limitations for their use in autonomous manipulation. Furthermore this project examines novel ways of leveraging learned graph neural networks for manipulation planning and control in clutter and for multi-step, multi-object manipulation tasks. In order to train these large-scale graph net representations this project will use extremely large scale, physically accurate, photo-realistic simulation. All perceptual and behavior generation techniques developed in this project will be experimentally validated on a set of challenging real-world manipulation tasks.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>09/10/2020</MinAmdLetterDate>
<MaxAmdLetterDate>05/19/2022</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2024057</AwardID>
<Investigator>
<FirstName>Dieter</FirstName>
<LastName>Fox</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Dieter Fox</PI_FULL_NAME>
<EmailAddress><![CDATA[fox@cs.washington.edu]]></EmailAddress>
<NSF_ID>000210667</NSF_ID>
<StartDate>09/10/2020</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name><![CDATA[University of Washington]]></Name>
<CityName>SEATTLE</CityName>
<ZipCode>981951016</ZipCode>
<PhoneNumber>2065434043</PhoneNumber>
<StreetAddress><![CDATA[4333 BROOKLYN AVE NE]]></StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Washington</StateName>
<StateCode>WA</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>WA07</CONGRESS_DISTRICT_ORG>
<ORG_UEI_NUM>HD1WMN6945W6</ORG_UEI_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF WASHINGTON</ORG_LGL_BUS_NAME>
<ORG_PRNT_UEI_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Washington]]></Name>
<CityName>Seattle</CityName>
<StateCode>WA</StateCode>
<ZipCode>981952500</ZipCode>
<StreetAddress><![CDATA[185 Stevens Way, Computer Scienc]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Washington</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>WA07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>748400</Code>
<Text>IIS Special Projects</Text>
</ProgramElement>
<ProgramElement>
<Code>801300</Code>
<Text>NRI-National Robotics Initiati</Text>
</ProgramElement>
<ProgramReference>
<Code>8086</Code>
<Text>Natl Robotics Initiative (NRI)</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code/>
<Name/>
<APP_SYMB_ID/>
</Appropriation>
<Appropriation>
<Code>0120</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0121</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Fund>
<Code>01002223DB</Code>
<Name><![CDATA[NSF RESEARCH & RELATED ACTIVIT]]></Name>
<FUND_SYMB_ID>040100</FUND_SYMB_ID>
</Fund>
<Fund>
<Code>01002021DB</Code>
<Name><![CDATA[NSF RESEARCH & RELATED ACTIVIT]]></Name>
<FUND_SYMB_ID>040100</FUND_SYMB_ID>
</Fund>
<Fund>
<Code>01002122DB</Code>
<Name><![CDATA[NSF RESEARCH & RELATED ACTIVIT]]></Name>
<FUND_SYMB_ID>040100</FUND_SYMB_ID>
</Fund>
<FUND_OBLG>2020~405037</FUND_OBLG>
<FUND_OBLG>2021~8000</FUND_OBLG>
<FUND_OBLG>2022~16000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Robots that can robustly perform a wide range of manipulation tasks can have significant impact on many important application domains, including industrial manufacturing, warehouse logistics, health care, and home care for the elderly.&nbsp; The goal of this project was to develop a learning framework that provides robust solutions to key challenges in manipulation. To achieve this, we developed advances in deep learning techniques and showed how to leverage photorealistic, physics-based simulation tools to provide training experiences for robot manipulators.</p> <p><br />Our initial effort has focused on developing an object-centric representation that enables spatial reasoning for robot manipulation of common household objects. The ability to infer spatial relations among objects from sensor observations is a crucial step towards automatic planning for complex tasks. Our approach, called SORNet, was at the forefront of generative AI techniques for robotics reasoning, learning powerful representations of objects in manipulation scenes. We showed that SORNet generalizes well to unseen objects without any additional training, and that it can make good predictions on real-world scenes as well (despite being trained in simulation only). Toward the end of this project, we re-visited the topic of learning spatial reasoning for robot manipulation. Our approach, called RoboPoint, took advantage of the most recent advances in large language and vision models to fine-tune such models on manipulation-specific reasoning tasks. Our results demonstrate significant improvements even over the most advanced models such as GPT-4o.</p> <p><br />Toward real world manipulation tasks, we also developed the Multi-Task Masked Transformer (M2T2) model, a unified model for learning multiple action primitives for manipulation tasks. Given a point cloud observation of a scene, M2T2 predicts collision-free gripper poses for two types of actions; 6-DoF grasping and 3-DoF placing, eliminating the need to use different methods for different actions. M2T2 is able to generate a diverse set of goal poses that provide sufficient options for low-level motion planners. It can also be combined with high-level reasoning models such as RoboPoint to solve complex tasks based on natural language input.</p> <p><br />Overall, the combination of robust manipulation skills enabled through tools such as SORNet and M2T2, along with the improved language-based understanding capabilities via the RoboPoint approach, greatly improve the ability of robot manipulators to operate in real world settings such as home environments or hospitals.&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>  Last Modified: 06/25/2024<br> Modified by: Dieter&nbsp;Fox</p></div> <div class="porSideCol" ></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[  Robots that can robustly perform a wide range of manipulation tasks can have significant impact on many important application domains, including industrial manufacturing, warehouse logistics, health care, and home care for the elderly. The goal of this project was to develop a learning framework that provides robust solutions to key challenges in manipulation. To achieve this, we developed advances in deep learning techniques and showed how to leverage photorealistic, physics-based simulation tools to provide training experiences for robot manipulators.    Our initial effort has focused on developing an object-centric representation that enables spatial reasoning for robot manipulation of common household objects. The ability to infer spatial relations among objects from sensor observations is a crucial step towards automatic planning for complex tasks. Our approach, called SORNet, was at the forefront of generative AI techniques for robotics reasoning, learning powerful representations of objects in manipulation scenes. We showed that SORNet generalizes well to unseen objects without any additional training, and that it can make good predictions on real-world scenes as well (despite being trained in simulation only). Toward the end of this project, we re-visited the topic of learning spatial reasoning for robot manipulation. Our approach, called RoboPoint, took advantage of the most recent advances in large language and vision models to fine-tune such models on manipulation-specific reasoning tasks. Our results demonstrate significant improvements even over the most advanced models such as GPT-4o.    Toward real world manipulation tasks, we also developed the Multi-Task Masked Transformer (M2T2) model, a unified model for learning multiple action primitives for manipulation tasks. Given a point cloud observation of a scene, M2T2 predicts collision-free gripper poses for two types of actions; 6-DoF grasping and 3-DoF placing, eliminating the need to use different methods for different actions. M2T2 is able to generate a diverse set of goal poses that provide sufficient options for low-level motion planners. It can also be combined with high-level reasoning models such as RoboPoint to solve complex tasks based on natural language input.    Overall, the combination of robust manipulation skills enabled through tools such as SORNet and M2T2, along with the improved language-based understanding capabilities via the RoboPoint approach, greatly improve the ability of robot manipulators to operate in real world settings such as home environments or hospitals.           Last Modified: 06/25/2024       Submitted by: DieterFox]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
