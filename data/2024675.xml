<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle><![CDATA[Collaborative Research: NRI: INT: Scalable, Customizable, Robot Learning with Humans]]></AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2020</AwardEffectiveDate>
<AwardExpirationDate>09/30/2023</AwardExpirationDate>
<AwardTotalIntnAmount>700000.00</AwardTotalIntnAmount>
<AwardAmount>700000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Juan Wachs</SignBlockName>
<PO_EMAI>jwachs@nsf.gov</PO_EMAI>
<PO_PHON>7032928714</PO_PHON>
</ProgramOfficer>
<AbstractNarration><![CDATA[Activities of daily living (ADLs) are both essential and routine aspects of self-care, including the ability to independently eat, dress, transfer from one position to another, bathe, and toilet. Robotic assistance with activities of daily living could increase the independence of people with disabilities, improve quality of life, and help address pressing societal needs, such as aging populations, high healthcare costs, and shortages of healthcare workers. While progress has been made towards such robotic-assistance, a key challenge is that many activities of daily living require robots to manipulate ​fabric in coordination with ​people​. Notably, many forms of bedside assistance include dexterous manipulation of bedding, hygiene often involves dexterous manipulation of washcloths and towels, and dressing involves a diverse array of clothes. This project seeks to make foundational progress on this major challenge through advancements in machine learning, simulation, and customizable human-robot interaction. This project will result in new capabilities in robot-assisted bedding adjustment, bathing, and dressing for people with disabilities. In addition, this project and its participating research groups will broaden participation by engaging under-represented groups, K-12 students, and undergraduates in research and education.&lt;br/&gt;&lt;br/&gt;This research aims to develop a co-robotic framework towards the Integrative Task: Customizable Fabric Manipulation for Home Assistance, with specific emphasis on bedding, bathing, and dressing, which have significant physical interaction between the human and the robot. This project will make foundational progress along three main thrusts, brought together under this overarching integrative application: (1) Learning to Assist from Raw Sensory Observations, (2) Physics Simulation for Learning Assistance, and (3) Customizable Robotic Assistance with Human-in-the-Loop Feedback. Customizability here refers to the ability of our system to adapt to the specific preferences and abilities of each individual. Through open-sourced code development and physics simulations, this project also seeks to lower the barrier to entry for others to work towards the general problem of customizable home assistance. Overall, this research will develop new techniques in robot-assisted dressing, bedding adjustment, and body bathing (hygiene) for people with disabilities, which has the potential to help millions of people achieve greater independence and a higher quality of life.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.]]></AbstractNarration>
<MinAmdLetterDate>09/08/2020</MinAmdLetterDate>
<MaxAmdLetterDate>10/13/2020</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2024675</AwardID>
<Investigator>
<FirstName>Anca</FirstName>
<LastName>Dragan</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Anca Dragan</PI_FULL_NAME>
<EmailAddress><![CDATA[anca@berkeley.edu]]></EmailAddress>
<NSF_ID>000704141</NSF_ID>
<StartDate>09/08/2020</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Pieter</FirstName>
<LastName>Abbeel</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Pieter Abbeel</PI_FULL_NAME>
<EmailAddress><![CDATA[pabbeel@cs.berkeley.edu]]></EmailAddress>
<NSF_ID>000511407</NSF_ID>
<StartDate>09/08/2020</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-Berkeley</Name>
<CityName>BERKELEY</CityName>
<ZipCode>947101749</ZipCode>
<PhoneNumber>5106433891</PhoneNumber>
<StreetAddress>1608 4TH ST STE 201</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>12</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA12</CONGRESS_DISTRICT_ORG>
<ORG_UEI_NUM>GS3YEVSS12N6</ORG_UEI_NUM>
<ORG_LGL_BUS_NAME>REGENTS OF THE UNIVERSITY OF CALIFORNIA, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_UEI_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of California-Berkeley]]></Name>
<CityName>Berkeley</CityName>
<StateCode>CA</StateCode>
<ZipCode>947101749</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>12</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA12</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8013</Code>
<Text>NRI-National Robotics Initiati</Text>
</ProgramElement>
<ProgramReference>
<Code>8086</Code>
<Text>Natl Robotics Initiative (NRI)</Text>
</ProgramReference>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<Appropriation>
<Code>0120</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Fund>
<Code>01002021DB</Code>
<Name><![CDATA[NSF RESEARCH & RELATED ACTIVIT]]></Name>
<FUND_SYMB_ID>040100</FUND_SYMB_ID>
</Fund>
<FUND_OBLG>2020~700000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span id="docs-internal-guid-ac34da31-7fff-02b3-7f84-8554336689d1"> <p dir="ltr"><span>This 3-year collaborative research project between UC Berkeley, Carnegie Mellon University, and Georgia Tech investigated how to make robots more effective around humans.&nbsp; This is in contrast to most of today&rsquo;s existing robotic deployments, where robots are placed in cages, strictly separated from humans. Specifically, this project pursued advances towards robotic assistance with </span><span>Activities of Daily Living (ADLs): essential and routine aspects of self-care, including the ability to independently eat, dress, transfer from one position to another, bathe, and toilet.&nbsp; Robotic assistance with activities of daily living could increase the independence of people with disabilities, improve quality of life, and help address pressing societal needs, such as aging populations, high healthcare costs, and shortages of healthcare workers. While progress has been made toward such robotic assistance, current systems rely on simplifying assumptions limiting their applicability.&nbsp;</span></p> <p dir="ltr"><span>To advance robotic capabilities in Activities of Daily Living, this project made progress along 3 main directions:</span></p> <br /> <p dir="ltr"><span>(1) Improved Robotic Visuomotor Skill Learning: While robots have been shown capable of learning visuomotor skills, the process is often time-consuming even for relatively simple tasks.&nbsp; Our work shows that masked world models and masked trajectory models, where the robot gets to observe part of an execution sequence and has to learn to fill back in the blank, can significantly speed up learning speed.&nbsp; By providing this additional task, robots become better at the original taks of&nbsp; robot action prediction.&nbsp; Our work on Decision Transformer showed the effectiveness of sequence modeling for skill acquisition and we proposed improved exploration incentives for learning agents.</span></p> <br /> <p dir="ltr"><span>(2) Improved Robotic Understanding of Interaction with Humans: The key to interacting well with humans is understanding general as well as individual human preferences.&nbsp; We made significant contributions to Reinforcement Learning from Human Feedback (RLHF) with PEBBLE improving the speed of learning from human preferences across pairs of alternatives and B-Pref providing a benchmark for RLHF for the wider community to make progress. We proposed ways to use passive human activity videos from the internet to train human-robot interaction models with Robotic Telekinesis. We also showed that simple videos of good behaviors can be used as training data for a generative model that provides a baseline reward model that gauges the reasonableness of robot behavior.&nbsp;</span></p> </span></p> <p><span id="docs-internal-guid-58cf9da3-7fff-1ffa-b212-4ed8a1d2d294"> <p dir="ltr"><span>(3) Integrated Improvements on Robotics Assistance with Activities of Daily Living: We improved the robustness of robot policies for human assistance in ADLs in Assistive Gym via a method that learns a representation for the diversity of human behaviors, and adapts it online via interaction with the patient. We also proposed an unsupervised learning approach to learn 3D keypoint-based representation from direct camera images to perform assistance tasks such as dressing in Assistive Gym. We made advancements in robotic perception relevant to robotic assistance with ADLs. Many common ADLs make it difficult for robots to perceive the human body while providing assistance. For example, bedding and clothing can hide the human body, and the robot itself can hide the human body while providing assistance.&nbsp; We developed methods that use physics simulations to overcome these challenges by efficiently generating fully-observable training data enabling robots to estimate 3D human body shape, pose, and contact pressure even when the person is hidden by a blanket. We also developed novel perception methods that use capacitive sensors to observe the human body when hidden by clothing or a damp cloth during dressing and bathing, and low-cost visual force/torque sensing that can support bedding manipulation and bathing. We also developed robust visual pressure estimation, which supports precision manipulation in clutter, such as picking up a pill and a paperclip. This could potentially be used to assist people who have impaired fine motor skills or visual impairments.</span></p> <p dir="ltr"><span>The project contributed to the training of 17 graduate students, 10 undergraduate students, and 2 post-docs.&nbsp; The project produced 37 peer-reviewed papers, all uploaded into the NSF archive as well as posted online for general access to the work.&nbsp; The works were presented by the PIs and their students at the main conferences in the field.&nbsp; </span><span>We have also open-sourced various project code and data, lowering the barrier of entry for others to learn from and build upon our work.</span></p> <br /> <p dir="ltr"><span>PI Kemp taught the Robotic Caregivers class three times while this award was active, including in collaboration with other instructors at other institutions across the US.&nbsp; This is, a project-based class focused on robotic assistance for older adults and people with disabilities.&nbsp; All of the teaching materials were made available for the public at the following website:&nbsp;</span></p> <p dir="ltr"><a href="https://sites.gatech.edu/robotic-caregivers/"><span>https://sites.gatech.edu/robotic-caregivers/</span></a><span>.&nbsp;</span></p> <br /> <p dir="ltr"><span>Beyond students at the universities, PI Dragan hosted </span><span>Berkeley AI4ALL, a summer camp to teach high school students from underpriviledged backgrounds about AI from a human-centered perspective. The camp ran all summers of the project, with graduate students and undergraduates who worked on the project volunteering and designing coursework for the participants.</span></p> <div><span><br /></span></div> </span></p> <p>&nbsp;</p><br> <p>  Last Modified: 02/19/2024<br> Modified by: Pieter&nbsp;Abbeel</p></div> <div class="porSideCol" ><div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)          </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2024/2024675/2024675_10705500_1708384101046_Picture1--rgov-214x142.png" original="/por/images/Reports/POR/2024/2024675/2024675_10705500_1708384101046_Picture1--rgov-800width.png" title="Robotic dressing"><img src="/por/images/Reports/POR/2024/2024675/2024675_10705500_1708384101046_Picture1--rgov-66x44.png" alt="Robotic dressing"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Dressing, bathing via local body state estimation through cloth using multidimensional capacitive sensing.   Characterizing Multidimensional Capacitive Servoing for Physical Human-Robot Interaction.</div> <div class="imageCredit">Prof. Charlie Kemp</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Pieter&nbsp;Abbeel <div class="imageTitle">Robotic dressing</div> </div> </li><li> <a href="/por/images/Reports/POR/2024/2024675/2024675_10705500_1708384042354_Picture4--rgov-214x142.png" original="/por/images/Reports/POR/2024/2024675/2024675_10705500_1708384042354_Picture4--rgov-800width.png" title="Robotic bedding manipulation"><img src="/por/images/Reports/POR/2024/2024675/2024675_10705500_1708384042354_Picture4--rgov-66x44.png" alt="Robotic bedding manipulation"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Visual estimation of force and torque for soft grippers, which supports assistive tasks like bedding manipulation and bathing.</div> <div class="imageCredit">Prof. Charlie Kemp</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Pieter&nbsp;Abbeel <div class="imageTitle">Robotic bedding manipulation</div> </div> </li><li> <a href="/por/images/Reports/POR/2024/2024675/2024675_10705500_1708383892297_Picture2--rgov-214x142.png" original="/por/images/Reports/POR/2024/2024675/2024675_10705500_1708383892297_Picture2--rgov-800width.png" title="Simulating a person's body under covers"><img src="/por/images/Reports/POR/2024/2024675/2024675_10705500_1708383892297_Picture2--rgov-66x44.png" alt="Simulating a person's body under covers"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Simulation-based optimization of policies to uncover selected parts of a person�s body in bed.</div> <div class="imageCredit">Prof. Charlie Kemp</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Pieter&nbsp;Abbeel <div class="imageTitle">Simulating a person's body under covers</div> </div> </li><li> <a href="/por/images/Reports/POR/2024/2024675/2024675_10705500_1708383965484_Picture3--rgov-214x142.png" original="/por/images/Reports/POR/2024/2024675/2024675_10705500_1708383965484_Picture3--rgov-800width.png" title="Estimating 3D body shape"><img src="/por/images/Reports/POR/2024/2024675/2024675_10705500_1708383965484_Picture3--rgov-66x44.png" alt="Estimating 3D body shape"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Physics simulations to train deep models that estimate of 3D body shape, pose, and contact pressure from a depth image in spite of occlusion from bedding.</div> <div class="imageCredit">Prof. Charlie Kemp</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Pieter&nbsp;Abbeel <div class="imageTitle">Estimating 3D body shape</div> </div> </li></ul> </div> </div></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[     This 3-year collaborative research project between UC Berkeley, Carnegie Mellon University, and Georgia Tech investigated how to make robots more effective around humans. This is in contrast to most of todays existing robotic deployments, where robots are placed in cages, strictly separated from humans. Specifically, this project pursued advances towards robotic assistance with Activities of Daily Living (ADLs): essential and routine aspects of self-care, including the ability to independently eat, dress, transfer from one position to another, bathe, and toilet. Robotic assistance with activities of daily living could increase the independence of people with disabilities, improve quality of life, and help address pressing societal needs, such as aging populations, high healthcare costs, and shortages of healthcare workers. While progress has been made toward such robotic assistance, current systems rely on simplifying assumptions limiting their applicability.   To advance robotic capabilities in Activities of Daily Living, this project made progress along 3 main directions:     (1) Improved Robotic Visuomotor Skill Learning: While robots have been shown capable of learning visuomotor skills, the process is often time-consuming even for relatively simple tasks. Our work shows that masked world models and masked trajectory models, where the robot gets to observe part of an execution sequence and has to learn to fill back in the blank, can significantly speed up learning speed. By providing this additional task, robots become better at the original taks of robot action prediction. Our work on Decision Transformer showed the effectiveness of sequence modeling for skill acquisition and we proposed improved exploration incentives for learning agents.     (2) Improved Robotic Understanding of Interaction with Humans: The key to interacting well with humans is understanding general as well as individual human preferences. We made significant contributions to Reinforcement Learning from Human Feedback (RLHF) with PEBBLE improving the speed of learning from human preferences across pairs of alternatives and B-Pref providing a benchmark for RLHF for the wider community to make progress. We proposed ways to use passive human activity videos from the internet to train human-robot interaction models with Robotic Telekinesis. We also showed that simple videos of good behaviors can be used as training data for a generative model that provides a baseline reward model that gauges the reasonableness of robot behavior.       (3) Integrated Improvements on Robotics Assistance with Activities of Daily Living: We improved the robustness of robot policies for human assistance in ADLs in Assistive Gym via a method that learns a representation for the diversity of human behaviors, and adapts it online via interaction with the patient. We also proposed an unsupervised learning approach to learn 3D keypoint-based representation from direct camera images to perform assistance tasks such as dressing in Assistive Gym. We made advancements in robotic perception relevant to robotic assistance with ADLs. Many common ADLs make it difficult for robots to perceive the human body while providing assistance. For example, bedding and clothing can hide the human body, and the robot itself can hide the human body while providing assistance. We developed methods that use physics simulations to overcome these challenges by efficiently generating fully-observable training data enabling robots to estimate 3D human body shape, pose, and contact pressure even when the person is hidden by a blanket. We also developed novel perception methods that use capacitive sensors to observe the human body when hidden by clothing or a damp cloth during dressing and bathing, and low-cost visual force/torque sensing that can support bedding manipulation and bathing. We also developed robust visual pressure estimation, which supports precision manipulation in clutter, such as picking up a pill and a paperclip. This could potentially be used to assist people who have impaired fine motor skills or visual impairments.   The project contributed to the training of 17 graduate students, 10 undergraduate students, and 2 post-docs. The project produced 37 peer-reviewed papers, all uploaded into the NSF archive as well as posted online for general access to the work. The works were presented by the PIs and their students at the main conferences in the field. We have also open-sourced various project code and data, lowering the barrier of entry for others to learn from and build upon our work.     PI Kemp taught the Robotic Caregivers class three times while this award was active, including in collaboration with other instructors at other institutions across the US. This is, a project-based class focused on robotic assistance for older adults and people with disabilities. All of the teaching materials were made available for the public at the following website:   https://sites.gatech.edu/robotic-caregivers/.     Beyond students at the universities, PI Dragan hosted Berkeley AI4ALL, a summer camp to teach high school students from underpriviledged backgrounds about AI from a human-centered perspective. The camp ran all summers of the project, with graduate students and undergraduates who worked on the project volunteering and designing coursework for the participants.           Last Modified: 02/19/2024       Submitted by: PieterAbbeel]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
