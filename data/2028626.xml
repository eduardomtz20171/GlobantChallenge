<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle><![CDATA[CAREER: Combining Learning and Reasoning for Spatial Language Understanding]]></AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>11/01/2019</AwardEffectiveDate>
<AwardExpirationDate>06/30/2024</AwardExpirationDate>
<AwardTotalIntnAmount>474520.00</AwardTotalIntnAmount>
<AwardAmount>474520</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Erion Plaku</SignBlockName>
<PO_EMAI>eplaku@nsf.gov</PO_EMAI>
<PO_PHON>7032924426</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The main goal of this project is to teach machines to understand human language when it contains spatial information. For example, to understand the statement, "Give me the book on AI on the table to your left," a robot needs to understand that the first "on" expresses the topic of the book, while the second "on" and the rest of the sentence convey spatial information. Next, it needs to work out the visual meaning of "on" to identify the correct book. Spatial language understanding is challenging because it requires meaning disambiguation, recognizing the links between referenced objects, and in most cases, background knowledge and common sense. This is particularly true for more complex sentences that include nesting relations or idioms containing spatial terms, like "she is the top expert in her field." The goal of this research is to develop techniques for the integration of machine learning and reasoning to advance spatial language comprehension. Solving this problem will provide tangible benefits to society in a wide variety of applications including healthcare (e.g., extracting biomedical information from patient reports and images), navigation and communication with assistant robots (particularly in risky situations like firefighter robots), situational awareness, information retrieval systems using various types of data, and geographical information systems.&lt;br/&gt;&lt;br/&gt;Providing reasoning capabilities to machine learning models is highly challenging. The proposed research advances this issue in an important and broad sub-problem: spatial language understanding. To this aim, a generic domain-independent symbolic spatial meaning representation will be introduced, which covers the main spatial semantic concepts for a variety of tasks. Deep structured and relational learning techniques will be developed to obtain such representations. The proposed techniques will facilitate indirect supervision by exploiting visual resources, will use ontologies that convey common sense and will integrate the axioms of spatial qualitative reasoning in training structured representations. Such an approach helps to avoid massive and complex data annotation for training structured and complex models. This research will evaluate whether using formal spatial calculi models as an intermediate structured representation will improve spatial reasoning and deeper learning. The interaction between learning and spatial reasoning will be investigated, while exploiting both symbolic and sub-symbolic representations. The impact of integrating explicit spatial reasoning will be tested on textual and visual question-answering tasks with a focus on locative questions. The outcome of this research will advance the generalizability of current state-of-the-art deep learning models at a time when there is a lack of training examples for complex unobserved situations. This work will help bridge the gap between symbolic AI and deep ML to advance spatial language understanding. The project will support research and education for graduate and undergraduate students and increase the contribution of women and minorities in STEM.&lt;br/&gt;&lt;br/&gt;This award is jointly funded by the Division of Information and Intelligent Systems in the Directorate for Computer &amp; Information Science &amp; Engineering and the Established Program to Stimulate Competitive Research in the Office of Integrative Activities.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>04/13/2020</MinAmdLetterDate>
<MaxAmdLetterDate>06/21/2023</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2028626</AwardID>
<Investigator>
<FirstName>Parisa</FirstName>
<LastName>Kordjamshidi</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Parisa Kordjamshidi</PI_FULL_NAME>
<EmailAddress><![CDATA[kordjams@msu.edu]]></EmailAddress>
<NSF_ID>000728296</NSF_ID>
<StartDate>04/13/2020</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name><![CDATA[Michigan State University]]></Name>
<CityName>EAST LANSING</CityName>
<ZipCode>488242600</ZipCode>
<PhoneNumber>5173555040</PhoneNumber>
<StreetAddress><![CDATA[426 AUDITORIUM RD RM 2]]></StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Michigan</StateName>
<StateCode>MI</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MI07</CONGRESS_DISTRICT_ORG>
<ORG_UEI_NUM>R28EKN92ZTZ9</ORG_UEI_NUM>
<ORG_LGL_BUS_NAME>MICHIGAN STATE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_UEI_NUM>VJKZC4D1JN36</ORG_PRNT_UEI_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Michigan State University]]></Name>
<CityName>East Lansing</CityName>
<StateCode>MI</StateCode>
<ZipCode>488241226</ZipCode>
<StreetAddress><![CDATA[428 S. Shaw Lane]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Michigan</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MI07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>749500</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>1045</Code>
<Text>CAREER-Faculty Erly Career Dev</Text>
</ProgramReference>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>9102</Code>
<Text>WOMEN, MINORITY, DISABLED, NEC</Text>
</ProgramReference>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<Appropriation>
<Code/>
<Name/>
<APP_SYMB_ID/>
</Appropriation>
<Appropriation>
<Code/>
<Name/>
<APP_SYMB_ID/>
</Appropriation>
<Appropriation>
<Code/>
<Name/>
<APP_SYMB_ID/>
</Appropriation>
<Appropriation>
<Code/>
<Name/>
<APP_SYMB_ID/>
</Appropriation>
<Fund>
<Code>01002122DB</Code>
<Name><![CDATA[NSF RESEARCH & RELATED ACTIVIT]]></Name>
<FUND_SYMB_ID>040100</FUND_SYMB_ID>
</Fund>
<Fund>
<Code>01002324DB</Code>
<Name><![CDATA[NSF RESEARCH & RELATED ACTIVIT]]></Name>
<FUND_SYMB_ID>040100</FUND_SYMB_ID>
</Fund>
<Fund>
<Code>01002223DB</Code>
<Name><![CDATA[NSF RESEARCH & RELATED ACTIVIT]]></Name>
<FUND_SYMB_ID>040100</FUND_SYMB_ID>
</Fund>
<Fund>
<Code>01002021DB</Code>
<Name><![CDATA[NSF RESEARCH & RELATED ACTIVIT]]></Name>
<FUND_SYMB_ID>040100</FUND_SYMB_ID>
</Fund>
<FUND_OBLG>2020~217782</FUND_OBLG>
<FUND_OBLG>2021~56738</FUND_OBLG>
<FUND_OBLG>2022~113000</FUND_OBLG>
<FUND_OBLG>2023~87000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Natural language can convey information about the location and movement of objects in space. This is a fundamental function of language that intelligent agents must capture to effectively interact with the physical environment. For example, in the expression 'Go towards the chair that is to the left of the table in front of the big window,' a computational model needs to recognize the spatial entities and their relationships to answer any 'where' or &lsquo;locative&rsquo; question. If we ask, 'Is the table in front of the big window?' the answer should be 'yes.' Similarly, the answer to 'Is the table to the right of the chair?' should also be 'yes' if the model understands the logical relationship between left and right to infer the correct answer.</p> <p>In this NSF project, we &nbsp;initially developed a framework to experiment with modern language models and evaluate their capabilities in spatial reasoning and inferring answers to questions that may not be explicitly mentioned in the text. We created both synthetic and human-annotated datasets for spatial reasoning in toy environments as well as in realistic domains. These datasets can serve as a source of supervision for fine-tuning large language models and for evaluating their spatial reasoning capabilities.</p> <p>We then demonstrated that our automatically generated data for tuning LLMs helps transfer knowledge from synthetic domains to realistic domains for complex spatial reasoning. This approach reduces the need for large amounts of human-labeled data. We developed a spatial logical reasoning knowledge base containing a small set of rules, such as 'A is to the left of B then B is to the right of A' or 'if A is to the left of B and B is to the left of C, then A is to the left of C.' We fine-tuned LLMs using neuro-symbolic approaches, where the models not only learn to answer questions correctly but are also trained to avoid violating spatial logical constraints. We showed that this approach helps the models capture these rules and achieve better generalization to novel domains where complex reasoning is required to answer spatial questions.&nbsp;</p> <p>We integrated the semantic representation of language and linguistic ontologies involving symbolic and graph structures into deep learning models to improve spatial, temporal, and procedural reasoning over text. Specifically, when a procedure describes the creation, movement, and transformation of entities from one form to another, our models are better able to capture the fine-grained semantics of the procedure, allowing them to track changes in the objects' locations at each step of the process.</p> <p>We integrated linguistic spatial semantics into navigation agents in multimodal environments. In such a setting, the agent receives visual information, connects the language to the visual modality, and performs actions accordingly. It follows the instructions and moves until it reaches the intended target. We have made several research contributions in this direction. In one attempt, we used spatial roles and relations from the instructions and linked them to visual object semantics to enable accurate navigation. We also separated reasoning about landmarks from reasoning over orientation instructions, tuning the models for these two skills separately. Additionally, we used the agent to generate detailed spatial descriptions of the environment while navigating. Our results have improved the state of the art in vision-and-language navigation, enhancing the spatial reasoning capabilities of navigating agents using the commonly used benchmarks and evaluation metrics.</p> <p>Our developed models, constructed datasets, annotations, and code are publicly available on the PI's website and the GitHub of the HLR research group. This project has supported both graduate and undergraduate research, including contributions from diverse groups and minorities in higher education. The PI has been involved in multiple pedagogical activities both locally and internationally. She has offered several cutting-edge research and educational tutorials on spatial language understanding at major international conferences in the field of natural language processing. All materials are recorded and publicly available. Additionally, she has been the lead organizer of multiple workshops on spatial language understanding and robotics NLP. These workshops bring together researchers from around the world across multiple disciplines&mdash;natural language processing, computer vision, and robotics&mdash;to present and discuss cutting-edge research and highlight the challenges and future directions. This grant has supported both national and international collaborations aimed at advancing knowledge and technology in the subfield of natural language understanding and grounding language into vision.</p> <p>&nbsp;</p><br> <p>  Last Modified: 10/14/2024<br> Modified by: Parisa&nbsp;Kordjamshidi</p></div> <div class="porSideCol" ></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[  Natural language can convey information about the location and movement of objects in space. This is a fundamental function of language that intelligent agents must capture to effectively interact with the physical environment. For example, in the expression 'Go towards the chair that is to the left of the table in front of the big window,' a computational model needs to recognize the spatial entities and their relationships to answer any 'where' or locative question. If we ask, 'Is the table in front of the big window?' the answer should be 'yes.' Similarly, the answer to 'Is the table to the right of the chair?' should also be 'yes' if the model understands the logical relationship between left and right to infer the correct answer.   In this NSF project, we initially developed a framework to experiment with modern language models and evaluate their capabilities in spatial reasoning and inferring answers to questions that may not be explicitly mentioned in the text. We created both synthetic and human-annotated datasets for spatial reasoning in toy environments as well as in realistic domains. These datasets can serve as a source of supervision for fine-tuning large language models and for evaluating their spatial reasoning capabilities.   We then demonstrated that our automatically generated data for tuning LLMs helps transfer knowledge from synthetic domains to realistic domains for complex spatial reasoning. This approach reduces the need for large amounts of human-labeled data. We developed a spatial logical reasoning knowledge base containing a small set of rules, such as 'A is to the left of B then B is to the right of A' or 'if A is to the left of B and B is to the left of C, then A is to the left of C.' We fine-tuned LLMs using neuro-symbolic approaches, where the models not only learn to answer questions correctly but are also trained to avoid violating spatial logical constraints. We showed that this approach helps the models capture these rules and achieve better generalization to novel domains where complex reasoning is required to answer spatial questions.   We integrated the semantic representation of language and linguistic ontologies involving symbolic and graph structures into deep learning models to improve spatial, temporal, and procedural reasoning over text. Specifically, when a procedure describes the creation, movement, and transformation of entities from one form to another, our models are better able to capture the fine-grained semantics of the procedure, allowing them to track changes in the objects' locations at each step of the process.   We integrated linguistic spatial semantics into navigation agents in multimodal environments. In such a setting, the agent receives visual information, connects the language to the visual modality, and performs actions accordingly. It follows the instructions and moves until it reaches the intended target. We have made several research contributions in this direction. In one attempt, we used spatial roles and relations from the instructions and linked them to visual object semantics to enable accurate navigation. We also separated reasoning about landmarks from reasoning over orientation instructions, tuning the models for these two skills separately. Additionally, we used the agent to generate detailed spatial descriptions of the environment while navigating. Our results have improved the state of the art in vision-and-language navigation, enhancing the spatial reasoning capabilities of navigating agents using the commonly used benchmarks and evaluation metrics.   Our developed models, constructed datasets, annotations, and code are publicly available on the PI's website and the GitHub of the HLR research group. This project has supported both graduate and undergraduate research, including contributions from diverse groups and minorities in higher education. The PI has been involved in multiple pedagogical activities both locally and internationally. She has offered several cutting-edge research and educational tutorials on spatial language understanding at major international conferences in the field of natural language processing. All materials are recorded and publicly available. Additionally, she has been the lead organizer of multiple workshops on spatial language understanding and robotics NLP. These workshops bring together researchers from around the world across multiple disciplinesnatural language processing, computer vision, and roboticsto present and discuss cutting-edge research and highlight the challenges and future directions. This grant has supported both national and international collaborations aimed at advancing knowledge and technology in the subfield of natural language understanding and grounding language into vision.        Last Modified: 10/14/2024       Submitted by: ParisaKordjamshidi]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
