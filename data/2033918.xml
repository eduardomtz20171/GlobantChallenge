<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle><![CDATA[NSF2026: EAGER:Cues and actions for efficient nonverbal human-robot communication]]></AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2020</AwardEffectiveDate>
<AwardExpirationDate>08/31/2024</AwardExpirationDate>
<AwardTotalIntnAmount>138679.00</AwardTotalIntnAmount>
<AwardAmount>138679</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Todd Leen</SignBlockName>
<PO_EMAI>tleen@nsf.gov</PO_EMAI>
<PO_PHON>7032927215</PO_PHON>
</ProgramOfficer>
<AbstractNarration>A large part of human group communication takes place nonverbally. People follow gaze, avoid collisions, search and rescue in teams, all without speaking to each other. In this respect, actions, rather than words, enable rapid two-way communication of information without interfering with the task at hand or posing additional mental burden required to understand speech and text. Integrating human-machine intelligence would benefit from a similar natural and fluid communication between humans and machines. This project develops novel methods to advance human-robot intelligence through a series of experimental studies and rigorous mathematical analysis. The experiments involve tasks designed to exploit the strengths of robots and humans; robots are able to repetitively explore a large environment and humans have better awareness of the situation and domain expertise. The experimental tasks are inspired by the difficult problem of monitoring the vast number of invasive aquatic species threatening the Great Lakes region. The mathematical analysis is aimed at discovering effective robot actions in response to changes in human cognitive load, and efficient nonverbal interaction strategies between humans and robots. Research results from this work will raise public awareness of invasive aquatic species in the Great Lakes region and present human-robot teaming as a prominent opportunity to solve large-scale problems. Engineering students involved in the project will contribute to the new generation of scientific workforce who can straddle boundaries across multiple disciplines such as robotics, computer science, and ecology.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;This research aims to enable tighter integration of human-robot intelligence by teasing out components of efficient nonverbal human-robot communication. These include: (i) level of engagement of the robotic swarm as a function of human cognitive load, (ii) recruitment strategies used by humans as they team up with the robotic swarm to map a complex dynamic environment, (iii) perception of robot swarming patterns by humans, and (iv) indirect indicators of human cognitive load that can enable faster interpretation in the wild. Towards this, experimental conditions will highlight the dependence of team performance on how robots respond to the cognitive load experienced by the human participants. Experiments will be conducted in virtual reality to enable realization of large robot swarms without the accompanying design and sensor programming challenges. Scalability of swarm robotics will be preserved by building all environmental sensing and interaction strategies upon local interaction rules. Information-theoretic measures of directional information flow will be used to quantify human perception of swarm patterns and isolate movement correlates of cognitive load.  This project has the support of the Human-Centered-Computing Program in the IIS Division in the CISE Directorate, and the NSF 2026 Fund Program in the Office of Integrated Activities.  The project enriches, extends, and explores the NSF 2026 Idea Machine Winning Entry “Integrated Human-Machine Intelligence”.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>07/31/2020</MinAmdLetterDate>
<MaxAmdLetterDate>07/31/2020</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2033918</AwardID>
<Investigator>
<FirstName>Sachit</FirstName>
<LastName>Butail</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Sachit Butail</PI_FULL_NAME>
<EmailAddress><![CDATA[sbutail@niu.edu]]></EmailAddress>
<NSF_ID>000746801</NSF_ID>
<StartDate>07/31/2020</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name><![CDATA[Northern Illinois University]]></Name>
<CityName>DEKALB</CityName>
<ZipCode>601152828</ZipCode>
<PhoneNumber>8157531581</PhoneNumber>
<StreetAddress><![CDATA[1425 W LINCOLN HWY]]></StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<StateCode>IL</StateCode>
<CONGRESSDISTRICT>14</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IL14</CONGRESS_DISTRICT_ORG>
<ORG_UEI_NUM>M2EEE68GGCY9</ORG_UEI_NUM>
<ORG_LGL_BUS_NAME>NORTHERN ILLINOIS UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_UEI_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[Northern Illinois University]]></Name>
<CityName/>
<StateCode>IL</StateCode>
<ZipCode>601152828</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>14</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IL14</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>081Y00</Code>
<Text>NSF 2026 Fund</Text>
</ProgramElement>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<Appropriation>
<Code>0120</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Fund>
<Code>01002021DB</Code>
<Name><![CDATA[NSF RESEARCH & RELATED ACTIVIT]]></Name>
<FUND_SYMB_ID>040100</FUND_SYMB_ID>
</Fund>
<FUND_OBLG>2020~138679</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>In this project, we identified cues of interaction between humans and multiple robots in situations that require large area coverage such as search and rescue and environmental monitoring. Identifying such cues entailed creating new rich virtual environments and validating robust measures of cognitive load for human-subjects experiments. In particular, we designed and developed virtual environments inspired by the Grand Canyon National Park for search and rescue missions and Great Lakes for underwater monitoring missions. The Great Lakes environment included five different species of fish whose movement and social behavior was inspired by their real counterparts. We separated the problem of identifying nonverbal cues into two halves in terms of who gives the cues to whom: human to robots and robots to human.</p>  <p>With respect to nonverbal cues from human to robots, we validated the use of movement cues as possible indicators of psychological constructs such as prior knowledge, cognitive load, and situational awareness. These were investigated as part of two experimental studies, one in a laboratory setting and another in a virtual setting. Both studies revealed that speed, turn rate, and time spent staying still by the teleoperated robot can be used as reliable estimators of prior knowledge and can therefore be used as nonverbal cues for robots to &ldquo;read&rdquo; and adaptively assist the human in complex missions. Human-in-the-loop simulations revealed improved performance of the human-robot team over established methods.</p>  <p>With respect to nonverbal cues from multiple robots to humans we sought to design an &ldquo;alphabet&rdquo; inspired from collective behaviors that a swarm of robots could use to communicate complex information to a trained human as they navigate a dynamic environment. Experiments were conducted where a swarm of robots communicated site information to humans with different combinations of such nonverbal cues. These communication strategies were compared with text-based communication in terms of how they affected performance and cognitive load.</p>  <p>Several undergraduate and graduate students contributed to this project as they received training in experimental methods, dynamical systems modeling, statistical analysis, virtual reality, and human-computer interaction. Virtual environments used in the experiments were regularly showcased to broader audience during STEM Fests and STEM Cafes. Findings from this project were disseminated through publications in international journals.</p><br> <p>  Last Modified: 12/23/2024<br> Modified by: Sachit&nbsp;Butail</p></div> <div class="porSideCol" ><div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)          </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2024/2033918/2033918_10691744_1734971096437_STEM_Fest_2021_Ground_Robot--rgov-214x142.jpg" original="/por/images/Reports/POR/2024/2033918/2033918_10691744_1734971096437_STEM_Fest_2021_Ground_Robot--rgov-800width.jpg" title="STEM Fest 2021 Ground Robot"><img src="/por/images/Reports/POR/2024/2033918/2033918_10691744_1734971096437_STEM_Fest_2021_Ground_Robot--rgov-66x44.jpg" alt="STEM Fest 2021 Ground Robot"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Graduate and undergraduate students showing virtual environments and ground robots used in the human-robot interaction experiments to younger audience during STEM Fest</div> <div class="imageCredit">Sachit Butail</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Sachit&nbsp;Butail <div class="imageTitle">STEM Fest 2021 Ground Robot</div> </div> </li><li> <a href="/por/images/Reports/POR/2024/2033918/2033918_10691744_1734971051240_STEM_Fest_2021_VE--rgov-214x142.jpeg" original="/por/images/Reports/POR/2024/2033918/2033918_10691744_1734971051240_STEM_Fest_2021_VE--rgov-800width.jpeg" title="STEM Fest 2021 VE"><img src="/por/images/Reports/POR/2024/2033918/2033918_10691744_1734971051240_STEM_Fest_2021_VE--rgov-66x44.jpeg" alt="STEM Fest 2021 VE"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Graduate student Arunim Bhattacharya helping young kids view the underwater virtual environment with five commonly found fish species within the Great Lakes</div> <div class="imageCredit">Sachit Butail</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Sachit&nbsp;Butail <div class="imageTitle">STEM Fest 2021 VE</div> </div> </li><li> <a href="/por/images/Reports/POR/2024/2033918/2033918_10691744_1734971176985_STEM_Fest_2022_Search_VR--rgov-214x142.jpg" original="/por/images/Reports/POR/2024/2033918/2033918_10691744_1734971176985_STEM_Fest_2022_Search_VR--rgov-800width.jpg" title="STEM Fest 2022 Search VR"><img src="/por/images/Reports/POR/2024/2033918/2033918_10691744_1734971176985_STEM_Fest_2022_Search_VR--rgov-66x44.jpg" alt="STEM Fest 2022 Search VR"></a> <div class="imageCaptionContainer"> <div class="imageCaption">STEM Fest visitors trying to search for a missing person in a replica of the Grand Canyon virtual environment</div> <div class="imageCredit">Sachit Butail</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Sachit&nbsp;Butail <div class="imageTitle">STEM Fest 2022 Search VR</div> </div> </li><li> <a href="/por/images/Reports/POR/2024/2033918/2033918_10691744_1734971327736_Grand_Canyon_VE--rgov-214x142.jpg" original="/por/images/Reports/POR/2024/2033918/2033918_10691744_1734971327736_Grand_Canyon_VE--rgov-800width.jpg" title="Grand Canyon VE"><img src="/por/images/Reports/POR/2024/2033918/2033918_10691744_1734971327736_Grand_Canyon_VE--rgov-66x44.jpg" alt="Grand Canyon VE"></a> <div class="imageCaptionContainer"> <div class="imageCaption">A snapshot of the Grand Canyon virtual environment used to conduct search experiments</div> <div class="imageCredit">Sachit Butail</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Sachit&nbsp;Butail <div class="imageTitle">Grand Canyon VE</div> </div> </li></ul> </div> </div></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[  In this project, we identified cues of interaction between humans and multiple robots in situations that require large area coverage such as search and rescue and environmental monitoring. Identifying such cues entailed creating new rich virtual environments and validating robust measures of cognitive load for human-subjects experiments. In particular, we designed and developed virtual environments inspired by the Grand Canyon National Park for search and rescue missions and Great Lakes for underwater monitoring missions. The Great Lakes environment included five different species of fish whose movement and social behavior was inspired by their real counterparts. We separated the problem of identifying nonverbal cues into two halves in terms of who gives the cues to whom: human to robots and robots to human.    With respect to nonverbal cues from human to robots, we validated the use of movement cues as possible indicators of psychological constructs such as prior knowledge, cognitive load, and situational awareness. These were investigated as part of two experimental studies, one in a laboratory setting and another in a virtual setting. Both studies revealed that speed, turn rate, and time spent staying still by the teleoperated robot can be used as reliable estimators of prior knowledge and can therefore be used as nonverbal cues for robots to read and adaptively assist the human in complex missions. Human-in-the-loop simulations revealed improved performance of the human-robot team over established methods.    With respect to nonverbal cues from multiple robots to humans we sought to design an alphabet inspired from collective behaviors that a swarm of robots could use to communicate complex information to a trained human as they navigate a dynamic environment. Experiments were conducted where a swarm of robots communicated site information to humans with different combinations of such nonverbal cues. These communication strategies were compared with text-based communication in terms of how they affected performance and cognitive load.    Several undergraduate and graduate students contributed to this project as they received training in experimental methods, dynamical systems modeling, statistical analysis, virtual reality, and human-computer interaction. Virtual environments used in the experiments were regularly showcased to broader audience during STEM Fests and STEM Cafes. Findings from this project were disseminated through publications in international journals.     Last Modified: 12/23/2024       Submitted by: SachitButail]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
