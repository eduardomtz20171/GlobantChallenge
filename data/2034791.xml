<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle><![CDATA[EAGER:Towards Adaptive and Robust Multimodal Emotion Recognition In-the-Wild]]></AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2020</AwardEffectiveDate>
<AwardExpirationDate>07/31/2023</AwardExpirationDate>
<AwardTotalIntnAmount>99916.00</AwardTotalIntnAmount>
<AwardAmount>99916</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Todd Leen</SignBlockName>
<PO_EMAI>tleen@nsf.gov</PO_EMAI>
<PO_PHON>7032927215</PO_PHON>
</ProgramOfficer>
<AbstractNarration><![CDATA[Emotions are essential to human life. They directly influence human perception and behaviors, and have big impacts on people's daily tasks, such as learning, social interaction, and decision-making. Automatic emotion recognition has found applications in many domains such as human-computer interaction, human-robot interaction, multimedia retrieval, social media analysis, and healthcare. Emotional states are expressed through a variety of channels including facial expression, voice prosody, spoken words, and body gestures. Automatic emotion recognition in real-world applications is a challenging task. Real-world emotions involve subtle expressive behaviors, different degrees of expressiveness in different channels, and the imperfect conditions such as background noise or music, poor illumination, and uncontrolled head poses for example. This EArly-concept Grant for Exploratory Research project aims to address the challenges of spontaneous emotion expressions and imperfect audio and video signals in-the-wild, and develop a novel multimodal emotion recognition system for real-world applications. The research will lead to advances in data collection, algorithm design, and bench-marking for the next generation of affective computing.&lt;br/&gt;&lt;br/&gt;This project consists of several research components. First, a multimodal dataset of spontaneous emotion expressions in-the-wild will be developed. The dataset will contain natural spontaneous emotion data in various challenging real life environments, and crowd-sourced ratings in different modalities (audio and video channels). A thorough benchmark analysis using this dataset will be conducted tostudy how different features, modalities, and signal impairments contribute to the success and failure of emotion recognition systems. Finally, novel multimodal emotion recognition algorithms will be designed using adaptive and robust multimodal learning and fusion. The research findings will be made available through dataset sharing, publications, talks, and open-source codes, allowing a multitude of developers, researchers, and companies to improve and evolve multimodal emotion recognition in real-world applications. The project will also provide novel research opportunities for graduate and undergraduate students, including women and minority students.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.]]></AbstractNarration>
<MinAmdLetterDate>07/30/2020</MinAmdLetterDate>
<MaxAmdLetterDate>07/30/2020</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2034791</AwardID>
<Investigator>
<FirstName>Houwei</FirstName>
<LastName>Cao</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Houwei Cao</PI_FULL_NAME>
<EmailAddress><![CDATA[hcao02@nyit.edu]]></EmailAddress>
<NSF_ID>000783681</NSF_ID>
<StartDate>07/30/2020</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>New York Institute of Technology</Name>
<CityName>NEW YORK</CityName>
<ZipCode>100237606</ZipCode>
<PhoneNumber>5166867737</PhoneNumber>
<StreetAddress>1855 BROADWAY</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>12</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY12</CONGRESS_DISTRICT_ORG>
<ORG_UEI_NUM>SVZSJHR2A4T6</ORG_UEI_NUM>
<ORG_LGL_BUS_NAME>NEW YORK INSTITUTE OF TECHNOLOGY</ORG_LGL_BUS_NAME>
<ORG_PRNT_UEI_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[New York Institute of Technology]]></Name>
<CityName>Old Westbury</CityName>
<StateCode>NY</StateCode>
<ZipCode>115688000</ZipCode>
<StreetAddress><![CDATA[Northern Boulevard]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY03</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<Appropriation>
<Code>0120</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Fund>
<Code>01002021DB</Code>
<Name><![CDATA[NSF RESEARCH & RELATED ACTIVIT]]></Name>
<FUND_SYMB_ID>040100</FUND_SYMB_ID>
</Fund>
<FUND_OBLG>2020~99916</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Automatic emotion recognition in real-world applications is a challenging task. Real-world emotions involve subtle expressive behaviors, different degrees of expressiveness in different channels, and imperfect conditions such as background noise or music, poor illumination, and uncontrolled head poses for example.</p> <p>Intellectual Merits Outcomes:</p> <p>This project addressed the various challenges of spontaneous emotion expressions and imperfect audio and video signals in-the-wild, and developed a novel robust multimodal emotion recognition system with inconsistent and subjective data for real-world applications.</p> <p>1. We first developed a multimodal emotional dataset with natural spontaneous emotion data in various challenging real-world environments. It contains 500 user generated videos collected from YouTube&nbsp;in three different scenarios of 1) naturalistic well-lit conditions (close to lab environment), 2) real-world human-computer interaction applications, and 3) arbitrary environments with completely uncontrolled audio-visual conditions.</p> <p>2. Spontaneous emotion recognition in-the-wild has to be robust against all the complicating factors in the emotion communication pipeline. We investigated how different factors contribute to various inconsistencies and subjectivities of multimodal spontaneous emotion data, and further proposed a robust multimodal emotion recognition system. A novel preference learning framework was &nbsp;introduced that simultaneously considers the discrepancies and agreements between the intended and the perceived labels in different modalities of audio-only, visual-only, and audio-visual, as well as the consistency among the perceptual ratings of all raters.</p> <p>3. To better understand the attentional deployment in the processing of facial expression, we investigated human&rsquo;s visual attention and fixation patterns when identifying six basic emotions of expressive talking faces, and further developed a novel&nbsp;emotion recognition system&nbsp;to predict the perceived emotions based on her gaze patterns and fixation sequences. Moreover, we investigated how facial expressions differ across different genders and races, and identified group-specific gaze patterns for different gender and race groups during emotion recognition. We further built the gender-dependent and race-dependent models.</p> <p>4. The COVID-19 pandemic has affected more than 200 countries on all continents over the past three years. Wearing face masks is one of the essential ways to control the spread of the virus. While face masks effectively reduce the risk of infection, it has created a new normal, significantly changing how people communicate in daily life. On one hand, masks can muffle sounds, particularly the high-frequency sounds, which makes it harder to understand certain voices and speech, as well as the paralinguistic information. On the other hand, masks also block facial expressions and prevent people from seeing and reading lips, which can help people to better understand what they are hearing and the emotional state of the speaker. We systematically investigated how different types of masks affect automatic emotion classification. Specifically, we studied how muffled speech and occluded facial expressions change the prediction of emotions and further investigated the individual contribution of audio, visual, and audio-visual modalities to the prediction of emotion with and without mask.</p> <p>5. We participated the INTERSPEECH 2020 Computational Paralinguistic Challenge and investigated various acoustic and lexical representations for the two&nbsp;new emerging tasks:&nbsp;recognizing emotions on elderly population, and predicting&nbsp;whether a speaker wears a surgical mask or not.&nbsp;We have shown that the proposed FV-MFCC feature is very promising. It has very strong prediction power on its own and can also provide complementary information when fused with other acoustic features.&nbsp;We also discussed the potential for improving prediction by combining the lexical and acoustic modalities together. Our proposed systems significantly outperform the official baselines on the test set in both the Mask Sub-challenges, and Elderly Sub-challenges, and won the runner-up in the&nbsp;Elderly Sub-challenges.</p> <p>Broader Impact Outcomes:</p> <p>1. Two PhD students and four master students have participated in this research. They all gained valuable experience to use AI and machine learning tools to solve practical problems involving affective computing, speech and language processing, and user behaviors analysis. Two graduated master students are now employed by top IT companies in the USA, and the other two graduated master students continued their PhD study.</p> <p>2. Five Undergraduate students have participated in this research. Two undergraduate students joined the project through REU program, and the other three students worked on the project as their senior design project. All students gained valuable hands-on research experience, and published papers in top-tier conferences.&nbsp;&nbsp;</p> <p>3. Our results were published in top-tier journal and conferences including IEEE Transactions on Affective Computing, ACII, ICASSP, and INTERSPEECH. We have presented our results through conferences and seminar talks to academia and industry.</p><br> <p>  Last Modified: 12/07/2023<br> Modified by: Houwei&nbsp;Cao</p></div> <div class="porSideCol" ></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[  Automatic emotion recognition in real-world applications is a challenging task. Real-world emotions involve subtle expressive behaviors, different degrees of expressiveness in different channels, and imperfect conditions such as background noise or music, poor illumination, and uncontrolled head poses for example.   Intellectual Merits Outcomes:   This project addressed the various challenges of spontaneous emotion expressions and imperfect audio and video signals in-the-wild, and developed a novel robust multimodal emotion recognition system with inconsistent and subjective data for real-world applications.   1. We first developed a multimodal emotional dataset with natural spontaneous emotion data in various challenging real-world environments. It contains 500 user generated videos collected from YouTubein three different scenarios of 1) naturalistic well-lit conditions (close to lab environment), 2) real-world human-computer interaction applications, and 3) arbitrary environments with completely uncontrolled audio-visual conditions.   2. Spontaneous emotion recognition in-the-wild has to be robust against all the complicating factors in the emotion communication pipeline. We investigated how different factors contribute to various inconsistencies and subjectivities of multimodal spontaneous emotion data, and further proposed a robust multimodal emotion recognition system. A novel preference learning framework was introduced that simultaneously considers the discrepancies and agreements between the intended and the perceived labels in different modalities of audio-only, visual-only, and audio-visual, as well as the consistency among the perceptual ratings of all raters.   3. To better understand the attentional deployment in the processing of facial expression, we investigated humans visual attention and fixation patterns when identifying six basic emotions of expressive talking faces, and further developed a novelemotion recognition systemto predict the perceived emotions based on her gaze patterns and fixation sequences. Moreover, we investigated how facial expressions differ across different genders and races, and identified group-specific gaze patterns for different gender and race groups during emotion recognition. We further built the gender-dependent and race-dependent models.   4. The COVID-19 pandemic has affected more than 200 countries on all continents over the past three years. Wearing face masks is one of the essential ways to control the spread of the virus. While face masks effectively reduce the risk of infection, it has created a new normal, significantly changing how people communicate in daily life. On one hand, masks can muffle sounds, particularly the high-frequency sounds, which makes it harder to understand certain voices and speech, as well as the paralinguistic information. On the other hand, masks also block facial expressions and prevent people from seeing and reading lips, which can help people to better understand what they are hearing and the emotional state of the speaker. We systematically investigated how different types of masks affect automatic emotion classification. Specifically, we studied how muffled speech and occluded facial expressions change the prediction of emotions and further investigated the individual contribution of audio, visual, and audio-visual modalities to the prediction of emotion with and without mask.   5. We participated the INTERSPEECH 2020 Computational Paralinguistic Challenge and investigated various acoustic and lexical representations for the twonew emerging tasks:recognizing emotions on elderly population, and predictingwhether a speaker wears a surgical mask or not.We have shown that the proposed FV-MFCC feature is very promising. It has very strong prediction power on its own and can also provide complementary information when fused with other acoustic features.We also discussed the potential for improving prediction by combining the lexical and acoustic modalities together. Our proposed systems significantly outperform the official baselines on the test set in both the Mask Sub-challenges, and Elderly Sub-challenges, and won the runner-up in theElderly Sub-challenges.   Broader Impact Outcomes:   1. Two PhD students and four master students have participated in this research. They all gained valuable experience to use AI and machine learning tools to solve practical problems involving affective computing, speech and language processing, and user behaviors analysis. Two graduated master students are now employed by top IT companies in the USA, and the other two graduated master students continued their PhD study.   2. Five Undergraduate students have participated in this research. Two undergraduate students joined the project through REU program, and the other three students worked on the project as their senior design project. All students gained valuable hands-on research experience, and published papers in top-tier conferences.   3. Our results were published in top-tier journal and conferences including IEEE Transactions on Affective Computing, ACII, ICASSP, and INTERSPEECH. We have presented our results through conferences and seminar talks to academia and industry.     Last Modified: 12/07/2023       Submitted by: HouweiCao]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
