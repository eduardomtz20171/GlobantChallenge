<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle><![CDATA[SBIR Phase I:  Assistive Robots for Personal Care and COVID-19 Protection]]></AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>01/01/2021</AwardEffectiveDate>
<AwardExpirationDate>03/31/2024</AwardExpirationDate>
<AwardTotalIntnAmount>255756.00</AwardTotalIntnAmount>
<AwardAmount>255756</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>15030000</Code>
<Directorate>
<Abbreviation>TIP</Abbreviation>
<LongName>Dir for Tech, Innovation, &amp; Partnerships</LongName>
</Directorate>
<Division>
<Abbreviation>TI</Abbreviation>
<LongName>Translational Impacts</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Alastair Monk</SignBlockName>
<PO_EMAI>amonk@nsf.gov</PO_EMAI>
<PO_PHON>7032924392</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase I project advances the state-of-the art of an emerging class of vision-based, autonomous navigation technologies to open new possibilities for low-cost/high-performance personal assistive robots. The robotics solution enables mobility-impaired individuals to have more agency over their environment and enjoy a higher quality-of-life. This helps address the severe shortage of caregivers for the elderly and post-acute care patients by empowering individuals to maintain their independence, extending the impact of caregivers, and reducing the cost of care in both home and facility settings.  Additionally, by providing affordable and reliable isolation support in COVID-19 care settings, the proposed solution can help decrease the financial burden and increase the public health outcomes associated with COVID-19 disease management. The core robotics solution has an immediate addressable market of 11 million high-needs users in the U.S. alone, with projected revenues of roughly $1.65 Billion five years after product launch. Further commercialization opportunities come from licensing parts of the developed navigation technology for other robotics applications and developing an ecosystem of complementary products around the core robotics solution.&lt;br/&gt;&lt;br/&gt;This Small Business Innovation Research Phase I project seeks to enable a new generation of assistive service robots that are comparable to commercial robots in performance, but significantly more affordable for individual use and personal care applications. The innovation adopts emerging visual positioning technologies from Augmented Reality to enable robust navigation for mobile robots using low-cost, consumer-grade electronics, while addressing a key limitation of visual positioning systems namely, that external lighting conditions and other changes in an environment can dramatically impact their performance. The innovation addresses these challenges via a combination of hardware and software that learns and stabilizes the highest value visual elements of the environment to maintain persistency across lighting conditions and long periods of time â€” a development critical to making assistive robots cost-effective for adoption at a large scale. Research objectives include: fully developing and integrating the visual persistency system, to achieve accurate and replicable robot navigation performance across a representative range of lighting conditions and visual characteristics of the target operating environments and benchmarking the resulting solution against state-of-the art technologies, to demonstrate its superior performance (i.e., it can successfully localize in at least 90% of cases where other solutions fail).&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>12/21/2020</MinAmdLetterDate>
<MaxAmdLetterDate>02/13/2023</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.041, 47.084</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2036684</AwardID>
<Investigator>
<FirstName>Michael</FirstName>
<LastName>Dooley</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Michael Dooley</PI_FULL_NAME>
<EmailAddress><![CDATA[mike@labradorsystems.com]]></EmailAddress>
<NSF_ID>000825810</NSF_ID>
<StartDate>12/21/2020</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name><![CDATA[LABRADOR SYSTEMS, INC.]]></Name>
<CityName>OAK PARK</CityName>
<ZipCode>913773844</ZipCode>
<PhoneNumber>4158476326</PhoneNumber>
<StreetAddress><![CDATA[351 SOUTHRIDGE DRIVE]]></StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>26</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA26</CONGRESS_DISTRICT_ORG>
<ORG_UEI_NUM>W2J5MPF9UYG8</ORG_UEI_NUM>
<ORG_LGL_BUS_NAME>LABRADOR SYSTEMS INC</ORG_LGL_BUS_NAME>
<ORG_PRNT_UEI_NUM>E2PQVDBMY7H7</ORG_PRNT_UEI_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[LABRADOR SYSTEMS, INC.]]></Name>
<CityName>Calabasas</CityName>
<StateCode>CA</StateCode>
<ZipCode>913021440</ZipCode>
<StreetAddress><![CDATA[5111 Douglas Fir Rd]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>32</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA32</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>537100</Code>
<Text>SBIR Phase I</Text>
</ProgramElement>
<ProgramReference>
<Code>096Z</Code>
<Text>COVID-19 Research</Text>
</ProgramReference>
<ProgramReference>
<Code>6840</Code>
<Text>ROBOTICS</Text>
</ProgramReference>
<ProgramReference>
<Code>8033</Code>
<Text>Hardware Software Integration</Text>
</ProgramReference>
<Appropriation>
<Code>0121</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Fund>
<Code>01002122DB</Code>
<Name><![CDATA[NSF RESEARCH & RELATED ACTIVIT]]></Name>
<FUND_SYMB_ID>040100</FUND_SYMB_ID>
</Fund>
<FUND_OBLG>2021~255756</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Labrador's core mission is to develop a new generation of assistive robots to help people live more independently and support their caregivers. This requires building highly functional autonomous mobile robots that can be affordable on a personal level and yet also operate in complex unstructured environments, such as in people's homes. (See Images 1 &amp; 2 for examples.)&nbsp; Applications include enabling seniors to live at home longer, reducing caregiver workload and supporting disease isolation within care facilities.</p> <p>To date these types of personal robots have not existed in our everyday lives. On one side, we have large commercial autonomous robots that generally operate in large environments and serve the needs of many people. (e.g. warehouse robots that help fulfill 1000's of customer orders) These robots almost always rely upon industrial hardware such as LiDAR sensors to enable their navigation and are too expensive for personal use. On the other side, we have small consumer robots that operate in small spaces, such as robotic vacuum cleaners, that are affordable for an individual or family to own. However, these robots are cost optimized to support a very narrow mission and aren't equipped with the systems needed to safely and reliably operate much larger and more functional robots around people.</p> <p>The purpose of the research funded by this grant is to help fill this technology gap. More specifically, our objective is to create and validate a navigation system for operating larger and more capable robots in small personal environments that can perform at the high level of reliability of established industrial solutions, but be powered by low-cost consumer electronic components that are manufactured at mass market scale.</p> <p>Labrador's navigation system uses low-cost cameras, sensors and processors commonly found in mobile phones and other consumer electronic devices for it is base hardware. The navigation software utilizes computer vision algorithms used in augmented reality to map and track positions within the 3D space of an indoor environment.</p> <p>Off-the-shelf, this combination of hardware and software can provide highly functional demos as well as limited point-in-time applications such as playing a game on an augmented reality headset. However, just as many people have discovered the limits of their mobile phone camera when trying to take photos in challenging lighting conditions, visual positioning systems are also highly subject to variations in lighting within the environment. (See Image 3)</p> <p>Labrador addresses this problem through the company's Feature Persistency System. (FPS) In basic terms, Labrador's FPS integrates additional hardware and software modules that stabilize the visual features in the surrounding environment that the robot uses to position itself. (Labrador has patented this system, including as described in US11287262B2.)</p> <p>The FPS works both during the initial visual mapping of an interior environment as well as the subsequent navigation through that space over time despite dramatic changes in lighting conditions. This capability is critical for achieving commercial-grade performance and reliability on low-cost vision-based robots, enabling for example mapping a home on a morning of bright sunny day and then having the robot navigate just as reliably 6 months later in a dimly lit room at dusk or in the pitch black of night.&nbsp;</p> <p>During this grant Labrador successfully completed development, integration and testing of the FPS. Testing and data collection were performed across a range of settings and changing lighting conditions, in both a controlled lab setting as well as in multiple real-world environments, including institutional settings (senior care facilities) and residential settings. (private homes)&nbsp;</p> <p>In repeated trials, the navigation system enhanced with FPS performed consistently well in detecting and matching visual features and accurately estimating the robot's position. This showed considerable improvement over trials where No FPS or only partial FPS (called Half FPS) were used. (See Figure 1)</p> <p>To verify these findings against an external benchmark, Labrador's navigation system with FPS was tested against a 3<sup>rd</sup> party visual positioning solution from a leading company in computer vision. Both systems were mounted on a mobile robot and tested at night in a controlled office/lab environment at decreasing levels of illumination.&nbsp;</p> <p>While the 3<sup>rd</sup> party solution provided accurate estimates of the robot's position at the higher levels of illumination, the 3<sup>rd</sup> party solution failed to generate a position estimate at the lower levels of illumination. By contrast, Labrador's navigation system with the Full FPS enabled provided highly accurate position estimates in all lighting conditions. (See Figure 2)</p> <p>Finally, Labrador made improvements to mapping algorithms to reduce field training time by up to 40% while maintaining the reliability of the navigation system.</p> <p>Overall, we believe this successful development of Labrador's Feature Persistency System is a key step forward in expanding the use of more functional robots in personal settings, as well as enabling new types of AI-powered vision applications to run on the same configuration of low-cost hardware.</p><br> <p>  Last Modified: 06/28/2024<br> Modified by: Michael&nbsp;Dooley</p></div> <div class="porSideCol" ><div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)          </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2024/2036684/2036684_10711647_1719612935745_Image_1--rgov-214x142.jpg" original="/por/images/Reports/POR/2024/2036684/2036684_10711647_1719612935745_Image_1--rgov-800width.jpg" title="Image 1"><img src="/por/images/Reports/POR/2024/2036684/2036684_10711647_1719612935745_Image_1--rgov-66x44.jpg" alt="Image 1"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Image 1: Rendering of the Labrador Retriever robot in an example use case within a home. Pilot robots used during the testing and data collection in this research look very similar to the robot depicted in this scene.</div> <div class="imageCredit">Licensed Adobe Stock Image + Labrador Systems. Inc.</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Michael&nbsp;Dooley <div class="imageTitle">Image 1</div> </div> </li><li> <a href="/por/images/Reports/POR/2024/2036684/2036684_10711647_1719615631583_Figure_1--rgov-214x142.png" original="/por/images/Reports/POR/2024/2036684/2036684_10711647_1719615631583_Figure_1--rgov-800width.png" title="Figure 1"><img src="/por/images/Reports/POR/2024/2036684/2036684_10711647_1719615631583_Figure_1--rgov-66x44.png" alt="Figure 1"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Figure 1: Distribution of error in the estimated position of the robot generated by three different configurations of Labrador's navigation system. Performance was best (had the least amount of error) with the full implementation of Labrador's Feature Persistency System. (FPS)</div> <div class="imageCredit">Labrador Systems, Inc.</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Michael&nbsp;Dooley <div class="imageTitle">Figure 1</div> </div> </li><li> <a href="/por/images/Reports/POR/2024/2036684/2036684_10711647_1719615566911_Figure_2--rgov-214x142.png" original="/por/images/Reports/POR/2024/2036684/2036684_10711647_1719615566911_Figure_2--rgov-800width.png" title="Figure 2"><img src="/por/images/Reports/POR/2024/2036684/2036684_10711647_1719615566911_Figure_2--rgov-66x44.png" alt="Figure 2"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Figure 2: Position error comparison between a 3rd Party solution and Labrador's Navigation System with the FPS fully enabled. The 3rd Party solution was not able to estimate a position at the four lowest levels of illumination, where the Labrador with FPS solution achieved high accuracy.</div> <div class="imageCredit">Labrador Systems, Inc.</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Michael&nbsp;Dooley <div class="imageTitle">Figure 2</div> </div> </li><li> <a href="/por/images/Reports/POR/2024/2036684/2036684_10711647_1719615877356_Image_2--rgov-214x142.jpg" original="/por/images/Reports/POR/2024/2036684/2036684_10711647_1719615877356_Image_2--rgov-800width.jpg" title="Image 2"><img src="/por/images/Reports/POR/2024/2036684/2036684_10711647_1719615877356_Image_2--rgov-66x44.jpg" alt="Image 2"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Image 2: Illustration of the navigation routes and destinations for the Labrador Retriever robot within an example residential setting.</div> <div class="imageCredit">Labrador Systems, Inc.</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Michael&nbsp;Dooley <div class="imageTitle">Image 2</div> </div> </li><li> <a href="/por/images/Reports/POR/2024/2036684/2036684_10711647_1719615954224_Image_3--rgov-214x142.png" original="/por/images/Reports/POR/2024/2036684/2036684_10711647_1719615954224_Image_3--rgov-800width.png" title="Image 3"><img src="/por/images/Reports/POR/2024/2036684/2036684_10711647_1719615954224_Image_3--rgov-66x44.png" alt="Image 3"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Image 3: Photos of the same location within a home at four different times show the deterioration in recognition of visual features as the lighting changes. Visual positioning systems often rely on features like the ones highlighted in the photos to track their position.</div> <div class="imageCredit">Labrador Systems, Inc.</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Michael&nbsp;Dooley <div class="imageTitle">Image 3</div> </div> </li></ul> </div> </div></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[  Labrador's core mission is to develop a new generation of assistive robots to help people live more independently and support their caregivers. This requires building highly functional autonomous mobile robots that can be affordable on a personal level and yet also operate in complex unstructured environments, such as in people's homes. (See Images 1 & 2 for examples.) Applications include enabling seniors to live at home longer, reducing caregiver workload and supporting disease isolation within care facilities.   To date these types of personal robots have not existed in our everyday lives. On one side, we have large commercial autonomous robots that generally operate in large environments and serve the needs of many people. (e.g. warehouse robots that help fulfill 1000's of customer orders) These robots almost always rely upon industrial hardware such as LiDAR sensors to enable their navigation and are too expensive for personal use. On the other side, we have small consumer robots that operate in small spaces, such as robotic vacuum cleaners, that are affordable for an individual or family to own. However, these robots are cost optimized to support a very narrow mission and aren't equipped with the systems needed to safely and reliably operate much larger and more functional robots around people.   The purpose of the research funded by this grant is to help fill this technology gap. More specifically, our objective is to create and validate a navigation system for operating larger and more capable robots in small personal environments that can perform at the high level of reliability of established industrial solutions, but be powered by low-cost consumer electronic components that are manufactured at mass market scale.   Labrador's navigation system uses low-cost cameras, sensors and processors commonly found in mobile phones and other consumer electronic devices for it is base hardware. The navigation software utilizes computer vision algorithms used in augmented reality to map and track positions within the 3D space of an indoor environment.   Off-the-shelf, this combination of hardware and software can provide highly functional demos as well as limited point-in-time applications such as playing a game on an augmented reality headset. However, just as many people have discovered the limits of their mobile phone camera when trying to take photos in challenging lighting conditions, visual positioning systems are also highly subject to variations in lighting within the environment. (See Image 3)   Labrador addresses this problem through the company's Feature Persistency System. (FPS) In basic terms, Labrador's FPS integrates additional hardware and software modules that stabilize the visual features in the surrounding environment that the robot uses to position itself. (Labrador has patented this system, including as described in US11287262B2.)   The FPS works both during the initial visual mapping of an interior environment as well as the subsequent navigation through that space over time despite dramatic changes in lighting conditions. This capability is critical for achieving commercial-grade performance and reliability on low-cost vision-based robots, enabling for example mapping a home on a morning of bright sunny day and then having the robot navigate just as reliably 6 months later in a dimly lit room at dusk or in the pitch black of night.   During this grant Labrador successfully completed development, integration and testing of the FPS. Testing and data collection were performed across a range of settings and changing lighting conditions, in both a controlled lab setting as well as in multiple real-world environments, including institutional settings (senior care facilities) and residential settings. (private homes)   In repeated trials, the navigation system enhanced with FPS performed consistently well in detecting and matching visual features and accurately estimating the robot's position. This showed considerable improvement over trials where No FPS or only partial FPS (called Half FPS) were used. (See Figure 1)   To verify these findings against an external benchmark, Labrador's navigation system with FPS was tested against a 3rd party visual positioning solution from a leading company in computer vision. Both systems were mounted on a mobile robot and tested at night in a controlled office/lab environment at decreasing levels of illumination.   While the 3rd party solution provided accurate estimates of the robot's position at the higher levels of illumination, the 3rd party solution failed to generate a position estimate at the lower levels of illumination. By contrast, Labrador's navigation system with the Full FPS enabled provided highly accurate position estimates in all lighting conditions. (See Figure 2)   Finally, Labrador made improvements to mapping algorithms to reduce field training time by up to 40% while maintaining the reliability of the navigation system.   Overall, we believe this successful development of Labrador's Feature Persistency System is a key step forward in expanding the use of more functional robots in personal settings, as well as enabling new types of AI-powered vision applications to run on the same configuration of low-cost hardware.     Last Modified: 06/28/2024       Submitted by: MichaelDooley]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
