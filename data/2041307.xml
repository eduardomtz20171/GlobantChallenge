<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle><![CDATA[EAGER: Learning Transferable Visual Features]]></AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2020</AwardEffectiveDate>
<AwardExpirationDate>08/31/2024</AwardExpirationDate>
<AwardTotalIntnAmount>241143.00</AwardTotalIntnAmount>
<AwardAmount>241143</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jie Yang</SignBlockName>
<PO_EMAI>jyang@nsf.gov</PO_EMAI>
<PO_PHON>7032924768</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Artificial intelligence and machine learning have shown great promise for many applications in computer vision, multimedia, robotics, autonomous driving, medical imaging analysis, assistive technology, etc. In order to obtain better performance, large-scale labeled data are generally required to train deep neural networks. To avoid extensive cost of collecting and annotating large-scale data, a major goal of machine learning is to exploit new algorithms to learn general features from limited labeled or unlabeled data. This project aims to explore self-supervised methods to learn general visual features across different modalities from large scale data without using any human-labeled annotations. The learned general visual features can then be transferred to many different applications, such as human activity analysis, 3D scene understanding, and assistive technologies. The research is tightly integrated with graduate/undergraduate education in the City University of New York, a minority serving institution and one of the most diverse campuses in the United States.&lt;br/&gt;&lt;br/&gt;Most prior work of visual feature learning has focused on a single modality of data. This research is to explore methods of learning transferable visual features from multiple modalities including texts, audios, images, videos, and 3D data as well as investigate new loss functions to find optimal features. In particular, the will conduct the following research tasks: (1) exploration of new algorithms to effectively learn transferable visual features across multimodalities without requesting human annotations of large scale data; (2) investigation of effective algorithms and loss functions for bridging the gap among different modalities to handle the different feature distributions from different modalities; and (3) evaluation and generalization of the proposed technologies on different applications including human activity analysis, 3D scene understanding, and medical image processing. The project will result in new algorithms to effectively learn transferable features from multimodality data including texts, images, videos, and 3D data without depending on data annotations. The work will lead to advances in computer vision and machine learning technologies and the outcome algorithms will be general and broadly applicable across different real-world applications.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>07/29/2020</MinAmdLetterDate>
<MaxAmdLetterDate>07/29/2020</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2041307</AwardID>
<Investigator>
<FirstName>YingLi</FirstName>
<LastName>Tian</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>YingLi Tian</PI_FULL_NAME>
<EmailAddress><![CDATA[ytian@ccny.cuny.edu]]></EmailAddress>
<NSF_ID>000517490</NSF_ID>
<StartDate>07/29/2020</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name><![CDATA[CUNY City College]]></Name>
<CityName>NEW YORK</CityName>
<ZipCode>100319101</ZipCode>
<PhoneNumber>2126505418</PhoneNumber>
<StreetAddress><![CDATA[160 CONVENT AVE]]></StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY13</CONGRESS_DISTRICT_ORG>
<ORG_UEI_NUM>L952KGDMSLV5</ORG_UEI_NUM>
<ORG_LGL_BUS_NAME>RESEARCH FOUNDATION OF THE CITY UNIVERSITY OF NEW YORK</ORG_LGL_BUS_NAME>
<ORG_PRNT_UEI_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[CUNY City College]]></Name>
<CityName>New York</CityName>
<StateCode>NY</StateCode>
<ZipCode>100319101</ZipCode>
<StreetAddress><![CDATA[Convent Ave at 138th St]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY13</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>749500</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<Appropriation>
<Code>0120</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Fund>
<Code>01002021DB</Code>
<Name><![CDATA[NSF RESEARCH & RELATED ACTIVIT]]></Name>
<FUND_SYMB_ID>040100</FUND_SYMB_ID>
</Fund>
<FUND_OBLG>2020~241143</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Recent<strong> </strong>deep learning-based methods have shown great promise for visual feature learning from images and videos for computer vision applications. To obtain better performance and overcome the overfitting problem, large-scale labeled data are generally required to train deep neural networks. To avoid extensive cost of collecting and annotating large-scale datasets, many researchers proposed unsupervised/self-supervised/weakly-supervised learning methods from large-scale unlabeled data without using any human-annotated labels. This project aims to explore self-supervised or weakly-supervised methods to learn general visual features across different modalities in both spatial and frequency domains from existing large-scale datasets without using any annotations labeled by humans. The learned visual features can then be transferred to many other applications in computer vision, multimedia, medical imaging analysis, and assistive technology.</p> <p>Most prior work of visual feature learning has focused on single modality or needs to train the feature models in advance. <em><span style="text-decoration: underline;">The goal of this research is to explore methods of learning transferable visual features from multiple modalities including texts, audios, images, videos, and 3D data (e.g. point cloud, mesh, etc.) in both spatial and frequency domains, as well as investigate new loss functions to find optimal features</span>.</em> In particular, this project focuses on learning transferrable visual features from different modalities in both spatial and frequency domains on the following research tasks: 1) exploration of new algorithms to effectively learn transferable visual features cross multimodalities without requesting annotations of large scale data;&nbsp;2) investigation of effective algorithms for bridging the gap among different modalities to handle the different feature distributions from different modalities; 3) Study of learning transferrable features in frequency domain; and 4) evaluation and generalization of the proposed technologies on different applications such as object recognition, human activity analysis, cross-modal retrieval, medical imaging analysis, etc.</p> <p>We have developed a set of new algorithms of learning different visual features. The learned features can be further applied to many applications in computer vision, multimedia applications, and assistive technology such as cross-modal retrieval, scene understanding, human behavior analysis, automatic vehicle driving, medical imaging, American Sign Language recognition etc. It deepens our understanding of how to learn general visual features without depending on human labeled annotations and transfers the learned features to different domains. The findings from the proposed research directly benefit researchers in computer vision, multimedia, and assistive technology.&nbsp;</p> <p>The City College of New York (CCNY) is the most diverse campus in the U.S. and is a minority serving institution. This research project attracted graduate and undergraduate students in underrepresented groups to STEM disciplines and provides an opportunity for both graduate and undergraduate students to work on cutting-edge research. The research results and datasets are disseminated via publications, websites, media, seminars, and regular courses. This project also enhances our curriculum through seminars and inclusion of research topics (and hands-on experiences) in undergraduate and graduate course of &ldquo;Digital Image Processing.&rdquo; This project provided an enriching experience that will serve the students in their future endeavors.</p> <p><br />The research also holds significant social and economic impact. Self-supervised and weakly-supervised learning methods enable the extraction of visual features from large-scale datasets without manual annotations. Unlike supervised methods that depend on labeled data, self-supervised methods define a pretext task and learn features through this task. This approach scales efficiently to large datasets and has recently achieved performance on par with or exceeding traditional supervised methods in some downstream tasks. Technologies that automatically learn important features without heavy reliance on human annotations lay a strong foundation for applications in autonomous driving, object recognition, human activity analysis, cross-modal retrieval, and medical imaging, thereby having a long-term economic impact.<br /><br />In summary, with the support of this project, five PhD students and one undergraduate student (now pursuing a PhD) have participated, gaining interdisciplinary knowledge and research experience in computer vision, machine learning, medical image analysis, and assistive technologies. Their experience has equipped them with problem-solving skills and theoretical model development capabilities. Additionally, we have published 23 papers in conferences and journals, have one paper under review, and supported four PhD dissertations in part through this project.</p><br> <p>  Last Modified: 09/03/2024<br> Modified by: Yingli&nbsp;Tian</p></div> <div class="porSideCol" ></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[  Recent deep learning-based methods have shown great promise for visual feature learning from images and videos for computer vision applications. To obtain better performance and overcome the overfitting problem, large-scale labeled data are generally required to train deep neural networks. To avoid extensive cost of collecting and annotating large-scale datasets, many researchers proposed unsupervised/self-supervised/weakly-supervised learning methods from large-scale unlabeled data without using any human-annotated labels. This project aims to explore self-supervised or weakly-supervised methods to learn general visual features across different modalities in both spatial and frequency domains from existing large-scale datasets without using any annotations labeled by humans. The learned visual features can then be transferred to many other applications in computer vision, multimedia, medical imaging analysis, and assistive technology.   Most prior work of visual feature learning has focused on single modality or needs to train the feature models in advance. The goal of this research is to explore methods of learning transferable visual features from multiple modalities including texts, audios, images, videos, and 3D data (e.g. point cloud, mesh, etc.) in both spatial and frequency domains, as well as investigate new loss functions to find optimal features. In particular, this project focuses on learning transferrable visual features from different modalities in both spatial and frequency domains on the following research tasks: 1) exploration of new algorithms to effectively learn transferable visual features cross multimodalities without requesting annotations of large scale data;2) investigation of effective algorithms for bridging the gap among different modalities to handle the different feature distributions from different modalities; 3) Study of learning transferrable features in frequency domain; and 4) evaluation and generalization of the proposed technologies on different applications such as object recognition, human activity analysis, cross-modal retrieval, medical imaging analysis, etc.   We have developed a set of new algorithms of learning different visual features. The learned features can be further applied to many applications in computer vision, multimedia applications, and assistive technology such as cross-modal retrieval, scene understanding, human behavior analysis, automatic vehicle driving, medical imaging, American Sign Language recognition etc. It deepens our understanding of how to learn general visual features without depending on human labeled annotations and transfers the learned features to different domains. The findings from the proposed research directly benefit researchers in computer vision, multimedia, and assistive technology.   The City College of New York (CCNY) is the most diverse campus in the U.S. and is a minority serving institution. This research project attracted graduate and undergraduate students in underrepresented groups to STEM disciplines and provides an opportunity for both graduate and undergraduate students to work on cutting-edge research. The research results and datasets are disseminated via publications, websites, media, seminars, and regular courses. This project also enhances our curriculum through seminars and inclusion of research topics (and hands-on experiences) in undergraduate and graduate course of Digital Image Processing. This project provided an enriching experience that will serve the students in their future endeavors.    The research also holds significant social and economic impact. Self-supervised and weakly-supervised learning methods enable the extraction of visual features from large-scale datasets without manual annotations. Unlike supervised methods that depend on labeled data, self-supervised methods define a pretext task and learn features through this task. This approach scales efficiently to large datasets and has recently achieved performance on par with or exceeding traditional supervised methods in some downstream tasks. Technologies that automatically learn important features without heavy reliance on human annotations lay a strong foundation for applications in autonomous driving, object recognition, human activity analysis, cross-modal retrieval, and medical imaging, thereby having a long-term economic impact.  In summary, with the support of this project, five PhD students and one undergraduate student (now pursuing a PhD) have participated, gaining interdisciplinary knowledge and research experience in computer vision, machine learning, medical image analysis, and assistive technologies. Their experience has equipped them with problem-solving skills and theoretical model development capabilities. Additionally, we have published 23 papers in conferences and journals, have one paper under review, and supported four PhD dissertations in part through this project.     Last Modified: 09/03/2024       Submitted by: YingliTian]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
