<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle><![CDATA[Collaborative Research: FMitF: Track I: Game Theoretic Updates for Network and Cloud Functions]]></AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2020</AwardEffectiveDate>
<AwardExpirationDate>09/30/2024</AwardExpirationDate>
<AwardTotalIntnAmount>295000.00</AwardTotalIntnAmount>
<AwardAmount>295000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Pavithra Prabhakar</SignBlockName>
<PO_EMAI>pprabhak@nsf.gov</PO_EMAI>
<PO_PHON>7032922585</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Updates are common in cloud-computing networks, and they occur for many reasons. Some network updates are planned while others are unplanned and automated. Since network updates can take seconds or minutes to complete, and cloud-computing networks must be "always on", updates must be efficient and transparent. Researchers have proposed various abstractions for network updating that leverage advances in formal methods to synthesize update plans and protocols, ensuring that the system remains well-behaved during an ongoing update. However, despite several high-profile cases of network updates gone wrong, operators continue to use relatively naive approaches. We investigate key shortcomings of prior work on update abstractions that limit their utility and widespread use in practice, and develop a new abstraction that addresses the heterogeneity, scale, and dynamic nature of real-world updates. The project's novelties are (1) a new game-theoretic foundation for network updates, (2) algorithms for synthesizing update controllers that are robust to failures and changing conditions during the update, (3) algorithms for explaining update failures, (4) a language design that allows synthesized controllers to be safely modified, and (5) implementations and evaluations of these mechanisms for virtual network functions and serverless-computing platforms. The project provides network operators with tools that make updates to networked systems easier, safer, and more reliable, and develops a framework that makes datacenter computing more reliable and secure.&lt;br/&gt;&lt;br/&gt;Some specific key shortcomings of previous work on network updates are the following. (1) They assume that the network behaves predictably during the update. However, at scale, network demands and concurrent updates can cause unpredictable or even adversarial behavior in response to the update. (2) They have limited explanatory power when an update plan cannot be found or cannot be completed. (3) They make it hard for operators to choose between alternative update plans. This project consists of a comprehensive research plan to address these shortcomings. The key technical innovation is a formulation of updates as the search for a winning strategy in a two-player game, between the operator (or control plane) and the network. This formulation allows a uniform modeling of key elements, including hardware and software failures, variations in demand, and the addition and removal of network elements. To produce updates that are robust to changing conditions and failures, this work uses program-synthesis techniques to automatically generate an update controller that corresponds to a winning strategy in the game. To help operators when fatal errors occur, the project develops algorithms that exploit this game-theoretic formulation to explain the root cause of update failures and present alternatives. Finally, to give operators more control over updates, the investigators develop approaches for synthesizing update controllers that are interpretable and modifiable. The game-theoretic formulation is applicable to several kinds of networked systems, and the project will instantiate and evaluate our tools for platforms that implement virtual network functions and serverless functions.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>12/17/2020</MinAmdLetterDate>
<MaxAmdLetterDate>12/17/2020</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2052696</AwardID>
<Investigator>
<FirstName>Arjun</FirstName>
<LastName>Guha</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Arjun Guha</PI_FULL_NAME>
<EmailAddress><![CDATA[a.guha@northeastern.edu]]></EmailAddress>
<NSF_ID>000633150</NSF_ID>
<StartDate>12/17/2020</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name><![CDATA[Northeastern University]]></Name>
<CityName>BOSTON</CityName>
<ZipCode>021155005</ZipCode>
<PhoneNumber>6173735600</PhoneNumber>
<StreetAddress><![CDATA[360 HUNTINGTON AVE]]></StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<StateCode>MA</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MA07</CONGRESS_DISTRICT_ORG>
<ORG_UEI_NUM>HLTMVS2JZBS6</ORG_UEI_NUM>
<ORG_LGL_BUS_NAME>NORTHEASTERN UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_UEI_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[Northeastern University]]></Name>
<CityName>Boston</CityName>
<StateCode>MA</StateCode>
<ZipCode>021155005</ZipCode>
<StreetAddress><![CDATA[360 Huntington Ave]]></StreetAddress>
<CountryCode>UK</CountryCode>
<CountryName>United Kingdom</CountryName>
<StateName>Massachusetts</StateName>
<CountryFlag>0</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MA07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>094Y00</Code>
<Text>FMitF: Formal Methods in the F</Text>
</ProgramElement>
<ProgramReference>
<Code>071Z</Code>
<Text>FMitF-Formal Methods in the Field</Text>
</ProgramReference>
<ProgramReference>
<Code>8206</Code>
<Text>Formal Methods and Verification</Text>
</ProgramReference>
<Appropriation>
<Code>0120</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Fund>
<Code>01002021DB</Code>
<Name><![CDATA[NSF RESEARCH & RELATED ACTIVIT]]></Name>
<FUND_SYMB_ID>040100</FUND_SYMB_ID>
</Fund>
<FUND_OBLG>2020~295000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>&nbsp;</p>  <div>  <div><span>This project had the following outcomes.</span></div>  <br />  <div><span>The first outcome of this project was TacTok, an approach to automate proof-writing using large language models. Interactive theorem provers, such as the Coq Proof Assistant, have been used for several years to build sophisticated verified systems. However, even with automation, it takes significant human effort to write proofs for theorem provers. This project developed TacTok, one of the first methods of automatically completing proofs for Coq that leveraged large language models (LLMs). The work appeared in 2020 and has since spurred several follow-up works on automating proof writing for Coq.</span></div>  <br />  <div><span>The second outcome of this project was rewrite-guided synthesis (ReGiS). Prior work on program synthesis fails to fully exploit the fact that in many domains, a synthesizer can reason both syntactically and semantically. ReGiS is a new form of program synthesis that uses both syntactic (rewrite-rule-based) and semantic (equality-checking-based) domain knowledge to improve synthesizer efficiency.</span></div>  <br />  <div><span>The third outcome of this project was MultiPL-E, the first massively-multilingual programming benchmark for large language models (LLMs). The MultiPL-E benchmark supports 20+ programming languages and was the first benchmark to quantify how LLMs capabilities vary by programming languages. MultiPL-E has become the de facto standard benchmark for assessing LLMs on non-Python programming languages. It is cited and used to evaluate most recent LLMs trained on code, including Meta Llama 3, IBM Granite Code Models, BigCode StarCoder 2,  Mistral Large 2, and others.</span></div>  </div>  <p>&nbsp;</p><br> <p>  Last Modified: 12/11/2024<br> Modified by: Arjun&nbsp;Guha</p></div> <div class="porSideCol" ></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[      This project had the following outcomes.     The first outcome of this project was TacTok, an approach to automate proof-writing using large language models. Interactive theorem provers, such as the Coq Proof Assistant, have been used for several years to build sophisticated verified systems. However, even with automation, it takes significant human effort to write proofs for theorem provers. This project developed TacTok, one of the first methods of automatically completing proofs for Coq that leveraged large language models (LLMs). The work appeared in 2020 and has since spurred several follow-up works on automating proof writing for Coq.     The second outcome of this project was rewrite-guided synthesis (ReGiS). Prior work on program synthesis fails to fully exploit the fact that in many domains, a synthesizer can reason both syntactically and semantically. ReGiS is a new form of program synthesis that uses both syntactic (rewrite-rule-based) and semantic (equality-checking-based) domain knowledge to improve synthesizer efficiency.     The third outcome of this project was MultiPL-E, the first massively-multilingual programming benchmark for large language models (LLMs). The MultiPL-E benchmark supports 20+ programming languages and was the first benchmark to quantify how LLMs capabilities vary by programming languages. MultiPL-E has become the de facto standard benchmark for assessing LLMs on non-Python programming languages. It is cited and used to evaluate most recent LLMs trained on code, including Meta Llama 3, IBM Granite Code Models, BigCode StarCoder 2,  Mistral Large 2, and others.           Last Modified: 12/11/2024       Submitted by: ArjunGuha]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
