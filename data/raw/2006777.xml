<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle><![CDATA[RI: Small: Robustness and Confidence in Machine-Learned Systems]]></AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2020</AwardEffectiveDate>
<AwardExpirationDate>09/30/2023</AwardExpirationDate>
<AwardTotalIntnAmount>450000.00</AwardTotalIntnAmount>
<AwardAmount>450000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Vladimir Pavlovic</SignBlockName>
<PO_EMAI>vpavlovi@nsf.gov</PO_EMAI>
<PO_PHON>7032928318</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The application of machine learning, in fields from medicine to mobile data gathering platforms, has substantial promise. Yet as data comes from a greater variety of sources in an ever-shifting world, how can one trust that machine-learned systems have not simply fit some strange idiosyncrasies they observe? This project develops methods for machine learning so that such systems are not brittle, sensitive to tiny changes in collected data, or likely to make critical mistakes on rare populations. With the growing importance of data analysis in science, industry, and healthcare, principled and practical approaches to robustness, safety, and calibration have immediate and wide-ranging effects.  A major goal of the project is to provide decision makers with trustworthy predictions from machine-learned models.  A second goal is pedagogical: with the meteoric rise of machine learning, there is a missed opportunity to educate students, researchers, and engineers to give them the ability to actually build trustworthy systems; this project aims toward a curriculum around such challenges.&lt;br/&gt;&lt;br/&gt;This project develops robust learning procedures in effort to build trustable machine learning. Three concrete thrusts underpin the work.  The first builds off of the investigator's work in distributional robustness, which fits models to maximize performance on populations near enough to available data.  The second is to use data creatively and correctly; this entails using the data to define robustness, understand method sensitivities, use unlabeled (cheap) data to build more robust representations, and construct data-based regularization. The third targets confidence and calibration, building models that provide assumption-free valid predictions. In this case, the aim is to seek predictors with calibrated confidence, building out of conformal prediction, which modern learning methods emphatically do not provide. More generally, distributional shifts challenge statistical machine learning methods, and the project aims for new validation and testing methodologies to understand such shifts, identify situations where methods are sensitive to changes in underlying data, and to allow valid confidence in predictions even in changing environments.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>09/04/2020</MinAmdLetterDate>
<MaxAmdLetterDate>10/15/2020</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2006777</AwardID>
<Investigator>
<FirstName>John</FirstName>
<LastName>Duchi</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>John Duchi</PI_FULL_NAME>
<EmailAddress><![CDATA[jduchi@stanford.edu]]></EmailAddress>
<NSF_ID>000697614</NSF_ID>
<StartDate>09/04/2020</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name><![CDATA[Stanford University]]></Name>
<CityName>STANFORD</CityName>
<ZipCode>943052004</ZipCode>
<PhoneNumber>6507232300</PhoneNumber>
<StreetAddress><![CDATA[450 JANE STANFORD WAY]]></StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>16</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA16</CONGRESS_DISTRICT_ORG>
<ORG_UEI_NUM>HJD6G4D6TJY5</ORG_UEI_NUM>
<ORG_LGL_BUS_NAME>THE LELAND STANFORD JUNIOR UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_UEI_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[Stanford University]]></Name>
<CityName>Stanford</CityName>
<StateCode>CA</StateCode>
<ZipCode>943054020</ZipCode>
<StreetAddress><![CDATA[390 Jane Stanford Way]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>16</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA16</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>749500</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0120</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Fund>
<Code>01002021DB</Code>
<Name><![CDATA[NSF RESEARCH & RELATED ACTIVIT]]></Name>
<FUND_SYMB_ID>040100</FUND_SYMB_ID>
</Fund>
<FUND_OBLG>2020~450000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The main goals of this proposal was to build and understand trustable verifiable machine learning systems.&nbsp; Three major thrusts constituted the effort: first, distributional robustness, which fits models designed to maximize performance not just on data but on distributions near enough.&nbsp; The second aw to develop methods that creatively and correctly use data, using the available data to identify failure modes of machine-learned procedures. The last was to understand and build predictors that have accurate confidence and make calibrated predictions, meaning that they accurately predict probabilities of target events.</p> <p><br />For the first effort, on distributional robustness, PI and students developed several families of new methods. These included procedures to address so-called ``covariate shift'' robustly: in situations in which we wish to predict some target Y from covariates X (think, for example, of identifying objects in an image), it is natural that the distribution of X changes, and so being robust to these changes is important. Here we developed methodology as well as fundamental limits for solving these problems.</p> <p><br />The second and third efforts dovetailed. For these, we revisited several classical methods of validation for statistical- and machine-learned models. By developing tools to make validation more robust, we could give confidence sets for method performance even under changing distributions. In addition, the project considered a situation common in the real world that only limited feedback about predictions is available. Think, for instance, of a system that predicts weather at very granular levels, including precipitation, temperature, with high temporal resolution. We developed new methods to calibrate such systems using only data with much less resolution---such as average temperature over a day, or rainfall over a week---than previously possible, allowing guarantees of model validity even given cheap and distance feedback.</p> <p>&nbsp;</p><br> <p>  Last Modified: 10/01/2024<br> Modified by: John&nbsp;Duchi</p></div> <div class="porSideCol" ></div> </div> <div class="porColContainerHR"> <div class="porContentCol"> <h2>Addendum # 1</h2><p><p>The main goals of this proposal were to build and understand trustable verifiable machine learning systems.&nbsp; Three major thrusts constituted the effort: first, distributional robustness, which fits models designed to maximize performance not just on data but on distributions near enough.&nbsp; The second was to develop methods that creatively and correctly use data, using the available data to identify failure modes of machine-learned procedures. The last was to understand and build predictors that have accurate confidence and make calibrated predictions, meaning that they accurately predict probabilities of target events.</p> <p><br />For the first effort, on distributional robustness, PI and students developed several families of new methods. These included procedures to address so-called "covariate shift" robustly: in situations in which we wish to predict some target Y from covariates X (think, for example, of identifying objects in an image), it is natural that the distribution of X changes, and so being robust to these changes is important. Here we developed methodology as well as fundamental limits for solving these problems.</p> <p><br />The second and third efforts dovetailed. For these, we revisited several classical methods of validation for statistical- and machine-learned models. By developing tools to make validation more robust, we could give confidence sets for method performance even under changing distributions. In addition, the project considered a situation common in the real world that only limited feedback about predictions is available. Think, for instance, of a system that predicts weather at very granular levels, including precipitation, temperature, with high temporal resolution. We developed new methods to calibrate such systems using only data with much less resolution---such as average temperature over a day, or rainfall over a week---than previously possible, allowing guarantees of model validity even given cheap and distance feedback.</p> <p>&nbsp;</p></p><br> <p>Added: 11/14/2024<br>Submitted by: John&nbsp;Duchi</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[  The main goals of this proposal was to build and understand trustable verifiable machine learning systems. Three major thrusts constituted the effort: first, distributional robustness, which fits models designed to maximize performance not just on data but on distributions near enough. The second aw to develop methods that creatively and correctly use data, using the available data to identify failure modes of machine-learned procedures. The last was to understand and build predictors that have accurate confidence and make calibrated predictions, meaning that they accurately predict probabilities of target events.    For the first effort, on distributional robustness, PI and students developed several families of new methods. These included procedures to address so-called ``covariate shift'' robustly: in situations in which we wish to predict some target Y from covariates X (think, for example, of identifying objects in an image), it is natural that the distribution of X changes, and so being robust to these changes is important. Here we developed methodology as well as fundamental limits for solving these problems.    The second and third efforts dovetailed. For these, we revisited several classical methods of validation for statistical- and machine-learned models. By developing tools to make validation more robust, we could give confidence sets for method performance even under changing distributions. In addition, the project considered a situation common in the real world that only limited feedback about predictions is available. Think, for instance, of a system that predicts weather at very granular levels, including precipitation, temperature, with high temporal resolution. We developed new methods to calibrate such systems using only data with much less resolution---such as average temperature over a day, or rainfall over a week---than previously possible, allowing guarantees of model validity even given cheap and distance feedback.        Last Modified: 10/01/2024       Submitted by: JohnDuchi   The main goals of this proposal were to build and understand trustable verifiable machine learning systems. Three major thrusts constituted the effort: first, distributional robustness, which fits models designed to maximize performance not just on data but on distributions near enough. The second was to develop methods that creatively and correctly use data, using the available data to identify failure modes of machine-learned procedures. The last was to understand and build predictors that have accurate confidence and make calibrated predictions, meaning that they accurately predict probabilities of target events.    For the first effort, on distributional robustness, PI and students developed several families of new methods. These included procedures to address so-called "covariate shift" robustly: in situations in which we wish to predict some target Y from covariates X (think, for example, of identifying objects in an image), it is natural that the distribution of X changes, and so being robust to these changes is important. Here we developed methodology as well as fundamental limits for solving these problems.    The second and third efforts dovetailed. For these, we revisited several classical methods of validation for statistical- and machine-learned models. By developing tools to make validation more robust, we could give confidence sets for method performance even under changing distributions. In addition, the project considered a situation common in the real world that only limited feedback about predictions is available. Think, for instance, of a system that predicts weather at very granular levels, including precipitation, temperature, with high temporal resolution. We developed new methods to calibrate such systems using only data with much less resolution---such as average temperature over a day, or rainfall over a week---than previously possible, allowing guarantees of model validity even given cheap and distance feedback.   Last Modified: 11/14/2024 Submitted by: JohnDuchi]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
