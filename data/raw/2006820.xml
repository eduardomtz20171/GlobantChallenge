<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle><![CDATA[RI: SMALL: Recognizing objects in images and their properties over time]]></AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2020</AwardEffectiveDate>
<AwardExpirationDate>09/30/2023</AwardExpirationDate>
<AwardTotalIntnAmount>419453.00</AwardTotalIntnAmount>
<AwardAmount>419453</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jie Yang</SignBlockName>
<PO_EMAI>jyang@nsf.gov</PO_EMAI>
<PO_PHON>7032924768</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Computer vision lives in the golden age of datasets. All aspects of human vision are systematically mapped and transcribed into an ever-larger pool of labeled data. All with one single goal: teach a vision system, nowadays a deep network, to imitate all aspects of human perception. The current recipe is simple: collect sufficient labeled data, then use supervised machine learning to mimic the supervision. There is just one issue with this approach: Systems trained this way are limited to imitate a single narrow task. In this project, we take a step towards unifying many vision tasks into one single system: A framework that infers all properties of all things through time. If successful, this system can identify objects and all their properties in any new unseen image, and bring the full power of computer vision to the non-expert. Applications include autonomous agents interacting with the world through the manipulation of objects, and assistive technologies for the elderly that observes the world through moving objects and their properties.&lt;br/&gt;&lt;br/&gt;The project will pursue three research thrusts. 1. Detecting all objects: In object detection, datasets specialize in domains. Driving datasets describe any vehicle type imaginable, indoor datasets focus on common household objects, and pedestrian datasets exclusively focus on humans. How can we train an object detection system that leverages all these sources of data? How can we relate these different data sources to each other? How do we deal with partial annotation in some data sources? 2. Inferring all properties: Object detection forms the basic building block for many aspects of visual reasoning. However, the most interesting tasks start after detection: What is the 2D or 3D pose of an object? Is this object deformable? Could it be a danger to an autonomous vehicle? Again, there are hundreds of tasks and data sources that describe all the properties of objects. How can we learn a detector that infers them all? 3. Recognition through time: Finally, detection should not be isolated in time. How do we reason about objects and properties through time? Can we learn to recognize objects in a temporally coherent manner using current image-based datasets?&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>09/10/2020</MinAmdLetterDate>
<MaxAmdLetterDate>10/15/2020</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2006820</AwardID>
<Investigator>
<FirstName>Philipp</FirstName>
<LastName>Kraehenbuehl</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Philipp Kraehenbuehl</PI_FULL_NAME>
<EmailAddress><![CDATA[philkr@utexas.edu]]></EmailAddress>
<NSF_ID>000783931</NSF_ID>
<StartDate>09/10/2020</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name><![CDATA[University of Texas at Austin]]></Name>
<CityName>AUSTIN</CityName>
<ZipCode>787121139</ZipCode>
<PhoneNumber>5124716424</PhoneNumber>
<StreetAddress><![CDATA[110 INNER CAMPUS DR]]></StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<StateCode>TX</StateCode>
<CONGRESSDISTRICT>25</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>TX25</CONGRESS_DISTRICT_ORG>
<ORG_UEI_NUM>V6AFQPN18437</ORG_UEI_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF TEXAS AT AUSTIN</ORG_LGL_BUS_NAME>
<ORG_PRNT_UEI_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Texas at Austin]]></Name>
<CityName>Austin</CityName>
<StateCode>TX</StateCode>
<ZipCode>787595316</ZipCode>
<StreetAddress><![CDATA[3925 W Braker Lane, Suite 3340]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>37</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>TX37</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>749500</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0120</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Fund>
<Code>01002021DB</Code>
<Name><![CDATA[NSF RESEARCH & RELATED ACTIVIT]]></Name>
<FUND_SYMB_ID>040100</FUND_SYMB_ID>
</Fund>
<FUND_OBLG>2020~419453</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This project developed methods and models to recognize general objects in an image through space and time. If you can name an object our system will likely find it in an image, and track it throughout a video. See https://github.com/facebookresearch/Detic for some demos.<br />The research in this project produced nine highly influential papers (cited 2000+ times in total over a span of 3 years). It opened up a new research field at the intersection of large-language models and open-vocabulary detection with significant follow-up research.<br />All the code and models produced by this project are freely available online under a permissive open-source license. They have been used by industry and academic labs alike.<br />Models developed under this project have been applied to autonomous navigation research, video recognition, generative AI tasks, and even protein engineering.<br />This project funded seven PhD students, three of whom have graduated as of this writing. All three are continuing their industry research. Indirectly, transfer the knowledge gained by this project into industrial applications.<br />Finally, the research problems studied in this project build the foundation of assignments in both the undergraduate deep learning class and an online Masters Deep Learning class at UT Austin. The undergraduate class reaches approximately 200 students per year and the online master class reachers over 500 students per semester.</p><br> <p>  Last Modified: 01/29/2024<br> Modified by: Philipp&nbsp;Kraehenbuehl</p></div> <div class="porSideCol" ><div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)          </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2024/2006820/2006820_10706073_1706536133722_detic_2--rgov-214x142.jpeg" original="/por/images/Reports/POR/2024/2006820/2006820_10706073_1706536133722_detic_2--rgov-800width.jpeg" title="Open-Vocabulary Detection Results"><img src="/por/images/Reports/POR/2024/2006820/2006820_10706073_1706536133722_detic_2--rgov-66x44.jpeg" alt="Open-Vocabulary Detection Results"></a> <div class="imageCaptionContainer"> <div class="imageCaption">A sample scene with detections of completely novel concepts.</div> <div class="imageCredit">Xingyi Zhou, Ishan Misra</div> <div class="imagePermisssions">Creative Commons</div> <div class="imageSubmitted">Philipp&nbsp;Kraehenbuehl <div class="imageTitle">Open-Vocabulary Detection Results</div> </div> </li><li> <a href="/por/images/Reports/POR/2024/2006820/2006820_10706073_1706536063588_detic_1--rgov-214x142.jpeg" original="/por/images/Reports/POR/2024/2006820/2006820_10706073_1706536063588_detic_1--rgov-800width.jpeg" title="Open-Vocabulary Detection Results"><img src="/por/images/Reports/POR/2024/2006820/2006820_10706073_1706536063588_detic_1--rgov-66x44.jpeg" alt="Open-Vocabulary Detection Results"></a> <div class="imageCaptionContainer"> <div class="imageCaption">An example output of our system for open-vocabulary detection. Purple as concepts the model learned about during training, green a novel concepts specified by free form language inputs</div> <div class="imageCredit">Xingyi Zhou, Ishan Misra</div> <div class="imagePermisssions">Creative Commons</div> <div class="imageSubmitted">Philipp&nbsp;Kraehenbuehl <div class="imageTitle">Open-Vocabulary Detection Results</div> </div> </li></ul> </div> </div></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[  This project developed methods and models to recognize general objects in an image through space and time. If you can name an object our system will likely find it in an image, and track it throughout a video. See https://github.com/facebookresearch/Detic for some demos. The research in this project produced nine highly influential papers (cited 2000+ times in total over a span of 3 years). It opened up a new research field at the intersection of large-language models and open-vocabulary detection with significant follow-up research. All the code and models produced by this project are freely available online under a permissive open-source license. They have been used by industry and academic labs alike. Models developed under this project have been applied to autonomous navigation research, video recognition, generative AI tasks, and even protein engineering. This project funded seven PhD students, three of whom have graduated as of this writing. All three are continuing their industry research. Indirectly, transfer the knowledge gained by this project into industrial applications. Finally, the research problems studied in this project build the foundation of assignments in both the undergraduate deep learning class and an online Masters Deep Learning class at UT Austin. The undergraduate class reaches approximately 200 students per year and the online master class reachers over 500 students per semester.     Last Modified: 01/29/2024       Submitted by: PhilippKraehenbuehl]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
