<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle><![CDATA[CNS Core: Small: Re-engineering Applications for Tensor Processing Units]]></AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/15/2020</AwardEffectiveDate>
<AwardExpirationDate>06/30/2024</AwardExpirationDate>
<AwardTotalIntnAmount>495000.00</AwardTotalIntnAmount>
<AwardAmount>495000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Marilyn McClure</SignBlockName>
<PO_EMAI>mmcclure@nsf.gov</PO_EMAI>
<PO_PHON>7032925197</PO_PHON>
</ProgramOfficer>
<AbstractNarration>To overcome the inefficiency of conventional processors for machine learning (ML) and artificial intelligence (AI) computing applications, hardware accelerators that provide operators to compute on tensors/matrices directly emerge in all types of computer systems. Any application that naturally consumes and produces tensors/matrices may take advantage of these accelerators. However, these accelerators might only be used for ML/AI applications due to the lack of an appropriate programming interface and system support. This project will bridge the gap of making these accelerators available for more applications. This project will redesign applications to use these accelerators to improve performance and energy.&lt;br/&gt;&lt;br/&gt;This project will (1) build real systems using commercialized ML/AI accelerators (e.g., Google's Tensor Processing Units (TPUs)), (2) develop a programming interface that allows programmers to create any type of application on the built system, (3) redesign the algorithms of applications to use efficiently the operators that TPUs offer, (4) compose library functions and runtime systems to process efficiently tasks but hide complexity from programmers, and (5) revisit the design of the storage subsystem to supply data more efficiently. Through these aforementioned tasks, this project will demonstrate the challenges and analyze the potentials of extending the applications of TPUs. &lt;br/&gt;&lt;br/&gt;This project, if successful, will provide the first general-purpose programming platform for TPUs. By making the outcomes available to the public, this project will be available to all research areas that depend on computation/analytics of tensor/matrix datasets. In fact, the target applications in this project already cover database systems, bioinformatics algorithms and physics simulations.  This project will also offer research/learning opportunities for a general audience, interdisciplinary researchers, and minority groups by classroom teaching, publications, talks and undergraduate summer interns.&lt;br/&gt;&lt;br/&gt;The research outcomes will be made through peer-reviewed conference and journal papers. The code developed and the configurations of hardware platforms will be publicly available through third party repository services (https://github.com/escalab/) after the research outcome is published. A cloud data storage service will be used to store all raw experimental data, copies of source code and applications datasets for at least three years and make them available upon appropriate requests.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>07/10/2020</MinAmdLetterDate>
<MaxAmdLetterDate>07/10/2020</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2007124</AwardID>
<Investigator>
<FirstName>Hung-Wei</FirstName>
<LastName>Tseng</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Hung-Wei Tseng</PI_FULL_NAME>
<EmailAddress><![CDATA[htseng@ucr.edu]]></EmailAddress>
<NSF_ID>000728421</NSF_ID>
<StartDate>07/10/2020</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name><![CDATA[University of California-Riverside]]></Name>
<CityName>RIVERSIDE</CityName>
<ZipCode>925210001</ZipCode>
<PhoneNumber>9518275535</PhoneNumber>
<StreetAddress><![CDATA[200 UNIVERSTY OFC BUILDING]]></StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>39</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA39</CONGRESS_DISTRICT_ORG>
<ORG_UEI_NUM>MR5QC5FCAVH5</ORG_UEI_NUM>
<ORG_LGL_BUS_NAME>REGENTS OF THE UNIVERSITY OF CALIFORNIA AT RIVERSIDE</ORG_LGL_BUS_NAME>
<ORG_PRNT_UEI_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of California-Riverside]]></Name>
<CityName/>
<StateCode>CA</StateCode>
<ZipCode>925210001</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>39</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA39</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>735400</Code>
<Text>CSR-Computer Systems Research</Text>
</ProgramElement>
<ProgramReference>
<Code>7354</Code>
<Text>COMPUTER SYSTEMS</Text>
</ProgramReference>
<Appropriation>
<Code>0120</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Fund>
<Code>01002021DB</Code>
<Name><![CDATA[NSF RESEARCH & RELATED ACTIVIT]]></Name>
<FUND_SYMB_ID>040100</FUND_SYMB_ID>
</Fund>
<FUND_OBLG>2020~495000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><br />As Dennard scaling is discontinued, computer architects seek performance scaling with improved process technologies through domain/application-specific accelerators. The most significant domain-specific accelerator that becomes ubiquitous during the project period is indeed the one this project targets: tensor accelerators, or more specifically, AI/ML accelerators. However, before this project, these accelerators were only available for AI/ML accelerators despite their mathematical functions serving as the cores of many applications beyond the domains of AI and ML.&nbsp;</p> <p><br />This work democratizes AI/ML accelerators, focusing on commercialized tensor processing units (TPUs) and tensor cores. It produces several critical software/hardware systems and programming interfaces that promote a broader spectrum of applications beyond AI/ML using both accelerators.&nbsp;</p> <p><br />The first is the GPTPU system, which provides an OpenCL-like programming interface to facilitate the authoring of applications outside the AI/ML domain. Through library functions accelerating the core matrix multiplications/convolutions in various applications covering various domains, this project demonstrates 2.46x speedup over more power-consuming CPU cores. More importantly, we show a 40% reduction in energy consumption and also reduced carbon footprints.&nbsp;</p> <p><br />The second system is a database system, TCUDB, where we transformed the entrenched hash-join algorithms that optimize for CPU architecture to leverage the matrix multiplication hardware that Tensor Cores feature. The resulting system shows 288x speedup compared with the state-of-the-art database engines in the critical entity matching problems for data mining. As the developed engine supports standard SQL query languages, the system also accepts and accelerates any database request that requires join operations.</p> <p><br />The third system is a hardware/software co-designed architecture, SIMD^2, where we proposed using matrix-semiring as a programming paradigm for matrix-based hardware accelerators. With matrix semi-ring as the theoretical background, we generalized the semantics of matrix multiplications to solve shortest path problems, spanning tree problems, and nearest neighbors problems. Through the implicit, massive parallelism in matrix-semiring theory, SIMD^2 can parallelize the path and tree problems without efficient parallel implementations on modern hardware. The extension of hardware to support operations beyond matrix multiplications accounts for 5% of the area overhead due to the large reuse of processing elements. The resulting system achieves up to 39x speedup compared to state-of-the-art GPU implementations of solvers for the same problem.</p> <p><br />In addition to the three large-scale systems, the project also lead to the development of TPUPoint, a profiling tool of Cloud TPUs. This project uses the insights that TPUPoint generates to guide and motivate the proposed approaches/systems. Leveraging a similar idea as TCUDB, this project also revisited a set of computer vision algorithms and tensorized those critical preprocessing stages in ML pipelines to maximize the use of tensor processing units instead of letting them idle and waste energy. We also released a benchmark suite, Accel-Bench, that contains our exploration of tensorizing applications and expects to inspire future research along the same line of research.&nbsp;</p> <p><br />The project also leads to developing an innovative data storage system, NDS. The system breaks the conventional rule of linear address spaces in storage system abstractions but promotes the concept of multi-dimensional building blocks to more efficiently support the multi-dimensional processing models for modern AI/ML accelerators. The result also shows a 5.7x speedup without upgrading GPU/accelerator hardware components.&nbsp;</p> <p><br />This work resulted in the release of 7 open-source projects on GitHub, ten papers, an award paper (top picks in computer architecture, recognizing the top research publications from the prior year), several invited research, and distinguished lecture talks at universities and industry locations. The result of the work also becomes part of the materials in the PI's operating systems and computer architecture classes. Among the students who have worked on the project and the PI mentored, four have joined the industry, one in the National Laboratory.</p><br> <p>  Last Modified: 09/11/2024<br> Modified by: Hung-Wei&nbsp;Tseng</p></div> <div class="porSideCol" ></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[   As Dennard scaling is discontinued, computer architects seek performance scaling with improved process technologies through domain/application-specific accelerators. The most significant domain-specific accelerator that becomes ubiquitous during the project period is indeed the one this project targets: tensor accelerators, or more specifically, AI/ML accelerators. However, before this project, these accelerators were only available for AI/ML accelerators despite their mathematical functions serving as the cores of many applications beyond the domains of AI and ML.    This work democratizes AI/ML accelerators, focusing on commercialized tensor processing units (TPUs) and tensor cores. It produces several critical software/hardware systems and programming interfaces that promote a broader spectrum of applications beyond AI/ML using both accelerators.    The first is the GPTPU system, which provides an OpenCL-like programming interface to facilitate the authoring of applications outside the AI/ML domain. Through library functions accelerating the core matrix multiplications/convolutions in various applications covering various domains, this project demonstrates 2.46x speedup over more power-consuming CPU cores. More importantly, we show a 40% reduction in energy consumption and also reduced carbon footprints.    The second system is a database system, TCUDB, where we transformed the entrenched hash-join algorithms that optimize for CPU architecture to leverage the matrix multiplication hardware that Tensor Cores feature. The resulting system shows 288x speedup compared with the state-of-the-art database engines in the critical entity matching problems for data mining. As the developed engine supports standard SQL query languages, the system also accepts and accelerates any database request that requires join operations.    The third system is a hardware/software co-designed architecture, SIMD^2, where we proposed using matrix-semiring as a programming paradigm for matrix-based hardware accelerators. With matrix semi-ring as the theoretical background, we generalized the semantics of matrix multiplications to solve shortest path problems, spanning tree problems, and nearest neighbors problems. Through the implicit, massive parallelism in matrix-semiring theory, SIMD^2 can parallelize the path and tree problems without efficient parallel implementations on modern hardware. The extension of hardware to support operations beyond matrix multiplications accounts for 5% of the area overhead due to the large reuse of processing elements. The resulting system achieves up to 39x speedup compared to state-of-the-art GPU implementations of solvers for the same problem.    In addition to the three large-scale systems, the project also lead to the development of TPUPoint, a profiling tool of Cloud TPUs. This project uses the insights that TPUPoint generates to guide and motivate the proposed approaches/systems. Leveraging a similar idea as TCUDB, this project also revisited a set of computer vision algorithms and tensorized those critical preprocessing stages in ML pipelines to maximize the use of tensor processing units instead of letting them idle and waste energy. We also released a benchmark suite, Accel-Bench, that contains our exploration of tensorizing applications and expects to inspire future research along the same line of research.    The project also leads to developing an innovative data storage system, NDS. The system breaks the conventional rule of linear address spaces in storage system abstractions but promotes the concept of multi-dimensional building blocks to more efficiently support the multi-dimensional processing models for modern AI/ML accelerators. The result also shows a 5.7x speedup without upgrading GPU/accelerator hardware components.    This work resulted in the release of 7 open-source projects on GitHub, ten papers, an award paper (top picks in computer architecture, recognizing the top research publications from the prior year), several invited research, and distinguished lecture talks at universities and industry locations. The result of the work also becomes part of the materials in the PI's operating systems and computer architecture classes. Among the students who have worked on the project and the PI mentored, four have joined the industry, one in the National Laboratory.     Last Modified: 09/11/2024       Submitted by: Hung-WeiTseng]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
