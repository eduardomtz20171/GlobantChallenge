<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle><![CDATA[Collaborative Research: RI: Small: Modeling and Learning Ethical Principles for Embedding into Group Decision Support Systems]]></AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>01/01/2021</AwardEffectiveDate>
<AwardExpirationDate>12/31/2023</AwardExpirationDate>
<AwardTotalIntnAmount>166411.00</AwardTotalIntnAmount>
<AwardAmount>166411</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Erion Plaku</SignBlockName>
<PO_EMAI>eplaku@nsf.gov</PO_EMAI>
<PO_PHON>7032924426</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Many settings in everyday life require making decisions by combining the subjective preferences of individuals in a group, such as where to go to eat, where to go on vacation, whom to hire, which ideas to fund, or what route to take. In many domains, these subjective preferences are combined with moral values, ethical principles, or business constraints that are applicable to the decision scenario and are often prioritized over the preferences.  The potential conflict of moral values with subjective preferences are keenly felt both when AI systems recommend products to us and when we use AI enabled systems to make group decisions.  This research seeks to make AI more accountable by providing mechanisms to bound the decisions that AI systems can make, ensuring that the outcomes of the group decision making process aligns with human values.  To achieve the goal of building ethically-bounded, AI-enabled group decision making systems, this project takes inspiration from humans, who often constrain their decisions and actions according to a number of exogenous priorities coming from moral, ethical, or business values.  This research project will address the current lack of principled, formal approaches for embedding ethics into AI agents and AI enabled group decision support systems by advancing the state of the art in the safety and robustness of AI agents which, given how broadly AI touches our daily lives, will have broad impact and benefit to society.&lt;br/&gt;&lt;br/&gt;Specifically, the long-term goal of this project is to establish mathematical and machine learning foundations for embedding ethical guidelines into AI for group decision-making systems.  Within the machine ethics field there are two main approaches: the bottom-up approach focused on data-driven machine learning techniques and the top-down approach following symbolic and logic-based formalisms. This project brings these two methodologies closer together through three specific aims. (1) Modeling and Evaluating Ethical Principles: this project will extend principles in social choice theory and fair division using preference models from the literature on knowledge representation and preference reasoning. (2) Learning Ethical Principles From Data: this project will develop novel machine-learning frameworks to learn individual ethical principles and then aggregate them for use in group decision making systems. And finally, (3) Embedding Ethical Principles into Group Decision Support Systems: this project will develop novel frameworks for designing AI-based mechanisms for ethical group decision-making.  This research will establish novel methods for the formal and experimental unification of aspects of the top-down or rule-based approach with the bottoms-up or data-based approach for embedding ethics into group decision making systems.  The project will also formalize a framework for ethical and constrained reasoning across teams of computational agents.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>09/09/2020</MinAmdLetterDate>
<MaxAmdLetterDate>10/19/2020</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2008011</AwardID>
<Investigator>
<FirstName>Kristen</FirstName>
<LastName>Venable</LastName>
<PI_MID_INIT>B</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Kristen B Venable</PI_FULL_NAME>
<EmailAddress><![CDATA[bvenable@uwf.edu]]></EmailAddress>
<NSF_ID>000644129</NSF_ID>
<StartDate>09/09/2020</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name><![CDATA[Florida Institute for Human and Machine Cognition, Inc.]]></Name>
<CityName>PENSACOLA</CityName>
<ZipCode>325026008</ZipCode>
<PhoneNumber>8502024473</PhoneNumber>
<StreetAddress><![CDATA[40 S ALCANIZ ST]]></StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Florida</StateName>
<StateCode>FL</StateCode>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>FL01</CONGRESS_DISTRICT_ORG>
<ORG_UEI_NUM>EMXDXET56K76</ORG_UEI_NUM>
<ORG_LGL_BUS_NAME>FLORIDA INSTITUTE FOR HUMAN &amp; MACHINE COGNITION INC</ORG_LGL_BUS_NAME>
<ORG_PRNT_UEI_NUM>EMXDXET56K76</ORG_PRNT_UEI_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Florida Institute for Human and Machine Cognition, Inc.]]></Name>
<CityName/>
<StateCode>FL</StateCode>
<ZipCode>325026008</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Florida</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>FL01</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>749500</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0120</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Fund>
<Code>01002021DB</Code>
<Name><![CDATA[NSF RESEARCH & RELATED ACTIVIT]]></Name>
<FUND_SYMB_ID>040100</FUND_SYMB_ID>
</Fund>
<FUND_OBLG>2020~166411</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Project outcomes for PI Venable contribution to &nbsp;the project.</p> <p>During this research project, our research focused on advancing the understanding of ethics in group decision-making systems. Our approach combined insights from cognitive theories and practical applications in voting, multi-agent systems, and AI ethics. We aligned our research with the three primary aims of our project: Modeling and Evaluating Ethical Principles, Learning Ethical Principles from Data and Embedding Ethical Principles into Group Decision Support Systems. In terms of the first aim, we made substantial progress in understanding ethical decision-making in uncertain contexts, particularly within voting systems. Our publication at the AAAI conference detailed a user study conducted on Amazon&rsquo;s Mechanical Turk to investigate how real-world voters approach uncertain scenarios. Specifically, we explored multi-winner approval voting, where participants could approve multiple candidates on their ballots. We found that, while individuals generally manipulated their votes to achieve better outcomes, they often did not identify the optimal manipulation strategy. Our study also highlighted that existing models from social choice and psychology literature did not adequately explain real-world voting behavior. To address this gap, we developed a new model that considers the size of the winning set and human cognitive constraints. This model proved to be more effective in capturing voter behavior in multi-winner approval voting contexts, laying a foundation for future research in this area.</p> <p>In terms of aim 2 (Learning Ethical Principles from Data) we explored learning constraints in the context of Markov Decision Processes (MDPs) from human demonstrations. MDPs are a fundamental framework for sequential decision-making, and our work focused on how humans navigate complex trade-offs and ethical considerations. We proposed a novel inverse reinforcement learning (IRL) method to learn implicit hard and soft constraints from human demonstrations, enabling AI agents to adapt to new settings. This approach could have significant implications for AI-human collaboration in various domains.</p> <p>For aim 3 (Embedding Ethical Principles into Group Decision Support Systems) we examined, among other things, &nbsp;the stable marriage problem (SMP), a well-known mathematical abstraction used in two-sided matching markets. Our work on Behavioral Stable Marriage Problems (BSMPs) introduced cognitive models of decision-making to understand how human biases impact classical results in SMP. This approach has practical applications in matching resident doctors to hospitals and students to schools.</p> <p>Our research also extended into other areas, exploring how AI agents can mimic human behavior in ethically constrained environments. We presented a computational approach leveraging multi-alternative decision field theory (MDFT) to create AI agents that can make human-like moral decisions. This methodology proved effective in both producing better decisions and accurately modeling human decision-making.</p> <p>&nbsp;</p> <p>Our research efforts during this period resulted in 7 archival conference publications and 4 refereed workshop publications at major international conferences. Additionally, we saw significant academic success within our team, with two PhD students completing their degrees during this project period.</p> <p>Beyond our primary research aims, we organized and participated in key workshops and conferences, fostering collaborations with experts in various fields. For example , The Workshop on Ethics and Trust in Human-AI Collaboration (ETHAICS 2023) attracted scholars from human-AI collaborative environments, ethics, trust, and cognitive theories. The 12th Knowledge Capture Conference (K-CAP 2023) brought together researchers to discuss scalable and precise knowledge capture methods, emphasizing ethical principles in AI systems. These events furthered our goal of creating AI agents capable of collaborating with humans while respecting ethical considerations.</p> <p><strong>&nbsp;</strong></p> <p>Overall, our work on this project has significantly advanced the understanding of ethics in group decision-making and set the stage for future collaborations and research directions. Our focus on cognitive models, ethical principles, and AI-human collaboration has provided a solid foundation for ongoing exploration in these critical areas.</p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>  Last Modified: 05/07/2024<br> Modified by: Kristen&nbsp;B&nbsp;Venable</p></div> <div class="porSideCol" ></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[  Project outcomes for PI Venable contribution to the project.   During this research project, our research focused on advancing the understanding of ethics in group decision-making systems. Our approach combined insights from cognitive theories and practical applications in voting, multi-agent systems, and AI ethics. We aligned our research with the three primary aims of our project: Modeling and Evaluating Ethical Principles, Learning Ethical Principles from Data and Embedding Ethical Principles into Group Decision Support Systems. In terms of the first aim, we made substantial progress in understanding ethical decision-making in uncertain contexts, particularly within voting systems. Our publication at the AAAI conference detailed a user study conducted on Amazons Mechanical Turk to investigate how real-world voters approach uncertain scenarios. Specifically, we explored multi-winner approval voting, where participants could approve multiple candidates on their ballots. We found that, while individuals generally manipulated their votes to achieve better outcomes, they often did not identify the optimal manipulation strategy. Our study also highlighted that existing models from social choice and psychology literature did not adequately explain real-world voting behavior. To address this gap, we developed a new model that considers the size of the winning set and human cognitive constraints. This model proved to be more effective in capturing voter behavior in multi-winner approval voting contexts, laying a foundation for future research in this area.   In terms of aim 2 (Learning Ethical Principles from Data) we explored learning constraints in the context of Markov Decision Processes (MDPs) from human demonstrations. MDPs are a fundamental framework for sequential decision-making, and our work focused on how humans navigate complex trade-offs and ethical considerations. We proposed a novel inverse reinforcement learning (IRL) method to learn implicit hard and soft constraints from human demonstrations, enabling AI agents to adapt to new settings. This approach could have significant implications for AI-human collaboration in various domains.   For aim 3 (Embedding Ethical Principles into Group Decision Support Systems) we examined, among other things, the stable marriage problem (SMP), a well-known mathematical abstraction used in two-sided matching markets. Our work on Behavioral Stable Marriage Problems (BSMPs) introduced cognitive models of decision-making to understand how human biases impact classical results in SMP. This approach has practical applications in matching resident doctors to hospitals and students to schools.   Our research also extended into other areas, exploring how AI agents can mimic human behavior in ethically constrained environments. We presented a computational approach leveraging multi-alternative decision field theory (MDFT) to create AI agents that can make human-like moral decisions. This methodology proved effective in both producing better decisions and accurately modeling human decision-making.      Our research efforts during this period resulted in 7 archival conference publications and 4 refereed workshop publications at major international conferences. Additionally, we saw significant academic success within our team, with two PhD students completing their degrees during this project period.   Beyond our primary research aims, we organized and participated in key workshops and conferences, fostering collaborations with experts in various fields. For example , The Workshop on Ethics and Trust in Human-AI Collaboration (ETHAICS 2023) attracted scholars from human-AI collaborative environments, ethics, trust, and cognitive theories. The 12th Knowledge Capture Conference (K-CAP 2023) brought together researchers to discuss scalable and precise knowledge capture methods, emphasizing ethical principles in AI systems. These events furthered our goal of creating AI agents capable of collaborating with humans while respecting ethical considerations.      Overall, our work on this project has significantly advanced the understanding of ethics in group decision-making and set the stage for future collaborations and research directions. Our focus on cognitive models, ethical principles, and AI-human collaboration has provided a solid foundation for ongoing exploration in these critical areas.           Last Modified: 05/07/2024       Submitted by: KristenBVenable]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
