<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle><![CDATA[III: Small: Collaborative Research: Learning Active Physics-Based Models from Data]]></AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2020</AwardEffectiveDate>
<AwardExpirationDate>06/30/2023</AwardExpirationDate>
<AwardTotalIntnAmount>250000.00</AwardTotalIntnAmount>
<AwardAmount>250000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Sylvia Spengler</SignBlockName>
<PO_EMAI>sspengle@nsf.gov</PO_EMAI>
<PO_PHON>7032927347</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project explores a novel algorithmic framework for automatic generation of digital models of objects from our natural world, that faithfully reproduce the structure and function of their physical counterparts. We specifically focus on modeling active deformable objects, i.e., objects capable of producing internal forces within their own bodies, such as biological muscles or robotic actuators. Our approach differs from the traditional modeling pipeline by learning the digital models from example data of the mechanism in-action, rather than by manually engineering them from the first principles. We adapt current state-of-the-art deep learning techniques to our problem, in particular artificial neural networks, by endowing them with knowledge about the physics-based behavior of deformable materials. This is expected to significantly upgrade the capabilities of generic neural networks, which would be otherwise forced to learn the laws of physics from data, which is an unnecessary task because fundamental properties of deformable media, such as conservation of energy and rotational invariance, should simply be taken for granted. The proposed algorithmic framework will greatly simplify the creation of digital replicas of objects in our natural world, while enhancing their fidelity. This will empower Virtual and Augmented Reality deployments to deliver life-like experiences in educational and skill-training applications, such as virtual operating rooms or emergency response scenarios. Computer-hosted doubles of functional objects are also a valuable prototyping tool in the design and optimization of physical functional replicas, such as prosthetic devices.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;To achieve these goals, we hybridize a neural network with a differentiable simulator, which outputs the quasistatic (i.e. equilibrated) shape of an active elastic model as a function of input control parameters, and subject to prescribed (known) boundary conditions. The finite element-based simulator is based on Projective Dynamics and designed with differentiability in mind, which is a key feature that will enable smooth combination with the classical backpropagation algorithm and integration within existing deep learning frameworks, such as PyTorch. The input to the simulator allows the actuation controls to be prescribed at very fine granularity, potentially enabling each finite element to become its own independently controllable actuator. These fine-grained actuation controls will be generated by a convolutional neural network, which creates them using a low-dimensional time-varying control vector and constant (i.e., time-invariant) network weights. We train this aggregate pipeline, jointly inferring both the weights of the control network as well as the values of the latent variables associated with different input configurations, as to best explain the training set as the action of a low-dimensional control space. This core framework will subsequently be extended to 1) allow for processing of contact and collisions, 2) optimization of spatially-varying material parameters, 3) lifting the quasi-statics assumption and simulating time-varying dynamics.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>06/01/2020</MinAmdLetterDate>
<MaxAmdLetterDate>06/01/2020</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2008584</AwardID>
<Investigator>
<FirstName>Eftychios</FirstName>
<LastName>Sifakis</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Eftychios Sifakis</PI_FULL_NAME>
<EmailAddress><![CDATA[sifakis@wisc.edu]]></EmailAddress>
<NSF_ID>000581486</NSF_ID>
<StartDate>06/01/2020</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name><![CDATA[University of Wisconsin-Madison]]></Name>
<CityName>MADISON</CityName>
<ZipCode>537151218</ZipCode>
<PhoneNumber>6082623822</PhoneNumber>
<StreetAddress><![CDATA[21 N PARK ST STE 6301]]></StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Wisconsin</StateName>
<StateCode>WI</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>WI02</CONGRESS_DISTRICT_ORG>
<ORG_UEI_NUM>LCLSJAGTNZQ7</ORG_UEI_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF WISCONSIN SYSTEM</ORG_LGL_BUS_NAME>
<ORG_PRNT_UEI_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Wisconsin-Madison]]></Name>
<CityName>Madison</CityName>
<StateCode>WI</StateCode>
<ZipCode>537061204</ZipCode>
<StreetAddress><![CDATA[1210 West Dayton Street]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Wisconsin</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>WI02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>736400</Code>
<Text>Info Integration &amp; Informatics</Text>
</ProgramElement>
<ProgramReference>
<Code>7364</Code>
<Text>INFO INTEGRATION &amp; INFORMATICS</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0120</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Fund>
<Code>01002021DB</Code>
<Name><![CDATA[NSF RESEARCH & RELATED ACTIVIT]]></Name>
<FUND_SYMB_ID>040100</FUND_SYMB_ID>
</Fund>
<FUND_OBLG>2020~250000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span id="docs-internal-guid-defbdf72-7fff-237c-2a51-fe1974a1f63b"> </span></p> <p dir="ltr"><span>The goal of this research project was to investigate novel ways to hybridize Machine Learning and Physics Based Simulation for the purpose of facilitating next-generation content creation tools for Computer-Generated face animation of digital actors.&nbsp;</span></p> <p>&nbsp;</p> <p dir="ltr"><span>A core idea that reflected in several of the products of this work was that Machine Learning techniques can fundamentally aid in the creation of simulation-ready models of 3D facial anatomy, without the painstaking and intricate process that digital artists would prepare such physics-inspired model for computer simulation: Traditionally, all anatomical parts of a human face, including detail shape of flesh, contact areas with bones, geometric placement of muscles, and directional layout of active fibers in active muscles would all need to be crafted from first principles, and adapted to individual identities of digital actors. This modeling effort would take weeks or months of expert tuning, and would not be trivially transferable to actors other than the one specific identity such model was prototyped for. The premise of our work was that this effort could be circumvented, if one could leverage a large dataset of high-resolution, 3D performance captured demonstrations of how a human actor articulates expressions and speech, and use machine learning to automatically infer the mechanism by which the human face navigates through all such expressions and poses. We have demonstrated this both in the context of an identity-specific model whose mechanism is learned from data, and multi-identity paradigms, where both the expression and the identity of the digital actor are tunable inputs to the neural network. In either case, the output of our system is a fully simulation-compatible 3D model, which can be integrated with additional physical effects, such as contact, collision, gravity, or external forces from the surrounding environment.</span></p> <p>&nbsp;</p> <p dir="ltr"><span>Ancillary to this core thrust, our research provided additional demonstrations of hybridization of machine learning with facial animation and/or simulation pipelines, including a framework for super-resolution by which a moderate-quality, real-time face simulator can be boosted to the quality level of expensive, intricate offline simulators, and a neural-network based system for automatically adjusting the age of facial performances in video, under varied and challenging motion and lighting conditions.</span></p> <div><span><br /></span></div> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>  Last Modified: 01/30/2024<br> Modified by: Eftychios&nbsp;Sifakis</p></div> <div class="porSideCol" ><div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)          </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2024/2008584/2008584_10674101_1706646956056_2021_Technical_Papers_Grama_Learning_active_quasistatic_physics_based_models_from_data--rgov-214x142.jpg" original="/por/images/Reports/POR/2024/2008584/2008584_10674101_1706646956056_2021_Technical_Papers_Grama_Learning_active_quasistatic_physics_based_models_from_data--rgov-800width.jpg" title="Physics-based model from data"><img src="/por/images/Reports/POR/2024/2008584/2008584_10674101_1706646956056_2021_Technical_Papers_Grama_Learning_active_quasistatic_physics_based_models_from_data--rgov-66x44.jpg" alt="Physics-based model from data"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Illustration of physics-based model whose actuation mechanism is learned directly from surface performance data.</div> <div class="imageCredit">ACM</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Eftychios&nbsp;Sifakis <div class="imageTitle">Physics-based model from data</div> </div> </li><li> <a href="/por/images/Reports/POR/2024/2008584/2008584_10674101_1706647087102_2305--rgov-214x142.jpg" original="/por/images/Reports/POR/2024/2008584/2008584_10674101_1706647087102_2305--rgov-800width.jpg" title="Super-resolution"><img src="/por/images/Reports/POR/2024/2008584/2008584_10674101_1706647087102_2305--rgov-66x44.jpg" alt="Super-resolution"></a> <div class="imageCaptionContainer"> <div class="imageCaption">A deep network for simulation super-resolution of face performances</div> <div class="imageCredit">ACM</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Eftychios&nbsp;Sifakis <div class="imageTitle">Super-resolution</div> </div> </li><li> <a href="/por/images/Reports/POR/2024/2008584/2008584_10674101_1706647023329_Teaser--rgov-214x142.png" original="/por/images/Reports/POR/2024/2008584/2008584_10674101_1706647023329_Teaser--rgov-800width.png" title="Reaging system"><img src="/por/images/Reports/POR/2024/2008584/2008584_10674101_1706647023329_Teaser--rgov-66x44.png" alt="Reaging system"></a> <div class="imageCaptionContainer"> <div class="imageCaption">A neural-network based video re-aging system under complex lighting and motion scenarios</div> <div class="imageCredit">ACM</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Eftychios&nbsp;Sifakis <div class="imageTitle">Reaging system</div> </div> </li></ul> </div> </div></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[      The goal of this research project was to investigate novel ways to hybridize Machine Learning and Physics Based Simulation for the purpose of facilitating next-generation content creation tools for Computer-Generated face animation of digital actors.      A core idea that reflected in several of the products of this work was that Machine Learning techniques can fundamentally aid in the creation of simulation-ready models of 3D facial anatomy, without the painstaking and intricate process that digital artists would prepare such physics-inspired model for computer simulation: Traditionally, all anatomical parts of a human face, including detail shape of flesh, contact areas with bones, geometric placement of muscles, and directional layout of active fibers in active muscles would all need to be crafted from first principles, and adapted to individual identities of digital actors. This modeling effort would take weeks or months of expert tuning, and would not be trivially transferable to actors other than the one specific identity such model was prototyped for. The premise of our work was that this effort could be circumvented, if one could leverage a large dataset of high-resolution, 3D performance captured demonstrations of how a human actor articulates expressions and speech, and use machine learning to automatically infer the mechanism by which the human face navigates through all such expressions and poses. We have demonstrated this both in the context of an identity-specific model whose mechanism is learned from data, and multi-identity paradigms, where both the expression and the identity of the digital actor are tunable inputs to the neural network. In either case, the output of our system is a fully simulation-compatible 3D model, which can be integrated with additional physical effects, such as contact, collision, gravity, or external forces from the surrounding environment.      Ancillary to this core thrust, our research provided additional demonstrations of hybridization of machine learning with facial animation and/or simulation pipelines, including a framework for super-resolution by which a moderate-quality, real-time face simulator can be boosted to the quality level of expensive, intricate offline simulators, and a neural-network based system for automatically adjusting the age of facial performances in video, under varied and challenging motion and lighting conditions.             Last Modified: 01/30/2024       Submitted by: EftychiosSifakis]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
