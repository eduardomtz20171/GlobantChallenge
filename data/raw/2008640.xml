<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle><![CDATA[Collaborative Research: SHF: Small: Automated Quantitative Assessment of Testing Difficulty]]></AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/15/2020</AwardEffectiveDate>
<AwardExpirationDate>07/31/2024</AwardExpirationDate>
<AwardTotalIntnAmount>128480.00</AwardTotalIntnAmount>
<AwardAmount>128480</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Sol Greenspan</SignBlockName>
<PO_EMAI>sgreensp@nsf.gov</PO_EMAI>
<PO_PHON>7032927841</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Society is heavily reliant on software systems running on an increasingly large number of programmable devices embedded in it. Moreover, the amount of software in safety-critical systems such as cars and planes keeps increasing. Software-quality assurance is one of the most fundamental problems in this modern computing-dominated era. One can read about dependability and security problems caused by poor-quality software in the news everyday. It is extremely crucial to develop techniques that can improve the quality of software systems before they cause disastrous consequences during operation. The most common software-quality assurance technique is software testing. Although there has been a surge of progress in automated software-testing techniques, it is hard to predict their effectiveness. Given a piece of software, there is no existing technique that can predict how challenging it will be to automatically test that piece of software. In this project the goal is to develop techniques for assessing the difficulty of automatically testing software.&lt;br/&gt;&lt;br/&gt;Existing software-complexity metrics do not provide meaningful assessments of testing difficulty. This project's goal is to develop scalable techniques that can provide a quantitative assessment of testing difficulty. In order to be scalable and practical, the method has to rely on a level of abstraction that provides efficient analysis, while preserving meaningful characteristics of program behavior that relate to testing difficulty. The approach used in this project builds on two concepts that provide a promising abstraction for quantitative assessment of testing difficulty: 1) path complexity, and 2) path selectivity. Path complexity assesses how the number of paths in a given program increases with increasing execution depth, and path selectivity assesses the difficulty of finding values that satisfy a path condition. The team of researchers working on this project will develop techniques that automatically compute path complexity and path selectivity and then combine them to obtain a quantitative measure for testing difficulty. By developing techniques that can assess software-testing difficulty, this project will enable development of more effective software-testing techniques based on better resource allocation for software-quality assurance tasks. This will lead to improvements in software quality, and reduction in software defects that cause dependability and security problems. Secondly, the research activity will help to expose graduate and undergraduate students to software-quality assurance challenges and techniques. Finally, the research activity will help to disseminate the knowledge, techniques and tools developed within the scope of this project through publishing in open literature and making available the software tools that are developed as open source.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>08/04/2020</MinAmdLetterDate>
<MaxAmdLetterDate>08/04/2020</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2008640</AwardID>
<Investigator>
<FirstName>Lucas</FirstName>
<LastName>Bang</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Lucas A Bang</PI_FULL_NAME>
<EmailAddress><![CDATA[bang@cs.hmc.edu]]></EmailAddress>
<NSF_ID>000809267</NSF_ID>
<StartDate>08/04/2020</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name><![CDATA[Harvey Mudd College]]></Name>
<CityName>CLAREMONT</CityName>
<ZipCode>917115901</ZipCode>
<PhoneNumber>9096218121</PhoneNumber>
<StreetAddress><![CDATA[301 PLATT BLVD]]></StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>28</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA28</CONGRESS_DISTRICT_ORG>
<ORG_UEI_NUM>C76JKA5JY2B3</ORG_UEI_NUM>
<ORG_LGL_BUS_NAME>HARVEY MUDD COLLEGE</ORG_LGL_BUS_NAME>
<ORG_PRNT_UEI_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[Harvey Mudd College]]></Name>
<CityName>Claremont</CityName>
<StateCode>CA</StateCode>
<ZipCode>917115990</ZipCode>
<StreetAddress><![CDATA[301 Platt Blvd, Computer Science]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>28</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA28</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>779800</Code>
<Text>Software &amp; Hardware Foundation</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7944</Code>
<Text>SOFTWARE ENG &amp; FORMAL METHODS</Text>
</ProgramReference>
<Appropriation>
<Code>0120</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Fund>
<Code>01002021DB</Code>
<Name><![CDATA[NSF RESEARCH & RELATED ACTIVIT]]></Name>
<FUND_SYMB_ID>040100</FUND_SYMB_ID>
</Fund>
<FUND_OBLG>2020~128480</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><strong id="docs-internal-guid-0d8d5407-7fff-e650-cd80-2de090eca792" style="font-weight: normal;">  <p style="line-height: 1.7142857142857142; margin-top: 8pt; margin-bottom: 8pt;" dir="ltr"><span style="font-size: 10.5pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">This project focused on developing innovative methods for assessing the difficulty of software testing, a critical component of ensuring software quality. Earlier software complexity metrics often fail to provide meaningful insights into testing difficulty, a key challenge in modern computing systems. Our work developed scalable, practical techniques that quantitatively assess testing difficulty through the concepts of path complexity. Path complexity quantifies how many different ways a program can execute as a function of the length of execution. The primary outcomes of this project, achieved at Harvey Mudd College (HMC), involved developing the tool Metrinome to perform path complexity analysis of programs.</span></p>  <p style="line-height: 1.7142857142857142; margin-top: 8pt; margin-bottom: 8pt;" dir="ltr"><span style="font-size: 10.5pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">Over the project&rsquo;s duration, six HMC undergraduate students were supported by the award and gained valuable research experience through their involvement in Metrinome's development and experimental validation. These students contributed to algorithm design, implementation, and data analysis, enhancing their skills and preparing them for advanced academic and professional opportunities. Several participants have pursued or plan to pursue graduate studies in computer science.</span></p>  <p style="line-height: 1.7142857142857142; margin-top: 8pt; margin-bottom: 8pt;" dir="ltr"><span style="font-size: 10.5pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">This project addressed a gap in software quality assurance by developing robust methods to assess and predict software testing difficulty. The innovations from this work, particularly the Metrinome tool and its extensions, contribute significantly to the fields of software testing and program analysis. The training of undergraduate researchers and the dissemination of open-source tools further amplify the project's impact, ensuring lasting availability to the research community.</span></p>  </strong><br class="Apple-interchange-newline" /></p><br> <p>  Last Modified: 12/03/2024<br> Modified by: Lucas&nbsp;A&nbsp;Bang</p></div> <div class="porSideCol" ></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[      This project focused on developing innovative methods for assessing the difficulty of software testing, a critical component of ensuring software quality. Earlier software complexity metrics often fail to provide meaningful insights into testing difficulty, a key challenge in modern computing systems. Our work developed scalable, practical techniques that quantitatively assess testing difficulty through the concepts of path complexity. Path complexity quantifies how many different ways a program can execute as a function of the length of execution. The primary outcomes of this project, achieved at Harvey Mudd College (HMC), involved developing the tool Metrinome to perform path complexity analysis of programs.    Over the projects duration, six HMC undergraduate students were supported by the award and gained valuable research experience through their involvement in Metrinome's development and experimental validation. These students contributed to algorithm design, implementation, and data analysis, enhancing their skills and preparing them for advanced academic and professional opportunities. Several participants have pursued or plan to pursue graduate studies in computer science.    This project addressed a gap in software quality assurance by developing robust methods to assess and predict software testing difficulty. The innovations from this work, particularly the Metrinome tool and its extensions, contribute significantly to the fields of software testing and program analysis. The training of undergraduate researchers and the dissemination of open-source tools further amplify the project's impact, ensuring lasting availability to the research community.        Last Modified: 12/03/2024       Submitted by: LucasABang]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
