<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle><![CDATA[III: Small: Collaborative Research: Learning Active Physics-Based Models from Data]]></AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2020</AwardEffectiveDate>
<AwardExpirationDate>06/30/2024</AwardExpirationDate>
<AwardTotalIntnAmount>249999.00</AwardTotalIntnAmount>
<AwardAmount>249999</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Sylvia Spengler</SignBlockName>
<PO_EMAI>sspengle@nsf.gov</PO_EMAI>
<PO_PHON>7032927347</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project explores a novel algorithmic framework for automatic generation of digital models of objects from our natural world, that faithfully reproduce the structure and function of their physical counterparts. We specifically focus on modeling active deformable objects, i.e., objects capable of producing internal forces within their own bodies, such as biological muscles or robotic actuators. Our approach differs from the traditional modeling pipeline by learning the digital models from example data of the mechanism in-action, rather than by manually engineering them from the first principles. We adapt current state-of-the-art deep learning techniques to our problem, in particular artificial neural networks, by endowing them with knowledge about the physics-based behavior of deformable materials. This is expected to significantly upgrade the capabilities of generic neural networks, which would be otherwise forced to learn the laws of physics from data, which is an unnecessary task because fundamental properties of deformable media, such as conservation of energy and rotational invariance, should simply be taken for granted. The proposed algorithmic framework will greatly simplify the creation of digital replicas of objects in our natural world, while enhancing their fidelity. This will empower Virtual and Augmented Reality deployments to deliver life-like experiences in educational and skill-training applications, such as virtual operating rooms or emergency response scenarios. Computer-hosted doubles of functional objects are also a valuable prototyping tool in the design and optimization of physical functional replicas, such as prosthetic devices.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;To achieve these goals, we hybridize a neural network with a differentiable simulator, which outputs the quasistatic (i.e. equilibrated) shape of an active elastic model as a function of input control parameters, and subject to prescribed (known) boundary conditions. The finite element-based simulator is based on Projective Dynamics and designed with differentiability in mind, which is a key feature that will enable smooth combination with the classical backpropagation algorithm and integration within existing deep learning frameworks, such as PyTorch. The input to the simulator allows the actuation controls to be prescribed at very fine granularity, potentially enabling each finite element to become its own independently controllable actuator. These fine-grained actuation controls will be generated by a convolutional neural network, which creates them using a low-dimensional time-varying control vector and constant (i.e., time-invariant) network weights. We train this aggregate pipeline, jointly inferring both the weights of the control network as well as the values of the latent variables associated with different input configurations, as to best explain the training set as the action of a low-dimensional control space. This core framework will subsequently be extended to 1) allow for processing of contact and collisions, 2) optimization of spatially-varying material parameters, 3) lifting the quasi-statics assumption and simulating time-varying dynamics.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>06/01/2020</MinAmdLetterDate>
<MaxAmdLetterDate>11/22/2022</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2008915</AwardID>
<Investigator>
<FirstName>Yin</FirstName>
<LastName>Yang</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Yin Yang</PI_FULL_NAME>
<EmailAddress><![CDATA[yin.yang@utah.edu]]></EmailAddress>
<NSF_ID>000663449</NSF_ID>
<StartDate>11/22/2022</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Ladislav</FirstName>
<LastName>Kavan</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Ladislav Kavan</PI_FULL_NAME>
<EmailAddress><![CDATA[ladislav.kavan@gmail.com]]></EmailAddress>
<NSF_ID>000645156</NSF_ID>
<StartDate>06/01/2020</StartDate>
<EndDate>11/22/2022</EndDate>
<RoleCode>Former Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name><![CDATA[University of Utah]]></Name>
<CityName>SALT LAKE CITY</CityName>
<ZipCode>841129049</ZipCode>
<PhoneNumber>8015816903</PhoneNumber>
<StreetAddress><![CDATA[201 PRESIDENTS CIR]]></StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Utah</StateName>
<StateCode>UT</StateCode>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>UT01</CONGRESS_DISTRICT_ORG>
<ORG_UEI_NUM>LL8GLEVH6MG3</ORG_UEI_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF UTAH</ORG_LGL_BUS_NAME>
<ORG_PRNT_UEI_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Utah]]></Name>
<CityName>Salt Lake City</CityName>
<StateCode>UT</StateCode>
<ZipCode>841128930</ZipCode>
<StreetAddress><![CDATA[50 Central Campus Dr., #3190]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Utah</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>UT01</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>736400</Code>
<Text>Info Integration &amp; Informatics</Text>
</ProgramElement>
<ProgramReference>
<Code>7364</Code>
<Text>INFO INTEGRATION &amp; INFORMATICS</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0120</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Fund>
<Code>01002021DB</Code>
<Name><![CDATA[NSF RESEARCH & RELATED ACTIVIT]]></Name>
<FUND_SYMB_ID>040100</FUND_SYMB_ID>
</Fund>
<FUND_OBLG>2020~249999</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span id="docs-internal-guid-fcf6993d-7fff-1d21-5273-936cee923f8f"> </span></p> <p dir="ltr"><span>This project explores the novel modalities for data-driven physics i.e., bridging the virtual digital simulation with real-world observations, to improve the existing physical models for better computational efficiency, accuracy, and expressivity.&nbsp; During the project period, we systematically investigated and studied a series of novel cutting-edge computational techniques along this overarching goal. Specifically, some important research outcomes include:</span></p> <ol> <li dir="ltr"> <p dir="ltr"><span><strong>Deep-encoder-based model reduction method.</strong> We leverage deep neural networks to express nonlinear kinematic relations between the displacement of a 3D model and a set of low-dimensional latent coordinates. To be able to faithfully capture the variation this map, we developed novel differentiation techniques based on the complex-step finite difference (CSFD) so that the second-order (or higher-order) differentiation of this deep encoder can be efficiently calculated. Therefore, we can accurately compute the non-inertia force induced by varying nonlinear reduction. The simulation becomes both significantly accelerated and accurate.</span></p> </li> <li dir="ltr"> <p dir="ltr"><span><strong>Homogenization of yarn-level fabric model.</strong> Building digital models of fabric at the yarn level i.e., model nonlinear dynamics of each yarn thread generates highly realistic fabric simulation. Nonetheless, such models are known computationally expensive due to the large number of simulation degrees of freedom (DOFs) of yarn-level discretization. This project developed a new homogenization technique, that learns the complicated yarn-level dynamics and expresses it on a volumetric discretization with a customized material model. This allows us to simulate yarn-level fabric with much fewer DOFs and a more stable material model.&nbsp;</span></p> </li> <li dir="ltr"> <p dir="ltr"><span><strong>What you see is what you simulate.</strong> Conventional digital simulation assumes the input 3D model is given. This necessitates a lengthy and complicated geometry processing for the mesh generation. This project investigated a novel simulation method that directly applies physics simulation to real-world images/videos based on neural implicit representations such as NeRF and Gaussian Splatting. By implicitly formulating nonlinear dynamic equations at image space, we enable &ldquo;what you see is what you simulate&rdquo; so that uses can generate physically accurate dynamics from multi-view images and videos.</span></p> </li> </ol> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>  Last Modified: 10/09/2024<br> Modified by: Yin&nbsp;Yang</p></div> <div class="porSideCol" ><div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)          </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2024/2008915/2008915_10674097_1728502647984_comparison_no_diff--rgov-214x142.png" original="/por/images/Reports/POR/2024/2008915/2008915_10674097_1728502647984_comparison_no_diff--rgov-800width.png" title="High-order differentiable autoencoder"><img src="/por/images/Reports/POR/2024/2008915/2008915_10674097_1728502647984_comparison_no_diff--rgov-66x44.png" alt="High-order differentiable autoencoder"></a> <div class="imageCaptionContainer"> <div class="imageCaption">We developed a novel nonlinear model reduction method to significantly improve simulation DOFs while a deep autoencoder. We enable the computation of high-order differenetiation of the network to capture non-ineartia force induced by the nonlinear reduction accurately.</div> <div class="imageCredit">SIGGRAPH 2021</div> <div class="imagePermisssions">Public Domain</div> <div class="imageSubmitted">Yin&nbsp;Yang <div class="imageTitle">High-order differentiable autoencoder</div> </div> </li><li> <a href="/por/images/Reports/POR/2024/2008915/2008915_10674097_1728503537597_vol_homo--rgov-214x142.png" original="/por/images/Reports/POR/2024/2008915/2008915_10674097_1728503537597_vol_homo--rgov-800width.png" title="Volume homogenization of yarn-level fabric"><img src="/por/images/Reports/POR/2024/2008915/2008915_10674097_1728503537597_vol_homo--rgov-66x44.png" alt="Volume homogenization of yarn-level fabric"></a> <div class="imageCaptionContainer"> <div class="imageCaption">This project enables a new data-driven homogenization technique that allows yarn-level simulation of fabric at a regular volumetric grid.</div> <div class="imageCredit">SIGGRAPH Asia 2024</div> <div class="imagePermisssions">Public Domain</div> <div class="imageSubmitted">Yin&nbsp;Yang <div class="imageTitle">Volume homogenization of yarn-level fabric</div> </div> </li><li> <a href="/por/images/Reports/POR/2024/2008915/2008915_10674097_1728502386663_pienerf--rgov-214x142.png" original="/por/images/Reports/POR/2024/2008915/2008915_10674097_1728502386663_pienerf--rgov-800width.png" title="PIE-NeRF"><img src="/por/images/Reports/POR/2024/2008915/2008915_10674097_1728502386663_pienerf--rgov-66x44.png" alt="PIE-NeRF"></a> <div class="imageCaptionContainer"> <div class="imageCaption">PIE-NeRF is an efficient and versatile pipeline that synthesizes physics-based novel motions of complex NeRF models interactively. In this example, the user interactively manipulates the plant by applying external forces with the mouse.</div> <div class="imageCredit">CVRP 2024</div> <div class="imagePermisssions">Public Domain</div> <div class="imageSubmitted">Yin&nbsp;Yang <div class="imageTitle">PIE-NeRF</div> </div> </li><li> <a href="/por/images/Reports/POR/2024/2008915/2008915_10674097_1728502476937_physgaussian--rgov-214x142.png" original="/por/images/Reports/POR/2024/2008915/2008915_10674097_1728502476937_physgaussian--rgov-800width.png" title="PhysGaussian"><img src="/por/images/Reports/POR/2024/2008915/2008915_10674097_1728502476937_physgaussian--rgov-66x44.png" alt="PhysGaussian"></a> <div class="imageCaptionContainer"> <div class="imageCaption">PhysGaussian is a unified simulation-rendering pipeline that incorporates 3D Gaussian splatting representation and continuum mechanics to generate physics-based dynamics and photo-realistic renderings simultaneously and seamlessly.</div> <div class="imageCredit">CVPR 2024</div> <div class="imagePermisssions">Public Domain</div> <div class="imageSubmitted">Yin&nbsp;Yang <div class="imageTitle">PhysGaussian</div> </div> </li></ul> </div> </div></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[      This project explores the novel modalities for data-driven physics i.e., bridging the virtual digital simulation with real-world observations, to improve the existing physical models for better computational efficiency, accuracy, and expressivity. During the project period, we systematically investigated and studied a series of novel cutting-edge computational techniques along this overarching goal. Specifically, some important research outcomes include:     Deep-encoder-based model reduction method. We leverage deep neural networks to express nonlinear kinematic relations between the displacement of a 3D model and a set of low-dimensional latent coordinates. To be able to faithfully capture the variation this map, we developed novel differentiation techniques based on the complex-step finite difference (CSFD) so that the second-order (or higher-order) differentiation of this deep encoder can be efficiently calculated. Therefore, we can accurately compute the non-inertia force induced by varying nonlinear reduction. The simulation becomes both significantly accelerated and accurate.     Homogenization of yarn-level fabric model. Building digital models of fabric at the yarn level i.e., model nonlinear dynamics of each yarn thread generates highly realistic fabric simulation. Nonetheless, such models are known computationally expensive due to the large number of simulation degrees of freedom (DOFs) of yarn-level discretization. This project developed a new homogenization technique, that learns the complicated yarn-level dynamics and expresses it on a volumetric discretization with a customized material model. This allows us to simulate yarn-level fabric with much fewer DOFs and a more stable material model.     What you see is what you simulate. Conventional digital simulation assumes the input 3D model is given. This necessitates a lengthy and complicated geometry processing for the mesh generation. This project investigated a novel simulation method that directly applies physics simulation to real-world images/videos based on neural implicit representations such as NeRF and Gaussian Splatting. By implicitly formulating nonlinear dynamic equations at image space, we enable what you see is what you simulate so that uses can generate physically accurate dynamics from multi-view images and videos.             Last Modified: 10/09/2024       Submitted by: YinYang]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
