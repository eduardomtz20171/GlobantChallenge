<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle><![CDATA[CHS: Small: Collaborative Research: Articulate+ - A Conversational Interface for Democr atizing Visual Analysis]]></AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2020</AwardEffectiveDate>
<AwardExpirationDate>09/30/2023</AwardExpirationDate>
<AwardTotalIntnAmount>165999.00</AwardTotalIntnAmount>
<AwardAmount>181999</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Dan Cosley</SignBlockName>
<PO_EMAI>dcosley@nsf.gov</PO_EMAI>
<PO_PHON>7032928832</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The Internet gives the public, including scientists, access to vast amounts of data. Visualization is a powerful tool to take that data and turn it into a form that is easier to understand, helping people make better decisions. However, creating good visualizations from data is not easy. The Articulate+ Project will work to generate new knowledge about how to empower people with information visualization, by using speech and gesture as inputs, in new ways. A goal is to enable users to benefit from imprecise statements about what they want, rather than needing to give specific commands telling the computer what to do. This project will make it easier for citizens to engage in conversations with the computer about economic, medical data, demographic, transportation data, climate, and sports data; to answers that are more meaningful, and so to democratize data literacy. &lt;br/&gt;&lt;br/&gt;This research will use speech, gesture, and log data as new input interaction modalities for visual analytics. It will develop new understandings of how the imprecise and vague nature of natural language queries and gestures, as contextualized by work on a specific visual analysis problem, can be modeled as an opportunity for an intelligent software system to learn more about the underlying intent of the users. This model of intent, in turn, will be used to provide contextualized visualizations, which are expected to help those users gain valuable insights from their data. The audio speech data will be used to computationally model overhearing, to help infer users' current contextualized goals. The gesture data will be used to disambiguate expressions that refer to visual elements of a visualization (e.g., "that pie chart about electricity consumption"). The project will develop new understandings of how to combine the log of visualization state, the computational model of overhearing, and the gesture data in order to generate new visualizations in support of users’ work on tasks. Evaluations will be performed in both controlled laboratory and naturalistic study settings to determine whether effective semantic parsers can be derived for these specific visual analysis domains, and how this contextual interface affects users’ experience of visualization and discovery.  The integrated annotated datasets from the studies will be made available to the research community satisfying a need for ecologically valid situated language interaction corpora.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>09/15/2020</MinAmdLetterDate>
<MaxAmdLetterDate>07/21/2021</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2008986</AwardID>
<Investigator>
<FirstName>Jason</FirstName>
<LastName>Leigh</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jason Leigh</PI_FULL_NAME>
<EmailAddress><![CDATA[leighj@hawaii.edu]]></EmailAddress>
<NSF_ID>000423955</NSF_ID>
<StartDate>09/15/2020</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name><![CDATA[University of Hawaii]]></Name>
<CityName>HONOLULU</CityName>
<ZipCode>968222247</ZipCode>
<PhoneNumber>8089567800</PhoneNumber>
<StreetAddress><![CDATA[2425 CAMPUS RD SINCLAIR RM 1]]></StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Hawaii</StateName>
<StateCode>HI</StateCode>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>HI01</CONGRESS_DISTRICT_ORG>
<ORG_UEI_NUM>NSCKLFSSABF2</ORG_UEI_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF HAWAII</ORG_LGL_BUS_NAME>
<ORG_PRNT_UEI_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Hawaii]]></Name>
<CityName>Honolulu</CityName>
<StateCode>HI</StateCode>
<ZipCode>968221100</ZipCode>
<StreetAddress><![CDATA[2444 Dole St]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Hawaii</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>HI01</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>736700</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0120</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0121</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Fund>
<Code>01002021DB</Code>
<Name><![CDATA[NSF RESEARCH & RELATED ACTIVIT]]></Name>
<FUND_SYMB_ID>040100</FUND_SYMB_ID>
</Fund>
<Fund>
<Code>01002122DB</Code>
<Name><![CDATA[NSF RESEARCH & RELATED ACTIVIT]]></Name>
<FUND_SYMB_ID>040100</FUND_SYMB_ID>
</Fund>
<FUND_OBLG>2020~73741</FUND_OBLG>
<FUND_OBLG>2021~108258</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span id="docs-internal-guid-234d1e01-7fff-45d5-ce78-b16adfe96d7c"> </span></p> <p dir="ltr"><span>The goal of this project is to make it possible for humans to speak to a computer as naturally as they would another human, and have the computer be able to interpret complex data and produce answers in the form of data visualizations such as charts and graphs. This is important because it enables experts such as scientists and engineers, as well as non-experts such as policy makers, decision makers and the general public, to better understand complex data through the use of state-of-the-art visualization techniques (the process of turning data into informative charts), without having to be data visualization experts themselves.</span></p> <p>&nbsp;</p> <p dir="ltr"><span>In working to achieve this we made advances in Artificial Intelligence (AI) and data visualization research, especially in how to create natural language processing techniques to translate natural human speech to the language of data visualization and to support collaboration among people.</span></p> <p>&nbsp;</p> <p dir="ltr"><span>The project successfully developed and demonstrated a working prototype (Articulate+) which was able to produce data visualizations by listening to a conversation between two people, and interjecting during the conversation to produce data visualizations that were relevant to the conversation. Our system was tested with 50 participants (in 25 groups of two), who could ask natural questions on COVID data; Articulate+ was able to answer questions by interfacing to a number of authentic datasets and producing relevant charts (such as &ldquo;show me the correlation between COVID risk and access to medical care&rdquo;). This moves us closer to a future where AI systems can routinely aid humans in complex tasks as if the AI were a human assistant- like a co-pilot of an airplane.&nbsp;</span></p> <p>&nbsp;</p> <p dir="ltr"><span>Our work represents a major paradigm shift from conventional AI systems like Amazon&rsquo;s Alexa and Apple&rsquo;s Siri, which can only respond to a user when explicitly commanded to do so using a wake word, like &ldquo;Alexa&hellip;&rdquo; or &ldquo;Hey Siri&hellip;&rdquo;. Our system is able to listen to a conversation, build up a context of what has already been said, and use that context to determine the relevant charts to produce. We discovered a number of benefits to this approach:</span></p> <p>&nbsp;</p> <ul> <li dir="ltr"> <p dir="ltr"><span>By having an AI continuously listening to a conversation, the collected context can be used by our system to correct imprecise, misheard, or incomplete queries from users, enabling the AI to better determine the intent of its users during a conversation.</span></p> </li> <li dir="ltr"> <p dir="ltr"><span>The common use of wake words in conventional AI systems, has an unanticipated effect of conditioning users to speak in an unnatural and stilted way when conversing with AI. By eliminating the need for wake words, conversations between users and our system flowed more naturally like in a human to human conversation.&nbsp;</span></p> </li> </ul> <p>&nbsp;</p> <p dir="ltr"><span>Our project also examined how well current state-of-the-art AI systems like ChatGPT can be made to perform data visualization tasks, in particular how its Large Language Models can be improved to produce more accurate data visualizations. We discovered that currently, ChatGPT is able to produce only the most basic chart types.</span></p> <p>&nbsp;</p> <p dir="ltr"><span>During the course of this project 4 papers were published. One graduate student obtained his Master&rsquo;s degree, and is continuing toward a PhD degree in AI, another graduate student finished her MS degree and is continuing toward a PhD in human-computer interaction, and yet another graduate&nbsp; student is continuing on with her PhD in natural language processing. One graduate student completed a PhD, and another graduated to become an assistant professor. Of the 5 students mentioned here, 3 are women. </span></p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>  Last Modified: 12/15/2023<br> Modified by: Jason&nbsp;Leigh</p></div> <div class="porSideCol" ><div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Image         </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation onePhoto" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2023/2008986/2008986_10706860_1702698962898_ARTI--rgov-214x142.jpg" original="/por/images/Reports/POR/2023/2008986/2008986_10706860_1702698962898_ARTI--rgov-800width.jpg" title="Articulate+ : a system for producing charts using natural language"><img src="/por/images/Reports/POR/2023/2008986/2008986_10706860_1702698962898_ARTI--rgov-66x44.jpg" alt="Articulate+ : a system for producing charts using natural language"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Two users talking to each other as well as Articulate+ to investigate COVID data.</div> <div class="imageCredit">Jason Leigh - Laboratory for Advanced Visualization and Applications, University of Hawaii at Manoa</div> <div class="imagePermisssions">Public Domain</div> <div class="imageSubmitted">Jason&nbsp;Leigh <div class="imageTitle">Articulate+ : a system for producing charts using natural language</div> </div> </li></ul> </div> </div></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[      The goal of this project is to make it possible for humans to speak to a computer as naturally as they would another human, and have the computer be able to interpret complex data and produce answers in the form of data visualizations such as charts and graphs. This is important because it enables experts such as scientists and engineers, as well as non-experts such as policy makers, decision makers and the general public, to better understand complex data through the use of state-of-the-art visualization techniques (the process of turning data into informative charts), without having to be data visualization experts themselves.      In working to achieve this we made advances in Artificial Intelligence (AI) and data visualization research, especially in how to create natural language processing techniques to translate natural human speech to the language of data visualization and to support collaboration among people.      The project successfully developed and demonstrated a working prototype (Articulate+) which was able to produce data visualizations by listening to a conversation between two people, and interjecting during the conversation to produce data visualizations that were relevant to the conversation. Our system was tested with 50 participants (in 25 groups of two), who could ask natural questions on COVID data; Articulate+ was able to answer questions by interfacing to a number of authentic datasets and producing relevant charts (such as show me the correlation between COVID risk and access to medical care). This moves us closer to a future where AI systems can routinely aid humans in complex tasks as if the AI were a human assistant- like a co-pilot of an airplane.      Our work represents a major paradigm shift from conventional AI systems like Amazons Alexa and Apples Siri, which can only respond to a user when explicitly commanded to do so using a wake word, like Alexa or Hey Siri. Our system is able to listen to a conversation, build up a context of what has already been said, and use that context to determine the relevant charts to produce. We discovered a number of benefits to this approach:        By having an AI continuously listening to a conversation, the collected context can be used by our system to correct imprecise, misheard, or incomplete queries from users, enabling the AI to better determine the intent of its users during a conversation.     The common use of wake words in conventional AI systems, has an unanticipated effect of conditioning users to speak in an unnatural and stilted way when conversing with AI. By eliminating the need for wake words, conversations between users and our system flowed more naturally like in a human to human conversation.        Our project also examined how well current state-of-the-art AI systems like ChatGPT can be made to perform data visualization tasks, in particular how its Large Language Models can be improved to produce more accurate data visualizations. We discovered that currently, ChatGPT is able to produce only the most basic chart types.      During the course of this project 4 papers were published. One graduate student obtained his Masters degree, and is continuing toward a PhD degree in AI, another graduate student finished her MS degree and is continuing toward a PhD in human-computer interaction, and yet another graduate student is continuing on with her PhD in natural language processing. One graduate student completed a PhD, and another graduated to become an assistant professor. Of the 5 students mentioned here, 3 are women.            Last Modified: 12/15/2023       Submitted by: JasonLeigh]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
