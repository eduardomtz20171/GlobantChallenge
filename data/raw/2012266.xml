<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle><![CDATA[Collaborative Research: Computational Harmonic Analysis Approach to Active Learning]]></AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2020</AwardEffectiveDate>
<AwardExpirationDate>06/30/2024</AwardExpirationDate>
<AwardTotalIntnAmount>219501.00</AwardTotalIntnAmount>
<AwardAmount>219501</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>03040000</Code>
<Directorate>
<Abbreviation>MPS</Abbreviation>
<LongName>Direct For Mathematical &amp; Physical Scien</LongName>
</Directorate>
<Division>
<Abbreviation>DMS</Abbreviation>
<LongName>Division Of Mathematical Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Yuliya Gorb</SignBlockName>
<PO_EMAI>ygorb@nsf.gov</PO_EMAI>
<PO_PHON>7032922113</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Research in supervised learning is concerned with uncovering relationships between training data and some function or label that is attached to each datum, with the goal of generalizing to new samples. Modern machine learning tools, such as deep networks, typically require a huge set of training data in order to classify the rest of the data with sufficient confidence. Obviously, assigning an accurate label to a datum can be an expensive task, involving a great deal of human effort. This project seeks to develop methods to classify large amounts of data with a theoretically minimal number of training labels. The key to classifying with a small number of labels comes with the ability to choose at which data points a label will be queried. This collaborative research project will study these methods, known as active machine learning, from a geometric and harmonic analysis perspective, focusing on both algorithmic insights and theoretical guarantees. The ability to perform classification with a small number of labeled points has important implications in a variety of applications, including remote sensing classification, medical data analysis, and general applications where it is expensive to collect labels.&lt;br/&gt;&lt;br/&gt;This project applies knowledge in computational harmonic analysis, function approximation, and machine learning to the study of active learning models, focusing on algorithmic insights, efficient implementations, and performance guarantees for both novel algorithms and currently existing machine learning algorithms. Mathematical tools, including localized kernel construction, approximation analysis in terms of intrinsic dimensionality, and harmonic analysis of eigenfunctions of operators on graphs and manifolds, have natural applications in the study of these areas. Specifically, the project addresses four fundamental questions that arise in the field: (1) How do you conservatively propagate the sampled labels to new points when the labels form a hierarchical clustering with possibly zero minimal separation between clusters? (2) Does the mechanism of kernel active learning generalize to graphs, where naive choice of points to sample becomes a combinatorial optimization problem? (3) Can we incorporate the structure of a neural network (or general parametric) classifier into the choice of labels queried and provably bound the generalization error for predictions on the rest of the data? (4) How can we tailor our framework to transfer learning and high-dimensional imaging?&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>05/19/2020</MinAmdLetterDate>
<MaxAmdLetterDate>05/19/2020</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.049</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2012266</AwardID>
<Investigator>
<FirstName>Alexander</FirstName>
<LastName>Cloninger</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Alexander Cloninger</PI_FULL_NAME>
<EmailAddress><![CDATA[acloninger@ucsd.edu]]></EmailAddress>
<NSF_ID>000655694</NSF_ID>
<StartDate>05/19/2020</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name><![CDATA[University of California-San Diego]]></Name>
<CityName>LA JOLLA</CityName>
<ZipCode>920930021</ZipCode>
<PhoneNumber>8585344896</PhoneNumber>
<StreetAddress><![CDATA[9500 GILMAN DR]]></StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>50</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA50</CONGRESS_DISTRICT_ORG>
<ORG_UEI_NUM>UYTTZT6G9DT1</ORG_UEI_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CALIFORNIA, SAN DIEGO</ORG_LGL_BUS_NAME>
<ORG_PRNT_UEI_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of California-San Diego]]></Name>
<CityName>La Jolla</CityName>
<StateCode>CA</StateCode>
<ZipCode>920930112</ZipCode>
<StreetAddress><![CDATA[9500 Gilman Drive #0112]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>50</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA50</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>127100</Code>
<Text>COMPUTATIONAL MATHEMATICS</Text>
</ProgramElement>
<ProgramReference>
<Code>079Z</Code>
<Text>Machine Learning Theory</Text>
</ProgramReference>
<ProgramReference>
<Code>9263</Code>
<Text>COMPUTATIONAL SCIENCE &amp; ENGING</Text>
</ProgramReference>
<Appropriation>
<Code>0120</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Fund>
<Code>01002021DB</Code>
<Name><![CDATA[NSF RESEARCH & RELATED ACTIVIT]]></Name>
<FUND_SYMB_ID>040100</FUND_SYMB_ID>
</Fund>
<FUND_OBLG>2020~219501</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Active learning is a paradigm in machine learning where the algorithm strategically selects the most informative data points for labeling, aiming to maximize model performance while minimizing labeling costs. This approach is particularly effective when dealing with large datasets where manual labeling is expensive or time-consuming. By focusing on data points near decision boundaries or data points that are representative of a given cluster, active learning algorithms can efficiently improve model accuracy and generalization. These boundary points are often the most challenging for the model to classify and therefore provide the most valuable information for refining the decision boundary.  This approach has been successfully applied in various domains, including text classification, image recognition, and biomedical research, where it has significantly reduced the time and resources required for building accurate machine learning models.</p> <p>Much of our research has focused on developing algorithms and theoretical analysis for querying the class label at a very small number of judiciously chosen points so as to be able to attach the appropriate class label to every point in the set.  As part of this award, we have developed a novel approach based on localized kernels that is capable of learning a hierarchical estimate of the supports of class label distributions without any assumptions on the number of classes or even on the minimal separation between the supports.   Similarly, we've developed statistical convergence results for the kernel witness function, which can be used to detect the boundaries between these supports.&nbsp; We've applied these methods to hyperspectral image classification, determining ``in-class'' and ``out-of-class'' regions in the latent space of a generative model, and determining propensity of treatment for various populations.</p> <p>Another line of research has focused on generalizing active learning algorithms, traditionally defined on Euclidean space, to data that is graph based.   As a part of this award, we have developed several approaches for learning where to sample labels to approximate function means on graphs, and show the number of labels needed is vastly smaller than the number of data points available.&nbsp; We also developed novel methods for active exploration sampling on graphs that serves as a computationally efficient proxy for maximizing the Fiedler value of the unlabeled data submatrix of the graph Laplacian.&nbsp; Similarly, we've developed various novel versions of distances on graphs, including generalizations of effective resistance and optimal transport to connection graphs, and distance measures between time series and random fields that go to zero when there is partial overlap of the distributions.  These notions of distance can be instrumental to defining active learning based exploration strategies on graph based data sets.  We've applied these methods to node classification in citation networks, learning on time series windows and image patches, and vector field clustering.</p> <p>Finally, we've developed methods for incorporating our analyses into off-the-shelf ML algorithms.&nbsp; As part of this grant, we developed hierarchical exploration based strategies for streaming boosted kernel regression, and show that the method can achieve zero training error using a small subset of the labels of the data points.&nbsp; Similarly, we've demonstrated that neural networks learn witness functions and powerful statistical tests between distributions incredibly quickly, much faster and with fewer number of points than traditional neural tangent kernel analysis would suggest.  This implies that the boundary between classes can be detected incredibly quickly in training, which would allow one to begin actively sampling labels near the boundary after only a couple epochs of training.&nbsp; We've applied these methods to statistical two sample testing between distributions, and forecasting of dynamical systems given an initial condition and training data from other trajectories.</p> <p>The research we conducted involved five graduate students at UCSD, and two graduate students and one postdoc at CGU.&nbsp; Our results for this collaborative award across UCSD's and CGU's grants have resulted in 28 publications and numerous preprints, was presented at 51 seminars and conferences, and 5 workshops or minisymposium were organized by the authors on topics related to this grant.&nbsp; Additionally, some of the mathematical methods we used and developed were taught in graduate courses developed by the PIs.</p> <p>&nbsp;</p><br> <p>  Last Modified: 10/18/2024<br> Modified by: Alexander&nbsp;Cloninger</p></div> <div class="porSideCol" ></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[  Active learning is a paradigm in machine learning where the algorithm strategically selects the most informative data points for labeling, aiming to maximize model performance while minimizing labeling costs. This approach is particularly effective when dealing with large datasets where manual labeling is expensive or time-consuming. By focusing on data points near decision boundaries or data points that are representative of a given cluster, active learning algorithms can efficiently improve model accuracy and generalization. These boundary points are often the most challenging for the model to classify and therefore provide the most valuable information for refining the decision boundary.  This approach has been successfully applied in various domains, including text classification, image recognition, and biomedical research, where it has significantly reduced the time and resources required for building accurate machine learning models.   Much of our research has focused on developing algorithms and theoretical analysis for querying the class label at a very small number of judiciously chosen points so as to be able to attach the appropriate class label to every point in the set.  As part of this award, we have developed a novel approach based on localized kernels that is capable of learning a hierarchical estimate of the supports of class label distributions without any assumptions on the number of classes or even on the minimal separation between the supports.   Similarly, we've developed statistical convergence results for the kernel witness function, which can be used to detect the boundaries between these supports. We've applied these methods to hyperspectral image classification, determining ``in-class'' and ``out-of-class'' regions in the latent space of a generative model, and determining propensity of treatment for various populations.   Another line of research has focused on generalizing active learning algorithms, traditionally defined on Euclidean space, to data that is graph based.   As a part of this award, we have developed several approaches for learning where to sample labels to approximate function means on graphs, and show the number of labels needed is vastly smaller than the number of data points available. We also developed novel methods for active exploration sampling on graphs that serves as a computationally efficient proxy for maximizing the Fiedler value of the unlabeled data submatrix of the graph Laplacian. Similarly, we've developed various novel versions of distances on graphs, including generalizations of effective resistance and optimal transport to connection graphs, and distance measures between time series and random fields that go to zero when there is partial overlap of the distributions.  These notions of distance can be instrumental to defining active learning based exploration strategies on graph based data sets.  We've applied these methods to node classification in citation networks, learning on time series windows and image patches, and vector field clustering.   Finally, we've developed methods for incorporating our analyses into off-the-shelf ML algorithms. As part of this grant, we developed hierarchical exploration based strategies for streaming boosted kernel regression, and show that the method can achieve zero training error using a small subset of the labels of the data points. Similarly, we've demonstrated that neural networks learn witness functions and powerful statistical tests between distributions incredibly quickly, much faster and with fewer number of points than traditional neural tangent kernel analysis would suggest.  This implies that the boundary between classes can be detected incredibly quickly in training, which would allow one to begin actively sampling labels near the boundary after only a couple epochs of training. We've applied these methods to statistical two sample testing between distributions, and forecasting of dynamical systems given an initial condition and training data from other trajectories.   The research we conducted involved five graduate students at UCSD, and two graduate students and one postdoc at CGU. Our results for this collaborative award across UCSD's and CGU's grants have resulted in 28 publications and numerous preprints, was presented at 51 seminars and conferences, and 5 workshops or minisymposium were organized by the authors on topics related to this grant. Additionally, some of the mathematical methods we used and developed were taught in graduate courses developed by the PIs.        Last Modified: 10/18/2024       Submitted by: AlexanderCloninger]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
