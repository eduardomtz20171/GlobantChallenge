<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle><![CDATA[Collaborative Research: Statistical Inference for High Dimensional and High Frequency Data]]></AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2020</AwardEffectiveDate>
<AwardExpirationDate>06/30/2024</AwardExpirationDate>
<AwardTotalIntnAmount>149992.00</AwardTotalIntnAmount>
<AwardAmount>149992</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>03040000</Code>
<Directorate>
<Abbreviation>MPS</Abbreviation>
<LongName>Direct For Mathematical &amp; Physical Scien</LongName>
</Directorate>
<Division>
<Abbreviation>DMS</Abbreviation>
<LongName>Division Of Mathematical Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Yulia Gel</SignBlockName>
<PO_EMAI>ygel@nsf.gov</PO_EMAI>
<PO_PHON>7032920000</PO_PHON>
</ProgramOfficer>
<AbstractNarration>To pursue the promise of the big data revolution, the current project will focus on a common form of data, high dimensional high frequency data (HDHFD), where a snapshot of the data involves a large number of variables, and at the same time new data streams in every fraction of milliseconds. With technological advances in data collection, HDHFD occurs in medical applications from neuroscience to patient care; finance and economics; geosciences such as earthquake data; marine science including fishing and shipping; turbulence; internet data; and other areas where data streaming is available. The Principal Investigators' (PIs') research focuses on how to extract information from complex big data and how to turn data into knowledge. In particular, the project seeks to develop cutting-edge mathematics and statistical methodology to uncover the structure governing HDHFD systems. This structure is characterized by a web of dependence across both time and dimension, and the role of analysis is to provide guidance on how to reduce the complexity while retaining the important features of the data architecture. An integral part of this research is also about how to quantify the uncertainty in estimates and forecasts in HDHFD systems. In addition to developing a general theory, the project is concerned with applications to financial data, including risk management, forecasting, and portfolio management. More precise estimators, with improved margins of error, will be useful in all these areas of finance. The results are of interest to main-street investors, regulators and policymakers, and the results are entirely in the public domain. &lt;br/&gt;&lt;br/&gt;The purpose of this project is to explore high dimensional high frequency data (HDHFD) from several angles. A fundamental approach is to extend the PIsâ€™ contiguity theory. Under a contiguous probability, the structure of the observations is often more accessible (frequently Gaussian) in local neighborhoods, facilitating statistical analysis. This is achieved without altering current models. In a contribution to factor modeling of the HDHFD data, the PIs will explore time-varying matrix decompositions, including the development of a singular value decomposition (SVD) for high frequency data, as a more direct path to a factor model. We plan to compare the new SVD with PCA based methods, as well as L1 type methods such as nonnegative matrix factorization. The PIs have discovered a new way to look at time and cross-dimension dependence, originally developed by the PIs in connection with their observed asymptotic variance (observed AVAR). They will now look into the possibility to "borrow" information across time and dimension. This tool will be used for matrix decompositions, as well as to develop volatility matrices for the drift part of a financial process, which will interface with their planned work on matrix decompositions. The PIs will explore a path to an observed AVAR that takes place in continuous time, thereby improving accuracy and simplifying both implementation and theoretical analysis.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>06/26/2020</MinAmdLetterDate>
<MaxAmdLetterDate>06/26/2020</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.049</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2015530</AwardID>
<Investigator>
<FirstName>Lan</FirstName>
<LastName>Zhang</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Lan Zhang</PI_FULL_NAME>
<EmailAddress><![CDATA[lanzhang@uic.edu]]></EmailAddress>
<NSF_ID>000313767</NSF_ID>
<StartDate>06/26/2020</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name><![CDATA[University of Illinois at Chicago]]></Name>
<CityName>CHICAGO</CityName>
<ZipCode>606124305</ZipCode>
<PhoneNumber>3129962862</PhoneNumber>
<StreetAddress><![CDATA[809 S MARSHFIELD AVE M/C 551]]></StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<StateCode>IL</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IL07</CONGRESS_DISTRICT_ORG>
<ORG_UEI_NUM>W8XEAJDKMXH3</ORG_UEI_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF ILLINOIS</ORG_LGL_BUS_NAME>
<ORG_PRNT_UEI_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Illinois at Chicago]]></Name>
<CityName>Chicago</CityName>
<StateCode>IL</StateCode>
<ZipCode>606073401</ZipCode>
<StreetAddress><![CDATA[601 S Morgan St]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IL07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>126900</Code>
<Text>STATISTICS</Text>
</ProgramElement>
<Appropriation>
<Code>0120</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Fund>
<Code>01002021DB</Code>
<Name><![CDATA[NSF RESEARCH & RELATED ACTIVIT]]></Name>
<FUND_SYMB_ID>040100</FUND_SYMB_ID>
</Fund>
<FUND_OBLG>2020~149992</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Background: This project focuses on creating a statistical approach to handling big data in real-time application, thus turning data into knowledge. The vast amount of data comes from its high frequency and its high dimensionality. High frequency reflects the faster arriving nature of the data: for example, financial quotes and trades occur in irregular intervals of milli-seconds or even nano-seconds. High dimension refers to a large portfolio with hundreds of assets at any given moment. This type of big data poses challenges far beyond the capability of standard statistical devices.</p> <p>Intellectual Merit: This project creates a general framework for inference in high dimensional and high frequency data (HD<sup>2</sup>), based on a mathematical device, contiguity. Under this probabilistic setting, the structure of the observations is often more accessible in local neighborhoods, facilitating statistical analysis. This is achieved without altering current models. Also, the PIs have discovered a new way to look at time and cross-dimension dependence, in the sense of "borrowing" information across time and dimension.</p> <p>The availability of high-frequency high-dimensional financial data has made it possible to estimate various financial quantities in a daily or even shorter time window, thus improving the quality of large areas of financial econometrics. However, estimates naturally come with uncertainty. And quantifying the standard error of these estimators is often forbidding due to the complexity of the microstructure, as well as time and edge irregularity in a short time frame. A main finding of this project is the construction of a robust implementation of an off-the-shelf uncertainty measure in "The Observed Asymptotic Variance: Hard Edges, and a Regression Approach" (Journal of Econometrics, 2021).</p> <p>Another contribution is to develop a feasible Principal Component Analysis (PCA) in the setting of big data. This is a data-driven unsupervised learning approach. The PIs develop a robust covariance estimator ( Smoothed-TSRV) for data with microstructure noise. Armed with this approach, the PIs then pass from estimated time-varying covariance matrix to PCA ("The Five Trolls under the Bridge: Principal Component Analysis with Asynchronous and Noisy High Frequency Data", JASA, 2020). In an application, our first principal component (PC) closely matches but potentially outperforms the S&amp;P 100 market index. The accuracy of this new PCA may provide a firmer footing on which to export the index concept to markets (such as commodities) where there is less theoretical basis for how to weigh index components. Indices currently do exist in these cases, of course, but with less foundation than in the case for equities. Indices have substantial social value.</p> <p>A third contribution is to develop tests and realized regression that are tailored for big data with real-time updating. In "Realized Regression with Asynchronous and Noisy High Frequency Data" (Journal of Econometrics, 2024), the PIs developed both the theory and the method on constructing the high frequency-based regression estimators. These estimators are substantially more accurate than in classical regression and for an instance, it is possible to estimate time-varying factors up to at least several times a day, and for a large dimension of data variables of interest.&nbsp;&nbsp;In a related work "High Dimensional Regression Coefficient Test with High Frequency Data" (Journal of Econometrics, in press 2024), the PIs propose new test that can help identify those time periods when the high-frequency factors carry incremental information beyond the traditional asset pricing factors. This work could shed light on market timing.</p> <p>The fourth contribution is to develop theory for estimators in the class of the second difference type (2DE). In "A CLT for second difference estimators with an application to volatility and intensity" (Annals of Statistics, 2022), 2DEs are used to study the substantial connection between volatility and trading intensity of financial data. The methodology brings together aspects of financial data analysis and medical survival analysis, including the empirical study of ADHD.</p> <p>Broader Impacts: The data are mostly financial, and interest focuses on quantities like realized daily volatility, regression beta, volatility risk, intensity, and so on. The PIs also work on applications to risk management, forecasting, and portfolio management. More precise estimators, with more precise standard errors, will be useful in all these areas of finance. In particular, as one has seen in several recent episodes of financial turmoil, markets can change abruptly. Better measurements of market behavior will make early detection possible when market conditions change, greatly aiding the management of risk. The results will be of interest to investors, regulators and policymakers. The dependence structure also has use in other areas of research that use high frequency and dimensional data, including neural science, earthquakes, turbulence, and other areas where streaming data are available.</p> <p>&nbsp;</p><br> <p>  Last Modified: 10/31/2024<br> Modified by: Lan&nbsp;Zhang</p></div> <div class="porSideCol" ></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[  Background: This project focuses on creating a statistical approach to handling big data in real-time application, thus turning data into knowledge. The vast amount of data comes from its high frequency and its high dimensionality. High frequency reflects the faster arriving nature of the data: for example, financial quotes and trades occur in irregular intervals of milli-seconds or even nano-seconds. High dimension refers to a large portfolio with hundreds of assets at any given moment. This type of big data poses challenges far beyond the capability of standard statistical devices.   Intellectual Merit: This project creates a general framework for inference in high dimensional and high frequency data (HD2), based on a mathematical device, contiguity. Under this probabilistic setting, the structure of the observations is often more accessible in local neighborhoods, facilitating statistical analysis. This is achieved without altering current models. Also, the PIs have discovered a new way to look at time and cross-dimension dependence, in the sense of "borrowing" information across time and dimension.   The availability of high-frequency high-dimensional financial data has made it possible to estimate various financial quantities in a daily or even shorter time window, thus improving the quality of large areas of financial econometrics. However, estimates naturally come with uncertainty. And quantifying the standard error of these estimators is often forbidding due to the complexity of the microstructure, as well as time and edge irregularity in a short time frame. A main finding of this project is the construction of a robust implementation of an off-the-shelf uncertainty measure in "The Observed Asymptotic Variance: Hard Edges, and a Regression Approach" (Journal of Econometrics, 2021).   Another contribution is to develop a feasible Principal Component Analysis (PCA) in the setting of big data. This is a data-driven unsupervised learning approach. The PIs develop a robust covariance estimator ( Smoothed-TSRV) for data with microstructure noise. Armed with this approach, the PIs then pass from estimated time-varying covariance matrix to PCA ("The Five Trolls under the Bridge: Principal Component Analysis with Asynchronous and Noisy High Frequency Data", JASA, 2020). In an application, our first principal component (PC) closely matches but potentially outperforms the S&P 100 market index. The accuracy of this new PCA may provide a firmer footing on which to export the index concept to markets (such as commodities) where there is less theoretical basis for how to weigh index components. Indices currently do exist in these cases, of course, but with less foundation than in the case for equities. Indices have substantial social value.   A third contribution is to develop tests and realized regression that are tailored for big data with real-time updating. In "Realized Regression with Asynchronous and Noisy High Frequency Data" (Journal of Econometrics, 2024), the PIs developed both the theory and the method on constructing the high frequency-based regression estimators. These estimators are substantially more accurate than in classical regression and for an instance, it is possible to estimate time-varying factors up to at least several times a day, and for a large dimension of data variables of interest.In a related work "High Dimensional Regression Coefficient Test with High Frequency Data" (Journal of Econometrics, in press 2024), the PIs propose new test that can help identify those time periods when the high-frequency factors carry incremental information beyond the traditional asset pricing factors. This work could shed light on market timing.   The fourth contribution is to develop theory for estimators in the class of the second difference type (2DE). In "A CLT for second difference estimators with an application to volatility and intensity" (Annals of Statistics, 2022), 2DEs are used to study the substantial connection between volatility and trading intensity of financial data. The methodology brings together aspects of financial data analysis and medical survival analysis, including the empirical study of ADHD.   Broader Impacts: The data are mostly financial, and interest focuses on quantities like realized daily volatility, regression beta, volatility risk, intensity, and so on. The PIs also work on applications to risk management, forecasting, and portfolio management. More precise estimators, with more precise standard errors, will be useful in all these areas of finance. In particular, as one has seen in several recent episodes of financial turmoil, markets can change abruptly. Better measurements of market behavior will make early detection possible when market conditions change, greatly aiding the management of risk. The results will be of interest to investors, regulators and policymakers. The dependence structure also has use in other areas of research that use high frequency and dimensional data, including neural science, earthquakes, turbulence, and other areas where streaming data are available.        Last Modified: 10/31/2024       Submitted by: LanZhang]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
