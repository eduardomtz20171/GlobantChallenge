<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle><![CDATA[SHF: Small: Tools for Productive High-performance Computing with GPUs]]></AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/08/2019</AwardEffectiveDate>
<AwardExpirationDate>06/30/2023</AwardExpirationDate>
<AwardTotalIntnAmount>416146.00</AwardTotalIntnAmount>
<AwardAmount>416146</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Almadena Chtchelkanova</SignBlockName>
<PO_EMAI>achtchel@nsf.gov</PO_EMAI>
<PO_PHON>7032927498</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Graphical Processing Units (GPUs) are widely and cheaply available and have become increasingly powerful relative to general-purpose CPUs. Therefore, they are attractive targets for compute-intensive applications in computational science and data science. However, development of software to run on GPUs is time-consuming and requires expertise held by only a very small fraction of the application developer community. This project is developing a collection of tools to assist in the productive development of high-performance software for GPUs, so that the barrier to effective use of GPUs by the scientific community can be lowered.&lt;br/&gt;&lt;br/&gt;A central idea being pursued in this research is the identification of primary hardware resource bottlenecks that limit performance of a GPU kernel, to guide the modification of the kernel in a manner that seeks to alleviate the identified bottleneck. Abstract kernel emulation along with sensitivity analysis with respect to hardware resource latency/throughput parameters are used for bottleneck identification. Three usage scenarios are targeted: (1) OpenMP offload, (2) domain-specific code generators, and (3) CUDA/OpenCL kernels.  The offload model introduced in OpenMP 4.0 is an attractive approach for transforming existing legacy codes as well as for newly developed codes, to facilitate productivity and portability. Domain-specific library generators exploit pattern-specific semantics in order to perform optimizing transformations that are beyond the scope of general-purpose optimizing compilers. Tensor contractions and stencils are two domains of particular emphasis. For all targeted usage scenarios, the collection of tools is intended to assist developers improve the performance of GPU code through a combination of model-driven search and auto-tuning.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>02/07/2020</MinAmdLetterDate>
<MaxAmdLetterDate>04/09/2020</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2018016</AwardID>
<Investigator>
<FirstName>Ponnuswamy</FirstName>
<LastName>Sadayappan</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Ponnuswamy Sadayappan</PI_FULL_NAME>
<EmailAddress><![CDATA[saday@cs.utah.edu]]></EmailAddress>
<NSF_ID>000182536</NSF_ID>
<StartDate>02/07/2020</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name><![CDATA[University of Utah]]></Name>
<CityName>SALT LAKE CITY</CityName>
<ZipCode>841129049</ZipCode>
<PhoneNumber>8015816903</PhoneNumber>
<StreetAddress><![CDATA[201 PRESIDENTS CIR]]></StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Utah</StateName>
<StateCode>UT</StateCode>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>UT01</CONGRESS_DISTRICT_ORG>
<ORG_UEI_NUM>LL8GLEVH6MG3</ORG_UEI_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF UTAH</ORG_LGL_BUS_NAME>
<ORG_PRNT_UEI_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Utah]]></Name>
<CityName>SALT LAKE CITY</CityName>
<StateCode>UT</StateCode>
<ZipCode>841128930</ZipCode>
<StreetAddress><![CDATA[75 S 2000 E]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Utah</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>UT01</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>779800</Code>
<Text>Software &amp; Hardware Foundation</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7942</Code>
<Text>HIGH-PERFORMANCE COMPUTING</Text>
</ProgramReference>
<Appropriation>
<Code>0118</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Fund>
<Code>01001819DB</Code>
<Name><![CDATA[NSF RESEARCH & RELATED ACTIVIT]]></Name>
<FUND_SYMB_ID>040100</FUND_SYMB_ID>
</Fund>
<FUND_OBLG>2018~416146</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>GPUs (Graphical Processing Units) can achieve very high performance, but the development of efficient GPU programs is very challenging, requiring specialized knowledge possessed by very few programmers. The goal of this project was to develop tools and techniques that could enable users to achieve high productivity in developing high-performance GPU applications.</p> <p>The main outcomes of the project, through publicly released software and research publications, were:</p> <p>1)<span> </span>Development of an efficient mixed-mode data representation for sparse tensors on GPUs and its use in implementing efficient sparse tensor decomposition.</p> <p>2)<span> </span> Development a novel high-performance implementation of parallel NMF (Non-negative Matrix Factorization) on GPUs,&nbsp; based on the HALS (Hierarchical Alternating Least Squares) scheme via algorithmic transformations to enhance data locality.</p> <p>3)<span> </span>Development of a segmented fused-tiled strategy for implementing CNN (Convolutional Neural Network) pipelines that overcomes GPU memory limitations faced by users with machine learning frameworks like PyTorch.</p> <p>4)<span> </span>Development of a domain/pattern-specific modeling approach for optimizing convolutional neural networks on GPUs, achieving higher performance than state-of-the-art libraries and auto-tuning frameworks.</p> <p>5)<span> </span>Implementation of an efficient implementation of convolutional neural networks that exploits feature map sparsity.</p> <p>6)<span> </span>Development of a high-performance kernel for Tucker-format convolutions and an analytical performance model to guide the selection of execution parameters.</p> <p>&nbsp;</p><br> <p>  Last Modified: 06/12/2024<br> Modified by: Ponnuswamy&nbsp;Sadayappan</p></div> <div class="porSideCol" ></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[  GPUs (Graphical Processing Units) can achieve very high performance, but the development of efficient GPU programs is very challenging, requiring specialized knowledge possessed by very few programmers. The goal of this project was to develop tools and techniques that could enable users to achieve high productivity in developing high-performance GPU applications.   The main outcomes of the project, through publicly released software and research publications, were:   1) Development of an efficient mixed-mode data representation for sparse tensors on GPUs and its use in implementing efficient sparse tensor decomposition.   2)  Development a novel high-performance implementation of parallel NMF (Non-negative Matrix Factorization) on GPUs, based on the HALS (Hierarchical Alternating Least Squares) scheme via algorithmic transformations to enhance data locality.   3) Development of a segmented fused-tiled strategy for implementing CNN (Convolutional Neural Network) pipelines that overcomes GPU memory limitations faced by users with machine learning frameworks like PyTorch.   4) Development of a domain/pattern-specific modeling approach for optimizing convolutional neural networks on GPUs, achieving higher performance than state-of-the-art libraries and auto-tuning frameworks.   5) Implementation of an efficient implementation of convolutional neural networks that exploits feature map sparsity.   6) Development of a high-performance kernel for Tucker-format convolutions and an analytical performance model to guide the selection of execution parameters.        Last Modified: 06/12/2024       Submitted by: PonnuswamySadayappan]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
