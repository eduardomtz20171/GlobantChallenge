<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle><![CDATA[EAGER: Annotating and extracting detailed syntactic information from a 1.1-billion-word corpus]]></AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/15/2020</AwardEffectiveDate>
<AwardExpirationDate>01/31/2024</AwardExpirationDate>
<AwardTotalIntnAmount>298386.00</AwardTotalIntnAmount>
<AwardAmount>298386</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>04040000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>BCS</Abbreviation>
<LongName>Division Of Behavioral and Cognitive Sci</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jorge Valdes Kroff</SignBlockName>
<PO_EMAI>jvaldesk@nsf.gov</PO_EMAI>
<PO_PHON>7032927920</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Over the past decade, very large text corpora of English have become available to researchers that turn out to be of considerable value for the language sciences. Even more recently, methods in natural language processing have advanced to a point where we can begin to imagine conducting linguistic research using automatically parsed and uncorrected corpora of the sort that has so far been conducted using human-corrected corpora. It is this new situation that the PIs wish to exploit by producing an automatically parsed billion-plus word corpus of early modern English based on the digitized Early English Books Online (EEBO) corpus that has recently been completed and made accessible to research. The aim is to create an automatically parsed database with a level of accuracy suitable for both linguistic and computational research, using the recently developed cutting-edge methods in natural language processing.  The resulting resource will make possible investigations hitherto impossible; specifically, the information contained in a parsed version of EEBO will permit researchers to investigate frequency effects not just of words, but of larger grammatical units (phrases and clauses). In addition to their inherent linguistic interest, the results of such investigations may lead to the discovery of more sophisticated meaning-based properties and how these vary, which should be of value for research in natural language processing. The PIs have made progress on this goal, having created a first automatically parsed version of the EEBO corpus and begun to assess its accuracy. Some features like the syntax of clausal negation are already within our reach, but for many other structures, it remains to be determined how accurate retrieval with large-scale methods can be.  &lt;br/&gt;&lt;br/&gt;Since EEBO is more than 300 times larger than even the largest individual human-corrected corpora, it is expected that a more accurately parsed version of it than the one now available will begin to allow researchers to study phenomena that are only sporadically attested in existing English corpora, to zero in on the very beginnings and ends of historical changes, to investigate many different types of frequency effects (including the novel ones already mentioned) with an accuracy and reliability not hitherto possible, and to rigorously evaluate mathematical models of language change.  Because the stage of English covered by EEBO (1500-1700) is already recognizably the modern language, a parsed version of EEBO can to some extent stand proxy for a corpus of Present-Day English for research in the language sciences.  As a result, it should be useful as a training and testing ground for applications in computational linguistics including part-of-speech tagging, parsing, named entity recognition, and eventually lemmatization, sense disambiguation, and others. EEBOâ€™s great genre variety and variable orthography and its moderate distance from Present-Day English will also make a parsed version of it a natural candidate for assessing and improving the robustness of these applications and for developing novel parser evaluation metrics that can serve as linguistically informed benchmarks for computational linguistics.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>07/27/2020</MinAmdLetterDate>
<MaxAmdLetterDate>07/27/2020</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2026850</AwardID>
<Investigator>
<FirstName>Seth</FirstName>
<LastName>Kulick</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Seth Kulick</PI_FULL_NAME>
<EmailAddress><![CDATA[skulick@seas.upenn.edu]]></EmailAddress>
<NSF_ID>000169661</NSF_ID>
<StartDate>07/27/2020</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Beatrice</FirstName>
<LastName>Santorini</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Beatrice Santorini</PI_FULL_NAME>
<EmailAddress><![CDATA[beatrice@sas.upenn.edu]]></EmailAddress>
<NSF_ID>000597506</NSF_ID>
<StartDate>07/27/2020</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name><![CDATA[University of Pennsylvania]]></Name>
<CityName>PHILADELPHIA</CityName>
<ZipCode>191046205</ZipCode>
<PhoneNumber>2158987293</PhoneNumber>
<StreetAddress><![CDATA[3451 WALNUT ST STE 440A]]></StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<StateCode>PA</StateCode>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>PA03</CONGRESS_DISTRICT_ORG>
<ORG_UEI_NUM>GM1XX56LEP58</ORG_UEI_NUM>
<ORG_LGL_BUS_NAME>TRUSTEES OF THE UNIVERSITY OF PENNSYLVANIA, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_UEI_NUM>GM1XX56LEP58</ORG_PRNT_UEI_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Pennsylvania]]></Name>
<CityName>Philadelphia</CityName>
<StateCode>PA</StateCode>
<ZipCode>191046228</ZipCode>
<StreetAddress><![CDATA[Department of Linguistics]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>PA03</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>131100</Code>
<Text>Linguistics</Text>
</ProgramElement>
<ProgramReference>
<Code>1311</Code>
<Text>LINGUISTICS</Text>
</ProgramReference>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<ProgramReference>
<Code>SMET</Code>
<Text>SCIENCE, MATH, ENG &amp; TECH EDUCATION</Text>
</ProgramReference>
<Appropriation>
<Code>0120</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Fund>
<Code>01002021DB</Code>
<Name><![CDATA[NSF RESEARCH & RELATED ACTIVIT]]></Name>
<FUND_SYMB_ID>040100</FUND_SYMB_ID>
</Fund>
<FUND_OBLG>2020~298386</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The overriding goal of this project is to determine whether<br />state-of-the-art parsers are accurate enough to be used for linguistic research on language change.&nbsp; Corpora of syntactically-annotated sentences (&ldquo;treebanks&rdquo;) have revolutionized the study of language change by providing the data needed for large-scale and precise investigations, combining grammatical analysis with the statistical tracking of changes over time.&nbsp; However, the slow speed of manual annotation limits such treebanks to 1-2 million words, which is too small for research questions that have emerged, such as deciding between alternative models<br />of change (notably, whether a change is better modeled as evolutionary competition or random drift).&nbsp; Although corpora exist that are large enough (over 1 billion words), they lack the needed syntactic annotation.&nbsp; Our research therefore investigated the question - can these large corpora be automatically annotated with sufficient accuracy to be used for linguistic research?<br /><br />Our particular focus in syntactic change is the rise of auxiliary "do" in various sentence types (compare Middle and Early Modern English "I know not the answer", "(When) arrived they?" to Modern English "I do not know the answer", "(When) did they arrive?").&nbsp; The large corpus for which we provided automatic syntactic annotation is Early English Books Online (EEBO), a 1.5 billion-word corpus of all books published in English from 1475 to 1700.&nbsp; To automatically parse EEBO, we trained a parser using the Penn Parsed Corpus of Early Modern English (PPCEME), a<br />1.8 million word manually constructed corpus from the same time period.<br /><br />The standard measure of evaluating accuracy is not sufficient for our purposes, since it lacks the precise information needed about the parsing accuracy on the particular structures that will be the object of the statistical models.&nbsp; We therefore developed a new evaluation metric that queried the parser output specifically for these structures, resulting in individual scores for each such structure.&nbsp; While we do not have gold-standard parses of EEBO for comparison to the parser&rsquo;s output, EEBO does contain material for which the PPCEME has gold-standard parses, and we took advantage of this overlap material to apply our new<br />metric to EEBO.&nbsp; We further experimented with a wide variety of parser models and evaluated all the results in a consistent way.&nbsp; Particularly important was training a Large Language Model on EEBO itself, for integration into the parser. We expect the results of these experiments to be of interest to both linguists and computational linguists.<br /><br />The project has three main outcomes.&nbsp; First, the query-based evaluation on the evaluation sections of PPCEME showed that the scores vary widely depending on the structure.&nbsp; For instance, questions like "Know you the answer?" or negative imperatives like "Fear not the enemy" are more difficult for the parser (in contrast to, say, ordinary negative declaratives like "I know not the answer").</p> <p>Second, structure-specific scores on the EEBO version of the overlap material consistently fall about 1 to 1.5 points behind those on the gold-standard material, a relatively modest loss.&nbsp; However, this is very dependent on the segmentation of sentences in EEBO.&nbsp; Sentences in PPCEME are manually segmented based on linguistic criteria, whereas EEBO's<br />much larger size forced us to automatically segment sentences based solely on punctuation.&nbsp; For instance, the orthographic sentence "They entered the room, and they greeted their friends." counts as two sentence tokens in PPCEME because it contains two main verbs, but as a single sentence token in EEBO because it contains only one period.&nbsp; The modest loss in<br />performance just mentioned is obtained when the sentence boundaries in the EEBO version of the overlap are adjusted to match those in the PPCEME version.&nbsp; When the automatic segmentation is used, scores drop appreciably - up to 7 points in some cases.&nbsp; Therefore, the answer to the main question of this project is that if the sentence segmentation problem is fixed, then the automatically parsed version of EEBO can be used for linguistic research, but with caution, depending on the structures, and factoring in the error rates resulting from our evaluation.<br /><br />Finally, we will make publicly available a computational infrastructure enabling us and other researchers to use EEBO to investigate other syntactic phenomena, to estimate the accuracy of automatic parses of these phenomena compared to some gold standard, and to decide whether that accuracy is sufficient for the purpose at hand.&nbsp; This framework can also be extended to other cases of a training corpus and a large collection of text to annotate for linguistic research.</p><br> <p>  Last Modified: 08/08/2024<br> Modified by: Beatrice&nbsp;Santorini</p></div> <div class="porSideCol" ></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[  The overriding goal of this project is to determine whether state-of-the-art parsers are accurate enough to be used for linguistic research on language change. Corpora of syntactically-annotated sentences (treebanks) have revolutionized the study of language change by providing the data needed for large-scale and precise investigations, combining grammatical analysis with the statistical tracking of changes over time. However, the slow speed of manual annotation limits such treebanks to 1-2 million words, which is too small for research questions that have emerged, such as deciding between alternative models of change (notably, whether a change is better modeled as evolutionary competition or random drift). Although corpora exist that are large enough (over 1 billion words), they lack the needed syntactic annotation. Our research therefore investigated the question - can these large corpora be automatically annotated with sufficient accuracy to be used for linguistic research?  Our particular focus in syntactic change is the rise of auxiliary "do" in various sentence types (compare Middle and Early Modern English "I know not the answer", "(When) arrived they?" to Modern English "I do not know the answer", "(When) did they arrive?"). The large corpus for which we provided automatic syntactic annotation is Early English Books Online (EEBO), a 1.5 billion-word corpus of all books published in English from 1475 to 1700. To automatically parse EEBO, we trained a parser using the Penn Parsed Corpus of Early Modern English (PPCEME), a 1.8 million word manually constructed corpus from the same time period.  The standard measure of evaluating accuracy is not sufficient for our purposes, since it lacks the precise information needed about the parsing accuracy on the particular structures that will be the object of the statistical models. We therefore developed a new evaluation metric that queried the parser output specifically for these structures, resulting in individual scores for each such structure. While we do not have gold-standard parses of EEBO for comparison to the parsers output, EEBO does contain material for which the PPCEME has gold-standard parses, and we took advantage of this overlap material to apply our new metric to EEBO. We further experimented with a wide variety of parser models and evaluated all the results in a consistent way. Particularly important was training a Large Language Model on EEBO itself, for integration into the parser. We expect the results of these experiments to be of interest to both linguists and computational linguists.  The project has three main outcomes. First, the query-based evaluation on the evaluation sections of PPCEME showed that the scores vary widely depending on the structure. For instance, questions like "Know you the answer?" or negative imperatives like "Fear not the enemy" are more difficult for the parser (in contrast to, say, ordinary negative declaratives like "I know not the answer").   Second, structure-specific scores on the EEBO version of the overlap material consistently fall about 1 to 1.5 points behind those on the gold-standard material, a relatively modest loss. However, this is very dependent on the segmentation of sentences in EEBO. Sentences in PPCEME are manually segmented based on linguistic criteria, whereas EEBO's much larger size forced us to automatically segment sentences based solely on punctuation. For instance, the orthographic sentence "They entered the room, and they greeted their friends." counts as two sentence tokens in PPCEME because it contains two main verbs, but as a single sentence token in EEBO because it contains only one period. The modest loss in performance just mentioned is obtained when the sentence boundaries in the EEBO version of the overlap are adjusted to match those in the PPCEME version. When the automatic segmentation is used, scores drop appreciably - up to 7 points in some cases. Therefore, the answer to the main question of this project is that if the sentence segmentation problem is fixed, then the automatically parsed version of EEBO can be used for linguistic research, but with caution, depending on the structures, and factoring in the error rates resulting from our evaluation.  Finally, we will make publicly available a computational infrastructure enabling us and other researchers to use EEBO to investigate other syntactic phenomena, to estimate the accuracy of automatic parses of these phenomena compared to some gold standard, and to decide whether that accuracy is sufficient for the purpose at hand. This framework can also be extended to other cases of a training corpus and a large collection of text to annotate for linguistic research.     Last Modified: 08/08/2024       Submitted by: BeatriceSantorini]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
