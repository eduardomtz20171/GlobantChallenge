<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle><![CDATA[EAGER: SaTC-EDU: Multi-Level Attack and Defense Simulation Environment for Artificial Intelligence Education and Research]]></AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2020</AwardEffectiveDate>
<AwardExpirationDate>07/31/2023</AwardExpirationDate>
<AwardTotalIntnAmount>300000.00</AwardTotalIntnAmount>
<AwardAmount>300000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>11010000</Code>
<Directorate>
<Abbreviation>EDU</Abbreviation>
<LongName>Directorate for STEM Education</LongName>
</Directorate>
<Division>
<Abbreviation>DGE</Abbreviation>
<LongName>Division Of Graduate Education</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Li Yang</SignBlockName>
<PO_EMAI>liyang@nsf.gov</PO_EMAI>
<PO_PHON>7032922677</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Artificial intelligence (AI) techniques, particularly machine learning (ML), are increasingly integrated into safety- and security-critical applications such as autonomous vehicles and malware detection. However, research has shown AI techniques can be vulnerable to cyber-attacks such as adversarial perturbation and data poisoning, potentially leading to catastrophic outcomes when decisions made by AI systems are manipulated. Despite significant research efforts in this area, the research community has disproportionately focused on only a few domains, such as image recognition, and a few simple adversarial setups. Meanwhile more security-critical domains, such as malware detection, and a variety of adversarial models that more fully represent the real-world, have been ignored. Furthermore, it is difficult to compare, contrast, and characterize the different approaches to developing robust AI systems because of the fragmented nature of efforts in this area. This also creates challenges for education efforts in AI and cybersecurity. This project aims to address these urgent issues with synergistic efforts in AI, cybersecurity, and education that will produce significant research and societal impacts. First, the results of the project will promote public awareness of the issues and research around the robustness AI via the dissemination of tools and materials. Second, the project will democratize research progress in robust AI to application domains that are currently underserved, such as malware detection. Third, the project represents a concrete step towards fostering a workforce with skills in building robust and secure AI systems. The platform developed by this project will be integrated into undergraduate and graduate courses at the University of California Irvine and made publicly available to researchers and educators. &lt;br/&gt;The specific aim of the project is to address issues of research fragmentation in robust and secure AI. The project team will develop a new platform, called Maestro, to simulate adversarial machine learning tasks, covering a variety of adversarial capabilities (access to gradients, model weights, predictions, etc.) for both attacks and defenses, under a formal access-control framework. The  Maestro platform  will make it easier to implement, compare, and develop novel adversarial ML algorithms, settings, and applications that have not been sufficiently explored, including backdoor exploitation of natural language processing (NLP), stealthy adversarial malware generation, and security analysis of program embedding. The architecture of Maestro not only provides a useful framework to structure pedagogical materials in AI and cybersecurity, but also will be used to build course materials using active learning and gamification strategies.  The latter will engage students while teaching them essential concepts about building reliable and robust AI systems.&lt;br/&gt;This project is supported by a special initiative of the Secure and Trustworthy Cyberspace (SaTC) program to foster new, previously unexplored, collaborations between the fields of cybersecurity, artificial intelligence, and education. The SaTC program aligns with the Federal Cybersecurity Research and Development Strategic Plan and the National Privacy Research Strategy to protect and preserve the growing social and economic benefits of cyber systems while ensuring security and privacy.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>07/27/2020</MinAmdLetterDate>
<MaxAmdLetterDate>07/27/2020</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.076</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2039634</AwardID>
<Investigator>
<FirstName>Sergio</FirstName>
<LastName>Gago Masague</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Sergio Gago Masague</PI_FULL_NAME>
<EmailAddress><![CDATA[sgagomas@uci.edu]]></EmailAddress>
<NSF_ID>000718590</NSF_ID>
<StartDate>07/27/2020</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Sameer</FirstName>
<LastName>Singh</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Sameer Singh</PI_FULL_NAME>
<EmailAddress><![CDATA[sameer@uci.edu]]></EmailAddress>
<NSF_ID>000727594</NSF_ID>
<StartDate>07/27/2020</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Zhou</FirstName>
<LastName>Li</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Zhou Li</PI_FULL_NAME>
<EmailAddress><![CDATA[zhou.li@uci.edu]]></EmailAddress>
<NSF_ID>000790157</NSF_ID>
<StartDate>07/27/2020</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name><![CDATA[University of California-Irvine]]></Name>
<CityName>IRVINE</CityName>
<ZipCode>926970001</ZipCode>
<PhoneNumber>9498247295</PhoneNumber>
<StreetAddress><![CDATA[160 ALDRICH HALL]]></StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>47</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA47</CONGRESS_DISTRICT_ORG>
<ORG_UEI_NUM>MJC5FCYQTPE6</ORG_UEI_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CALIFORNIA IRVINE</ORG_LGL_BUS_NAME>
<ORG_PRNT_UEI_NUM>MJC5FCYQTPE6</ORG_PRNT_UEI_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[The Regents of the University of California]]></Name>
<CityName>Irvine</CityName>
<StateCode>CA</StateCode>
<ZipCode>926972700</ZipCode>
<StreetAddress><![CDATA[3227 Engineering Hall]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>47</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA47</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>806000</Code>
<Text>Secure &amp;Trustworthy Cyberspace</Text>
</ProgramElement>
<ProgramReference>
<Code>025Z</Code>
<Text>SaTC: Secure and Trustworthy Cyberspace</Text>
</ProgramReference>
<ProgramReference>
<Code>093Z</Code>
<Text>AI Education/Workforce Develop</Text>
</ProgramReference>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<ProgramReference>
<Code>9178</Code>
<Text>UNDERGRADUATE EDUCATION</Text>
</ProgramReference>
<ProgramReference>
<Code>9179</Code>
<Text>GRADUATE INVOLVEMENT</Text>
</ProgramReference>
<ProgramReference>
<Code>SMET</Code>
<Text>SCIENCE, MATH, ENG &amp; TECH EDUCATION</Text>
</ProgramReference>
<Appropriation>
<Code>0120</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0420</Code>
<Name>NSF Education &amp; Human Resource</Name>
<APP_SYMB_ID>040106</APP_SYMB_ID>
</Appropriation>
<Fund>
<Code>01002021DB</Code>
<Name><![CDATA[NSF RESEARCH & RELATED ACTIVIT]]></Name>
<FUND_SYMB_ID>040100</FUND_SYMB_ID>
</Fund>
<Fund>
<Code>04002021DB</Code>
<Name><![CDATA[NSF Education & Human Resource]]></Name>
<FUND_SYMB_ID>040106</FUND_SYMB_ID>
</Fund>
<FUND_OBLG>2020~300000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span id="docs-internal-guid-76c7d673-7fff-012a-a590-07eefe0f612b"> </span></p> <p dir="ltr"><span>Artificial intelligence (AI) techniques, particularly machine learning (ML), are increasingly integrated into safety- and security-critical applications such as autonomous vehicles. However, research has shown AI techniques can be vulnerable to cyber-attacks such as adversarial perturbation and data poisoning, potentially leading to catastrophic outcomes when decisions made by AI systems are manipulated. This project aims to promote robust AI with synergistic efforts in AI, cybersecurity, and education.&nbsp;</span></p> <p>&nbsp;</p> <p dir="ltr"><span>The major activity of this project is to develop a new educational platform named Maestro, which provides goal-based scenarios related to robust AI where college students are exposed to&nbsp; assignments in a competitive programming environment. The platform consists of code templates and essential libraries to let the students implement attack homework (e.g., Fast Gradient Sign Method), defense homework (e.g., adversarial training), attack project (the student can implement any attack method against a selected defense method), defense project (the student can implement any defense method against a selected attack method) and war phase (the student plays attack and defense in turns). We also implemented a leaderboard module to rank students' submissions based on metrics like accuracy and efficiency, and a module that integrated Maestro with other education platforms like Gradescope. The platform is expected to greatly help instructors who want to teach robust AI with hands-on labs/homeworks. We released the public GitHub repo (https://github.com/C0ldstudy/maestro-student) with documentation, and we are promoting this platform in various channels.</span></p> <p>&nbsp;</p> <p dir="ltr"><span>We have taught a pilot course 'COMPSCI 175: Projects in AI' at UC Irvine, which was offered at Winter and Spring quarters 2022 to undergraduate students, with the help of Maestro. This is a project-based course, and the students worked on robust AI projects individually and in teams, by developing attack and defense methods in the computer vision domain. In total 147 students took the class. This course provides an ideal opportunity for us to evaluate Maestro, and understand the opportunities and challenges in teaching robust AI at colleges. At the conclusion of the courses, students participated in the survey anonymously regarding the gamification, experience and challenge categories. We carefully analyzed students survey and drew multiple findings: 1) The students are quite interested in this topic and are becoming quite aware of the security issues of AI; 2) Maestro has successfully motivated students to learn robust AI, with its gamified design;&nbsp; 3) the leaderboard module receives very positive feedback;&nbsp; 4) the amount of materials for students to learn robust AI are still a lot for a 10-week course and some students feel the topic is challenging.&nbsp;</span></p> <p>&nbsp;</p> <p dir="ltr"><span>We published a paper at EAAI'23 and a poster at SIGCSE'23 to elaborate the technical details of Maestro and our study of students' feedback. We have given tutorials about explaining ML models and adversarial attacks against AI interpretability in AI conferences/workshops including EMNLP 2020, NeurIPS 2020, AAAI 2021, and CPAIOR 2021.</span></p> <p>&nbsp;</p> <p dir="ltr"><span>Besides the education activities, we also conduct new research activities related to robust AI, focusing on application domains that are currently underserved. First, we thoroughly assessed the confidentiality and integrity of AI models used by Autonomous Driving, including Multi-Sensor Fusion (MSF) model and Point Cloud Semantic Segmentation (PCSS) model. The results show they are all vulnerable under adaptive attackers. The works were published in ACSAC'22 and DSN'23. Second, we studied the security of AI models used by face verification systems (FVS), and we found an attacker can impersonate a victim at a high success rate. The study was published at ACSAC'21. Third, we studied the confidentiality issues of Deep Neural Network (DNN) models that are executed on a shared Field Programmable Gate Array (FPGA) board, and found an attacker can learn the model structure used by the other victim model at high accuracy, by programming power trojans. The paper was published in an IEEE journal TIFS in 2021. Fourth, we studied how the predictions from Natural language processing (NLP) models can be manipulated with small, concealed changes to the training data, and published our study at NAACL'21. For all the above studies, we further provide recommendations about how the issues can be mitigated, and we believe through our studies, more attention from academia and industry will be directed to develop new mechanisms to protect AI models in various settings.</span></p> <div><span><br /></span></div> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>  Last Modified: 12/06/2023<br> Modified by: Zhou&nbsp;Li</p></div> <div class="porSideCol" ><div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)          </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2023/2039634/2039634_10689769_1701895968455_maestro--rgov-214x142.bmp" original="/por/images/Reports/POR/2023/2039634/2039634_10689769_1701895968455_maestro--rgov-800width.bmp" title="Maestro Platform Workflow"><img src="/por/images/Reports/POR/2023/2039634/2039634_10689769_1701895968455_maestro--rgov-66x44.bmp" alt="Maestro Platform Workflow"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Maestro Platform Workflow</div> <div class="imageCredit">UCI Maestro Team</div> <div class="imagePermisssions">Public Domain</div> <div class="imageSubmitted">Zhou&nbsp;Li <div class="imageTitle">Maestro Platform Workflow</div> </div> </li><li> <a href="/por/images/Reports/POR/2023/2039634/2039634_10689769_1701896055340_survey--rgov-214x142.bmp" original="/por/images/Reports/POR/2023/2039634/2039634_10689769_1701896055340_survey--rgov-800width.bmp" title="UCI Student Course Survey"><img src="/por/images/Reports/POR/2023/2039634/2039634_10689769_1701896055340_survey--rgov-66x44.bmp" alt="UCI Student Course Survey"></a> <div class="imageCaptionContainer"> <div class="imageCaption">UCI Student Course Survey</div> <div class="imageCredit">UCI Maestro Team</div> <div class="imagePermisssions">Public Domain</div> <div class="imageSubmitted">Zhou&nbsp;Li <div class="imageTitle">UCI Student Course Survey</div> </div> </li><li> <a href="/por/images/Reports/POR/2023/2039634/2039634_10689769_1701896013475_leaderboard--rgov-214x142.png" original="/por/images/Reports/POR/2023/2039634/2039634_10689769_1701896013475_leaderboard--rgov-800width.png" title="Maestro Leaderboard Example"><img src="/por/images/Reports/POR/2023/2039634/2039634_10689769_1701896013475_leaderboard--rgov-66x44.png" alt="Maestro Leaderboard Example"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Maestro Leaderboard Example</div> <div class="imageCredit">UCI Maestro Team</div> <div class="imagePermisssions">Public Domain</div> <div class="imageSubmitted">Zhou&nbsp;Li <div class="imageTitle">Maestro Leaderboard Example</div> </div> </li></ul> </div> </div></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[      Artificial intelligence (AI) techniques, particularly machine learning (ML), are increasingly integrated into safety- and security-critical applications such as autonomous vehicles. However, research has shown AI techniques can be vulnerable to cyber-attacks such as adversarial perturbation and data poisoning, potentially leading to catastrophic outcomes when decisions made by AI systems are manipulated. This project aims to promote robust AI with synergistic efforts in AI, cybersecurity, and education.      The major activity of this project is to develop a new educational platform named Maestro, which provides goal-based scenarios related to robust AI where college students are exposed to assignments in a competitive programming environment. The platform consists of code templates and essential libraries to let the students implement attack homework (e.g., Fast Gradient Sign Method), defense homework (e.g., adversarial training), attack project (the student can implement any attack method against a selected defense method), defense project (the student can implement any defense method against a selected attack method) and war phase (the student plays attack and defense in turns). We also implemented a leaderboard module to rank students' submissions based on metrics like accuracy and efficiency, and a module that integrated Maestro with other education platforms like Gradescope. The platform is expected to greatly help instructors who want to teach robust AI with hands-on labs/homeworks. We released the public GitHub repo (https://github.com/C0ldstudy/maestro-student) with documentation, and we are promoting this platform in various channels.      We have taught a pilot course 'COMPSCI 175: Projects in AI' at UC Irvine, which was offered at Winter and Spring quarters 2022 to undergraduate students, with the help of Maestro. This is a project-based course, and the students worked on robust AI projects individually and in teams, by developing attack and defense methods in the computer vision domain. In total 147 students took the class. This course provides an ideal opportunity for us to evaluate Maestro, and understand the opportunities and challenges in teaching robust AI at colleges. At the conclusion of the courses, students participated in the survey anonymously regarding the gamification, experience and challenge categories. We carefully analyzed students survey and drew multiple findings: 1) The students are quite interested in this topic and are becoming quite aware of the security issues of AI; 2) Maestro has successfully motivated students to learn robust AI, with its gamified design; 3) the leaderboard module receives very positive feedback; 4) the amount of materials for students to learn robust AI are still a lot for a 10-week course and some students feel the topic is challenging.      We published a paper at EAAI'23 and a poster at SIGCSE'23 to elaborate the technical details of Maestro and our study of students' feedback. We have given tutorials about explaining ML models and adversarial attacks against AI interpretability in AI conferences/workshops including EMNLP 2020, NeurIPS 2020, AAAI 2021, and CPAIOR 2021.      Besides the education activities, we also conduct new research activities related to robust AI, focusing on application domains that are currently underserved. First, we thoroughly assessed the confidentiality and integrity of AI models used by Autonomous Driving, including Multi-Sensor Fusion (MSF) model and Point Cloud Semantic Segmentation (PCSS) model. The results show they are all vulnerable under adaptive attackers. The works were published in ACSAC'22 and DSN'23. Second, we studied the security of AI models used by face verification systems (FVS), and we found an attacker can impersonate a victim at a high success rate. The study was published at ACSAC'21. Third, we studied the confidentiality issues of Deep Neural Network (DNN) models that are executed on a shared Field Programmable Gate Array (FPGA) board, and found an attacker can learn the model structure used by the other victim model at high accuracy, by programming power trojans. The paper was published in an IEEE journal TIFS in 2021. Fourth, we studied how the predictions from Natural language processing (NLP) models can be manipulated with small, concealed changes to the training data, and published our study at NAACL'21. For all the above studies, we further provide recommendations about how the issues can be mitigated, and we believe through our studies, more attention from academia and industry will be directed to develop new mechanisms to protect AI models in various settings.             Last Modified: 12/06/2023       Submitted by: ZhouLi]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
