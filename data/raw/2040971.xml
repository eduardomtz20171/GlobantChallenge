<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle><![CDATA[III: Towards Causal Fair Decision-making]]></AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>05/15/2021</AwardEffectiveDate>
<AwardExpirationDate>04/30/2024</AwardExpirationDate>
<AwardTotalIntnAmount>739500.00</AwardTotalIntnAmount>
<AwardAmount>739500</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Sylvia Spengler</SignBlockName>
<PO_EMAI>sspengle@nsf.gov</PO_EMAI>
<PO_PHON>7032927347</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Artificial Intelligence (AI) plays an increasingly prominent role in modern society because decisions that were once made by humans are now being delegated to automated systems. These systems are currently in charge of deciding bank loans, criminals' incarceration, and the hiring of new employees, and it is not difficult to envision a future where AI will underpin most of the society's decision-making infrastructure. Despite the high stakes entailed by this task, there is still almost no understanding of some basic properties of such systems, including issues of fairness and transparency. For instance, there is a proliferation of criteria and methods trying to account for unfairness in decision-making, but choosing a metric that the AI system must adhere to be deemed fair remains an elusive, almost daunting task. Also, these metrics are almost invariably carried out in an arbitrary fashion, without much justification or rationale.  In this project, we will develop the mathematical foundations for (1) assisting data scientists analyzing the existence and (possibly) the `magnitude' of unfairness in an already deployed decision-system and (2) guiding system's designers in the process of selecting a fairness criterion in their to-be-deployed system while ascertaining an established level of fairness and accuracy. &lt;br/&gt;&lt;br/&gt;This proposal aims to make both foundational and methodological contributions towards the goal of causal fair decision-making. At a foundational level, we build on causality theory to elicit the principles necessary to formally understand the problem of fairness, which is intertwined with the true causal mechanisms underlying the data.  In particular, we study various measures of fairness available in the literature and their detection and explanatory power relative to the unobserved causal mechanisms.  On the methodological side,  we aim to bridge the gap between causal analysis and scalable machine learning methods through novel ideas for efficient estimation, prediction, and optimization under causal fairness measures. This includes weighted empirical risk minimization methods for estimating causal fairness measures from offline data, active learning and exploration techniques for hybrid (offline and online) learning, robust optimization methods to handle model misspecification, and reinforcement learning techniques for understanding long-term impact of fair/unfair policies.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>05/06/2021</MinAmdLetterDate>
<MaxAmdLetterDate>11/22/2022</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2040971</AwardID>
<Investigator>
<FirstName>Elias</FirstName>
<LastName>Bareinboim</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Elias Bareinboim</PI_FULL_NAME>
<EmailAddress><![CDATA[eb@cs.columbia.edu]]></EmailAddress>
<NSF_ID>000717905</NSF_ID>
<StartDate>05/06/2021</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Daniel</FirstName>
<LastName>Hsu</LastName>
<PI_MID_INIT>J</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Daniel J Hsu</PI_FULL_NAME>
<EmailAddress><![CDATA[djhsu@cs.columbia.edu]]></EmailAddress>
<NSF_ID>000672405</NSF_ID>
<StartDate>05/06/2021</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Augustin</FirstName>
<LastName>Chaintreau</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Augustin Chaintreau</PI_FULL_NAME>
<EmailAddress><![CDATA[ac3318@columbia.edu]]></EmailAddress>
<NSF_ID>000575992</NSF_ID>
<StartDate>05/06/2021</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Shipra</FirstName>
<LastName>Agrawal</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Shipra Agrawal</PI_FULL_NAME>
<EmailAddress><![CDATA[sa3305@columbia.edu]]></EmailAddress>
<NSF_ID>000704864</NSF_ID>
<StartDate>05/06/2021</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Desmond</FirstName>
<LastName>Patton</LastName>
<PI_MID_INIT>U</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Desmond U Patton</PI_FULL_NAME>
<EmailAddress><![CDATA[dupatton@upenn.edu]]></EmailAddress>
<NSF_ID>000696611</NSF_ID>
<StartDate>05/06/2021</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name><![CDATA[Columbia University]]></Name>
<CityName>NEW YORK</CityName>
<ZipCode>100277922</ZipCode>
<PhoneNumber>2128546851</PhoneNumber>
<StreetAddress><![CDATA[615 W 131ST ST]]></StreetAddress>
<StreetAddress2><![CDATA[MC 8741]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY13</CONGRESS_DISTRICT_ORG>
<ORG_UEI_NUM>F4N1QNPB95M4</ORG_UEI_NUM>
<ORG_LGL_BUS_NAME>THE TRUSTEES OF COLUMBIA UNIVERSITY IN THE CITY OF NEW YORK</ORG_LGL_BUS_NAME>
<ORG_PRNT_UEI_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[Columbia University]]></Name>
<CityName>New York</CityName>
<StateCode>NY</StateCode>
<ZipCode>100276601</ZipCode>
<StreetAddress><![CDATA[550 West 120th Street]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY13</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>736400</Code>
<Text>Info Integration &amp; Informatics</Text>
</ProgramElement>
<ProgramReference>
<Code>0757</Code>
<Text>COOP PLAN OPs &amp; SERVICES</Text>
</ProgramReference>
<ProgramReference>
<Code>075Z</Code>
<Text>Artificial Intelligence (AI)</Text>
</ProgramReference>
<ProgramReference>
<Code>7364</Code>
<Text>INFO INTEGRATION &amp; INFORMATICS</Text>
</ProgramReference>
<Appropriation>
<Code>0121</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Fund>
<Code>01002122DB</Code>
<Name><![CDATA[NSF RESEARCH & RELATED ACTIVIT]]></Name>
<FUND_SYMB_ID>040100</FUND_SYMB_ID>
</Fund>
<FUND_OBLG>2021~739500</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span id="docs-internal-guid-f9b5c4d8-7fff-fac0-46a9-c0b1774a1847"> </span></p> <p dir="ltr"><span>This project aimed to create new methods and frameworks to ensure that decision-making systems, increasingly controlled by AI, are fair and equitable. Our work focused on developing tools to analyze and correct discrimination based on sensitive attributes like gender, race, or religion while ensuring that decisions lead to the best possible outcomes for individuals.</span></p> <p dir="ltr"><span>One of the major outcomes was the development of a new framework to understand the causes of discrimination in automated systems. This framework allows us to better identify when and how certain groups may be unfairly treated and design systems that correct these biases. In particular, we introduced measures of "benefit fairness," ensuring that decisions consider the benefit an individual would receive from a given outcome compared to others while accounting for protected characteristics.</span></p> <p dir="ltr"><span>We also created practical statistical methods for estimating these fairness measures, even when data is limited. These tools are designed to handle real-world settings where AI must make decisions based on a combination of observational (passive) and experimental&nbsp;(active) data. We also tackled the problem of learning the causal relationships in complex systems when data comes from multiple, heterogeneous environments. For example, learning causal relationships from gene sequencing data collected in different labs or understanding neural data from different species. We developed new algorithms that allow researchers to combine data from multiple domains, leading to more accurate and reliable insights. This also allows us to use the fairness tools in settings without apriori causal knowledge.</span></p> <p dir="ltr"><span>Overall, this project has contributed to both the understanding of fairness in automated decision-making and the development of methods to ensure that these systems are not only efficient but also just. The broader impact of this work extends across fields such as healthcare, criminal justice, and public policy, where fair outcomes are critical. By creating tools that help detect and correct unfairness, we aim to support more equitable AI systems that benefit all individuals fairly.</span></p><br> <p>  Last Modified: 09/12/2024<br> Modified by: Elias&nbsp;Bareinboim</p></div> <div class="porSideCol" ></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[      This project aimed to create new methods and frameworks to ensure that decision-making systems, increasingly controlled by AI, are fair and equitable. Our work focused on developing tools to analyze and correct discrimination based on sensitive attributes like gender, race, or religion while ensuring that decisions lead to the best possible outcomes for individuals.   One of the major outcomes was the development of a new framework to understand the causes of discrimination in automated systems. This framework allows us to better identify when and how certain groups may be unfairly treated and design systems that correct these biases. In particular, we introduced measures of "benefit fairness," ensuring that decisions consider the benefit an individual would receive from a given outcome compared to others while accounting for protected characteristics.   We also created practical statistical methods for estimating these fairness measures, even when data is limited. These tools are designed to handle real-world settings where AI must make decisions based on a combination of observational (passive) and experimental(active) data. We also tackled the problem of learning the causal relationships in complex systems when data comes from multiple, heterogeneous environments. For example, learning causal relationships from gene sequencing data collected in different labs or understanding neural data from different species. We developed new algorithms that allow researchers to combine data from multiple domains, leading to more accurate and reliable insights. This also allows us to use the fairness tools in settings without apriori causal knowledge.   Overall, this project has contributed to both the understanding of fairness in automated decision-making and the development of methods to ensure that these systems are not only efficient but also just. The broader impact of this work extends across fields such as healthcare, criminal justice, and public policy, where fair outcomes are critical. By creating tools that help detect and correct unfairness, we aim to support more equitable AI systems that benefit all individuals fairly.     Last Modified: 09/12/2024       Submitted by: EliasBareinboim]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
