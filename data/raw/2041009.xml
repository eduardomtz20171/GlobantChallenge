<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle><![CDATA[FAI: Towards Holistic Bias Mitigation in Computer Vision Systems]]></AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>02/01/2021</AwardEffectiveDate>
<AwardExpirationDate>01/31/2024</AwardExpirationDate>
<AwardTotalIntnAmount>375000.00</AwardTotalIntnAmount>
<AwardAmount>375000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Sylvia Spengler</SignBlockName>
<PO_EMAI>sspengle@nsf.gov</PO_EMAI>
<PO_PHON>7032927347</PO_PHON>
</ProgramOfficer>
<AbstractNarration>With the increasing use of artificial intelligence (AI) systems in life-changing decisions, such as hiring or firing of individuals or the length of jail sentences, there has been an increasing concern about the fairness of these systems. There is a need to guarantee that AI systems are not biased against segments of the population. This project aims to mitigate AI bias in the domain of computer vision, a driving application for much of the recent advances in a popular form of AI known as deep learning. Computer vision systems are increasingly prevalent in areas of society ranging from healthcare to law enforcement: from apps that analyze skin pictures for melanoma detection to face recognition systems used in criminal investigations. These systems are subject to three major sources of bias: biased data, biased annotations, and biased models. Biased data follows from poor image collection practices, typically the under-representation of certain population groups. Biased annotation follows from the use of annotation platforms with untrained image labelers, who tend to produce annotations that reflect their own image interpretations, rather than objective labels. Biased models can ensue from either the existence of data or annotation biases on the datasets used to train the models, or the choice of biased model architectures.  The three bias components have received different attention in the literature, with most previous work focusing on the mitigation of model bias. However, this usually boils down to downplaying groups for which there is a lot of data and promoting groups for which data is scarce. This practice can hurt overall system performance. The remaining sources of bias, datasets and annotation, have received very little algorithmic attention. &lt;br/&gt;&lt;br/&gt;The project aims to overcome this problem, by introducing a new framework to jointly address the three sources of bias within one unified bias mitigation architecture. This architecture aims to train fair classifiers by iterative optimization of three distinct modules: 1) Dataset bias mitigation algorithms that identify and downweigh biased examples and seek additional examples in a large pool of data to counterbalance the associated biases. 2) Label bias mitigation systems based on machine teaching algorithms that establish clear, replicable, and auditable procedures to teach annotators how to label images without label bias. 3) Model auditing techniques based on counterfactual visual explanations that enable the visualization of the factors contributing to model decisions and why they are biased. The three modules combine into an architecture for joint dataset, label, and model bias mitigation by iterative optimization of datasets, annotators, and models to minimize bias. The project will generate software for dataset bias mitigation, unbiased annotator training, explanations and visualizations, model auditing, and fair model training, which will be made available from the investigator website. This will be complemented with datasets for the design of various form of bias mitigation algorithms, and tools to help practitioners detect and combat bias. Several activities are also planned to broaden the participation of underrepresented K-12 and undergraduate students in the STEM field. They will include the participation of a team of such students, recruited from University of California San Diego programs that aim to increase the participation of these groups in STEM, and aim to provide these students with early exposure to the challenges of real-world engineering, fair machine learning, and deep learning systems.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>01/25/2021</MinAmdLetterDate>
<MaxAmdLetterDate>05/20/2021</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2041009</AwardID>
<Investigator>
<FirstName>Nuno</FirstName>
<LastName>Vasconcelos</LastName>
<PI_MID_INIT>M</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Nuno M Vasconcelos</PI_FULL_NAME>
<EmailAddress><![CDATA[nuno@ece.ucsd.edu]]></EmailAddress>
<NSF_ID>000104017</NSF_ID>
<StartDate>01/25/2021</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name><![CDATA[University of California-San Diego]]></Name>
<CityName>LA JOLLA</CityName>
<ZipCode>920930021</ZipCode>
<PhoneNumber>8585344896</PhoneNumber>
<StreetAddress><![CDATA[9500 GILMAN DR]]></StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>50</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA50</CONGRESS_DISTRICT_ORG>
<ORG_UEI_NUM>UYTTZT6G9DT1</ORG_UEI_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CALIFORNIA, SAN DIEGO</ORG_LGL_BUS_NAME>
<ORG_PRNT_UEI_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of California-San Diego]]></Name>
<CityName/>
<StateCode>CA</StateCode>
<ZipCode>920930934</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>50</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA50</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>114Y00</Code>
<Text>Fairness in Artificial Intelli</Text>
</ProgramElement>
<ProgramReference>
<Code>075Z</Code>
<Text>Artificial Intelligence (AI)</Text>
</ProgramReference>
<Appropriation>
<Code>0121</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Fund>
<Code>01002122DB</Code>
<Name><![CDATA[NSF RESEARCH & RELATED ACTIVIT]]></Name>
<FUND_SYMB_ID>040100</FUND_SYMB_ID>
</Fund>
<FUND_OBLG>2021~375000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span>Computer vision systems are increasingly prevalent in areas of society ranging from healthcare to law enforcement. These systems are subject to three major sources of bias: biased data, biased annotations, and biased models. Biased data follows from poor image collection practices, typically the under-representation of certain population groups. Biased annotation follows from the use of annotation platforms with untrained image labelers, who tend to produce annotations that reflect their own image interpretations, rather than objective labels. Biased models can ensue from either the existence of data or annotation biases on the datasets used to train the models, or the choice of biased model architectures. The three bias components have received different attention in the literature, with most previous work focusing on the mitigation of model bias. However, this usually boils down to downplaying groups for which there is a lot of data and promoting groups for which data is scarce. This practice can hurt overall system performance. The remaining sources of bias, datasets and annotation, have received very little algorithmic attention.&nbsp;</span></p> <p><span>The project addressed this problem by introducing a new framework to jointly address the three sources of bias within one unified bias mitigation architecture. This architecture, illustrated in Figure 1, aims to train fair classifiers by iterative optimization of three distinct modules: 1) Dataset bias mitigation algorithms that identify and downweigh biased examples and seek additional examples in a large pool of data to counterbalance the associated biases. 2) Label bias mitigation systems based on machine teaching algorithms that establish clear, replicable, and auditable procedures to teach annotators how to label images without label bias. 3) Model auditing techniques based on visual explanations that enable the visualization of the factors contributing to model decisions and why they are biased.&nbsp;</span></p> <p><span>Beyond the architecture, the project produced technical contributions along all the individual directions. Figure 2 illustrates a&nbsp;new approach to explainable AI, denoted as deliberative explanations. These aim to expose the deliberations carried out by a neural network to arrive at a prediction, by uncovering the insecurities of the network about the latter. The explanation consists of a list of insecurities, each composed of 1) an image region, and 2) an ambiguity formed by the pair of classes responsible for the network uncertainty about the region. Since insecurity detection requires quantifying the difficulty of network predictions, deliberative explanations combine ideas from the literature on visual explanations and assessment of classification difficulty.&nbsp;</span></p> <p><span>Figure 3 illustrates the problem of machine teaching, where the goal is to teach non-experts to label images of expert domains, in this case butterfly species. The annotators are instructed through a visual interface, showing annotated examples chosen by a teaching algorithm. </span><span>A new formulation of machine teaching was proposed under the assumption of an optimal student, where optimality is defined in the usual machine learning sense of empirical risk minimization. It was shown that, if allowed unbounded effort, the optimal student always learns the optimal predictor for a classification task. Hence, the role of the optimal teacher is to select the teaching set that minimizes student effort. This was formulated as a problem of functional optimization where, at each teaching iteration, the teacher seeks to align the steepest descent directions of the risk of (1) the teaching set and (2) entire example population. The optimal teacher, denoted MaxGrad, was then shown to maximize the gradient of the risk on the set of new examples selected per iteration. MaxGrad teaching algorithms were finally provided for both binary and multiclass tasks. Figure 4 illustrates the steps of the MaxGrad algorithm.</span></p> <p><span>A source&nbsp;of bias is poor model performance for different populations due to the uneven data coverage. For example, extensive progress has been achieved for translation between the "major" languages (English, French, Spanish) but fewer advances have been observed for less resourced languages, lacking the very large datasets and models needed to train translation systems. Since "an image is worth a thousand words," the addition of images should reduce the required number of examples needed for training. While existing multimodal methods show promising performance over text-only translation systems, they require paired text and image as input during inference, which limits their applicability to real-world scenarios. We introduced a visual hallucination framework, called VALHALLA, which requires only source sentences at inference time and instead uses hallucinated visual representations for multimodal machine translation, as illustrated in Figure 5. Given a source sentence an autoregressive hallucination transformer predicts a visual representation, and the combined text and hallucinated representations are utilized to generate the target translation. We train the hallucination transformer jointly with the translation transformer using cross-entropy losses and an additional loss that encourages consistency between predictions using either ground-truth or hallucinated visual representations. The model architecture, illustrated in Figure 6, was shown to be successful in several translation datasets.</span></p> <p><span><br /></span></p><br> <p>  Last Modified: 04/25/2024<br> Modified by: Nuno&nbsp;M&nbsp;Vasconcelos</p></div> <div class="porSideCol" ><div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)          </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2024/2041009/2041009_10714035_1714011580135_Vallhala--rgov-214x142.jpg" original="/por/images/Reports/POR/2024/2041009/2041009_10714035_1714011580135_Vallhala--rgov-800width.jpg" title="Figure 5"><img src="/por/images/Reports/POR/2024/2041009/2041009_10714035_1714011580135_Vallhala--rgov-66x44.jpg" alt="Figure 5"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Figure 5: VALHALLA aims to reduce the number of examples needed to train language translation systems by learning to hallucinate images,</div> <div class="imageCredit">Nuno Vasconcelos</div> <div class="imagePermisssions">Creative Commons</div> <div class="imageSubmitted">Nuno&nbsp;M&nbsp;Vasconcelos <div class="imageTitle">Figure 5</div> </div> </li><li> <a href="/por/images/Reports/POR/2024/2041009/2041009_10714035_1714011640466_valhalla_arch--rgov-214x142.jpg" original="/por/images/Reports/POR/2024/2041009/2041009_10714035_1714011640466_valhalla_arch--rgov-800width.jpg" title="Figure 6"><img src="/por/images/Reports/POR/2024/2041009/2041009_10714035_1714011640466_valhalla_arch--rgov-66x44.jpg" alt="Figure 6"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Figure 6: The VALHALLA model architecture.</div> <div class="imageCredit">Nuno Vasconcelos</div> <div class="imagePermisssions">Creative Commons</div> <div class="imageSubmitted">Nuno&nbsp;M&nbsp;Vasconcelos <div class="imageTitle">Figure 6</div> </div> </li><li> <a href="/por/images/Reports/POR/2024/2041009/2041009_10714035_1714011401192_maxgrad--rgov-214x142.jpg" original="/por/images/Reports/POR/2024/2041009/2041009_10714035_1714011401192_maxgrad--rgov-800width.jpg" title="Figure 4"><img src="/por/images/Reports/POR/2024/2041009/2041009_10714035_1714011401192_maxgrad--rgov-66x44.jpg" alt="Figure 4"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Figure 4: The MaxGrad machine teaching algorithm.</div> <div class="imageCredit">Nuno Vasconcelos</div> <div class="imagePermisssions">Creative Commons</div> <div class="imageSubmitted">Nuno&nbsp;M&nbsp;Vasconcelos <div class="imageTitle">Figure 4</div> </div> </li><li> <a href="/por/images/Reports/POR/2024/2041009/2041009_10714035_1714011320603_butterflies--rgov-214x142.png" original="/por/images/Reports/POR/2024/2041009/2041009_10714035_1714011320603_butterflies--rgov-800width.png" title="Figure 3"><img src="/por/images/Reports/POR/2024/2041009/2041009_10714035_1714011320603_butterflies--rgov-66x44.png" alt="Figure 3"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Figure 3: Machine teaching aims to teach non-experts to label images of expert domains, in this case butterfly species.</div> <div class="imageCredit">Nuno Vasconcelos</div> <div class="imagePermisssions">Creative Commons</div> <div class="imageSubmitted">Nuno&nbsp;M&nbsp;Vasconcelos <div class="imageTitle">Figure 3</div> </div> </li><li> <a href="/por/images/Reports/POR/2024/2041009/2041009_10714035_1714011093161_memorable--rgov-214x142.jpg" original="/por/images/Reports/POR/2024/2041009/2041009_10714035_1714011093161_memorable--rgov-800width.jpg" title="Figure 1"><img src="/por/images/Reports/POR/2024/2041009/2041009_10714035_1714011093161_memorable--rgov-66x44.jpg" alt="Figure 1"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Figure 1: architecture used to train classifiers by iterative optimization of model and dataset.</div> <div class="imageCredit">Nuno Vasconcelos</div> <div class="imagePermisssions">Creative Commons</div> <div class="imageSubmitted">Nuno&nbsp;M&nbsp;Vasconcelos <div class="imageTitle">Figure 1</div> </div> </li><li> <a href="/por/images/Reports/POR/2024/2041009/2041009_10714035_1714011213813_explanations--rgov-214x142.jpg" original="/por/images/Reports/POR/2024/2041009/2041009_10714035_1714011213813_explanations--rgov-800width.jpg" title="Figure 2"><img src="/por/images/Reports/POR/2024/2041009/2041009_10714035_1714011213813_explanations--rgov-66x44.jpg" alt="Figure 2"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Figure 2: Deliberative explanations consist of a list of insecurities, each composed of 1) an image region, and 2) an ambiguity formed by the pair of classes responsible for the network uncertainty about the region.</div> <div class="imageCredit">Nuno Vasconcelos</div> <div class="imagePermisssions">Creative Commons</div> <div class="imageSubmitted">Nuno&nbsp;M&nbsp;Vasconcelos <div class="imageTitle">Figure 2</div> </div> </li></ul> </div> </div></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[  Computer vision systems are increasingly prevalent in areas of society ranging from healthcare to law enforcement. These systems are subject to three major sources of bias: biased data, biased annotations, and biased models. Biased data follows from poor image collection practices, typically the under-representation of certain population groups. Biased annotation follows from the use of annotation platforms with untrained image labelers, who tend to produce annotations that reflect their own image interpretations, rather than objective labels. Biased models can ensue from either the existence of data or annotation biases on the datasets used to train the models, or the choice of biased model architectures. The three bias components have received different attention in the literature, with most previous work focusing on the mitigation of model bias. However, this usually boils down to downplaying groups for which there is a lot of data and promoting groups for which data is scarce. This practice can hurt overall system performance. The remaining sources of bias, datasets and annotation, have received very little algorithmic attention.   The project addressed this problem by introducing a new framework to jointly address the three sources of bias within one unified bias mitigation architecture. This architecture, illustrated in Figure 1, aims to train fair classifiers by iterative optimization of three distinct modules: 1) Dataset bias mitigation algorithms that identify and downweigh biased examples and seek additional examples in a large pool of data to counterbalance the associated biases. 2) Label bias mitigation systems based on machine teaching algorithms that establish clear, replicable, and auditable procedures to teach annotators how to label images without label bias. 3) Model auditing techniques based on visual explanations that enable the visualization of the factors contributing to model decisions and why they are biased.   Beyond the architecture, the project produced technical contributions along all the individual directions. Figure 2 illustrates anew approach to explainable AI, denoted as deliberative explanations. These aim to expose the deliberations carried out by a neural network to arrive at a prediction, by uncovering the insecurities of the network about the latter. The explanation consists of a list of insecurities, each composed of 1) an image region, and 2) an ambiguity formed by the pair of classes responsible for the network uncertainty about the region. Since insecurity detection requires quantifying the difficulty of network predictions, deliberative explanations combine ideas from the literature on visual explanations and assessment of classification difficulty.   Figure 3 illustrates the problem of machine teaching, where the goal is to teach non-experts to label images of expert domains, in this case butterfly species. The annotators are instructed through a visual interface, showing annotated examples chosen by a teaching algorithm. A new formulation of machine teaching was proposed under the assumption of an optimal student, where optimality is defined in the usual machine learning sense of empirical risk minimization. It was shown that, if allowed unbounded effort, the optimal student always learns the optimal predictor for a classification task. Hence, the role of the optimal teacher is to select the teaching set that minimizes student effort. This was formulated as a problem of functional optimization where, at each teaching iteration, the teacher seeks to align the steepest descent directions of the risk of (1) the teaching set and (2) entire example population. The optimal teacher, denoted MaxGrad, was then shown to maximize the gradient of the risk on the set of new examples selected per iteration. MaxGrad teaching algorithms were finally provided for both binary and multiclass tasks. Figure 4 illustrates the steps of the MaxGrad algorithm.   A sourceof bias is poor model performance for different populations due to the uneven data coverage. For example, extensive progress has been achieved for translation between the "major" languages (English, French, Spanish) but fewer advances have been observed for less resourced languages, lacking the very large datasets and models needed to train translation systems. Since "an image is worth a thousand words," the addition of images should reduce the required number of examples needed for training. While existing multimodal methods show promising performance over text-only translation systems, they require paired text and image as input during inference, which limits their applicability to real-world scenarios. We introduced a visual hallucination framework, called VALHALLA, which requires only source sentences at inference time and instead uses hallucinated visual representations for multimodal machine translation, as illustrated in Figure 5. Given a source sentence an autoregressive hallucination transformer predicts a visual representation, and the combined text and hallucinated representations are utilized to generate the target translation. We train the hallucination transformer jointly with the translation transformer using cross-entropy losses and an additional loss that encourages consistency between predictions using either ground-truth or hallucinated visual representations. The model architecture, illustrated in Figure 6, was shown to be successful in several translation datasets.         Last Modified: 04/25/2024       Submitted by: NunoMVasconcelos]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
