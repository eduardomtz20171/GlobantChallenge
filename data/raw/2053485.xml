<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle><![CDATA[Collaborative Research: Langevin Markov Chain Monte Carlo Methods for Machine Learning]]></AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>06/01/2021</AwardEffectiveDate>
<AwardExpirationDate>08/31/2024</AwardExpirationDate>
<AwardTotalIntnAmount>180009.00</AwardTotalIntnAmount>
<AwardAmount>180009</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>03040000</Code>
<Directorate>
<Abbreviation>MPS</Abbreviation>
<LongName>Direct For Mathematical &amp; Physical Scien</LongName>
</Directorate>
<Division>
<Abbreviation>DMS</Abbreviation>
<LongName>Division Of Mathematical Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Yong Zeng</SignBlockName>
<PO_EMAI>yzeng@nsf.gov</PO_EMAI>
<PO_PHON>7032927299</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The research in this project will focus on a particular class of algorithms for machine learning and data science. In particular, the investigators consider the large class of Markov Chain Monte Carlo (MCMC) methods which arise in several contexts in machine learning and data science. The project will develop new algorithms within the subclass called Langevin MCMC methods. These new algorithms will be scalable to high dimensions and large datasets and will be faster than traditional ones. The features of scalability and fast convergence are important for use in Bayesian statistical inference as well as in non-convex stochastic optimization methods for machine learning. The algorithms will allow efficient training and calibration of predictive machine learning models from large-scale data and have a direct impact on a broad range of data-driven application areas from information technology to computer vision.  Graduate students will be trained and involved in research.&lt;br/&gt; &lt;br/&gt;In this project, the PIs investigate a new class of algorithms within the class of Langevin MCMC methods.  These algorithms can be applied in three contexts of machine learning and data science. First, they can be used for Bayesian (learning) inference problems with high-dimensional models, where the objective is to sample from a posterior distribution given a prior distribution on the parameter space and the likelihood of the observed data. Second, they can be used for solving stochastic non-convex optimization problems including the challenging problems arising in deep learning. Third, they arise in modeling and approximating workhorse algorithms in data science such as stochastic gradient descent methods. By leveraging out the connections between stochastic gradient algorithms and MCMC algorithms, the proposed approach results in a new class of stochastic gradient algorithms called Hamiltonian Accelerated Stochastic Gradient that can outperform existing methods in deep learning practice. A first goal of the project is to study theoretical convergence properties of the proposed algorithms further to fill out the current gap between theory and practice, as well as to develop new scalable algorithms that can extend the existing framework. A second goal is to investigate existing Langevin algorithms further to provide non-asymptotic rigorous performance guarantees relevant to machine learning and data science practice.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>03/17/2021</MinAmdLetterDate>
<MaxAmdLetterDate>05/21/2021</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.049</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2053485</AwardID>
<Investigator>
<FirstName>Mert</FirstName>
<LastName>Gurbuzbalaban</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Mert Gurbuzbalaban</PI_FULL_NAME>
<EmailAddress><![CDATA[mg1366@rutgers.edu]]></EmailAddress>
<NSF_ID>000739809</NSF_ID>
<StartDate>03/17/2021</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name><![CDATA[Rutgers University Newark]]></Name>
<CityName>NEWARK</CityName>
<ZipCode>071023026</ZipCode>
<PhoneNumber>9739720283</PhoneNumber>
<StreetAddress><![CDATA[123 WASHINGTON ST]]></StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New Jersey</StateName>
<StateCode>NJ</StateCode>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NJ10</CONGRESS_DISTRICT_ORG>
<ORG_UEI_NUM>T3NGNR66YK89</ORG_UEI_NUM>
<ORG_LGL_BUS_NAME>RUTGERS, THE STATE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_UEI_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[Rutgers University Newark]]></Name>
<CityName>Piscataway</CityName>
<StateCode>NJ</StateCode>
<ZipCode>088548081</ZipCode>
<StreetAddress><![CDATA[100 Rockafeller Rd, Piscataway]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New Jersey</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>06</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NJ06</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>806900</Code>
<Text>CDS&amp;E-MSS</Text>
</ProgramElement>
<ProgramReference>
<Code>079Z</Code>
<Text>Machine Learning Theory</Text>
</ProgramReference>
<ProgramReference>
<Code>9263</Code>
<Text>COMPUTATIONAL SCIENCE &amp; ENGING</Text>
</ProgramReference>
<Appropriation>
<Code>0121</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Fund>
<Code>01002122DB</Code>
<Name><![CDATA[NSF RESEARCH & RELATED ACTIVIT]]></Name>
<FUND_SYMB_ID>040100</FUND_SYMB_ID>
</Fund>
<FUND_OBLG>2021~180009</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>With advancements in data processing and storage capabilities, we have entered the era of big data. Developing algorithms for machine learning that can efficiently process this data for analysis or predictive purposes has become increasingly crucial. Stochastic gradient descent algorithms (SGD) and Langevin algorithms, based on stochastic gradients, are prominent in the field, especially at the intersection of machine learning and big data. Despite their widespread use, these algorithms are not thoroughly understood in terms of their theoretical generalization power--that is, their ability to perform well on previously unseen data. Additionally, they present several limitations; for instance, they can be slow for many applications, or they may lack performance guarantees.</p> <p>In this NSF project, we developed novel SGD and Langevin algorithms tailored for various applications that significantly outperform existing algorithms in practice across several contexts. We also provided theoretical performance guarantees. Moreover, we achieved numerous theoretical insights that have improved our understanding of their generalization properties. These insights enabled us to design algorithms that surpass traditional SGD and Langevin methods in effectiveness.</p> <p>The applications of our findings are vast, ranging from deep learning, where we develop predictive models that mimic brain function, to Bayesian learning of predictive models in various settings, including logistic and linear regression.</p> <p>&nbsp;</p><br> <p>  Last Modified: 10/04/2024<br> Modified by: Mert&nbsp;Gurbuzbalaban</p></div> <div class="porSideCol" ></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[  With advancements in data processing and storage capabilities, we have entered the era of big data. Developing algorithms for machine learning that can efficiently process this data for analysis or predictive purposes has become increasingly crucial. Stochastic gradient descent algorithms (SGD) and Langevin algorithms, based on stochastic gradients, are prominent in the field, especially at the intersection of machine learning and big data. Despite their widespread use, these algorithms are not thoroughly understood in terms of their theoretical generalization power--that is, their ability to perform well on previously unseen data. Additionally, they present several limitations; for instance, they can be slow for many applications, or they may lack performance guarantees.   In this NSF project, we developed novel SGD and Langevin algorithms tailored for various applications that significantly outperform existing algorithms in practice across several contexts. We also provided theoretical performance guarantees. Moreover, we achieved numerous theoretical insights that have improved our understanding of their generalization properties. These insights enabled us to design algorithms that surpass traditional SGD and Langevin methods in effectiveness.   The applications of our findings are vast, ranging from deep learning, where we develop predictive models that mimic brain function, to Bayesian learning of predictive models in various settings, including logistic and linear regression.        Last Modified: 10/04/2024       Submitted by: MertGurbuzbalaban]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
